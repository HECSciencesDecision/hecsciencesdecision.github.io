[
  {
    "objectID": "presentations/archives/2020-01-31.html",
    "href": "presentations/archives/2020-01-31.html",
    "title": "Longitudinal functional regression: tests of significance",
    "section": "",
    "text": "We consider longitudinal functional regression, where, for each subject, the response consists of multiple curves observed at different time visits. We discuss tests of significance in two general settings. First, when there are no additional covariates, we develop a hypothesis testing methodology for formally assessing that the mean function does not vary over time. Second, in the presence of other covariates, we propose a testing procedure to determine the significance of the covariate’s time-varying effect formally. The methods account for the complex dependence structure of the response and are computationally efficient. Numerical studies confirm that the testing approaches have the correct size and are have a superior power relative to available competitors. We illustrate the methods on a real data application."
  },
  {
    "objectID": "presentations/archives/2020-01-31.html#résumé",
    "href": "presentations/archives/2020-01-31.html#résumé",
    "title": "Longitudinal functional regression: tests of significance",
    "section": "",
    "text": "We consider longitudinal functional regression, where, for each subject, the response consists of multiple curves observed at different time visits. We discuss tests of significance in two general settings. First, when there are no additional covariates, we develop a hypothesis testing methodology for formally assessing that the mean function does not vary over time. Second, in the presence of other covariates, we propose a testing procedure to determine the significance of the covariate’s time-varying effect formally. The methods account for the complex dependence structure of the response and are computationally efficient. Numerical studies confirm that the testing approaches have the correct size and are have a superior power relative to available competitors. We illustrate the methods on a real data application."
  },
  {
    "objectID": "presentations/archives/2020-01-31.html#biographie-de-la-conférencière",
    "href": "presentations/archives/2020-01-31.html#biographie-de-la-conférencière",
    "title": "Longitudinal functional regression: tests of significance",
    "section": "Biographie de la conférencière:",
    "text": "Biographie de la conférencière:\nAna-Maria Staicu is Associate Professor in the Department of Statistics, North Carolina State University. She completed a PhD in 2007 from University of Toronto CA, under the supervision of Nancy Reid. Before joining NC State in 2009, she was a Brunel research fellow at the University of Bristol UK and also worked with Ciprian Crainiceanu and Ray Carroll. Her research interests are primarily in functional data analysis, longitudinal data analysis, nonparametric statistics, and brain imaging analysis. It has been applied to brain tractography studies of MS and brain imaging studies more general, wearable computing, animal studies, and environmental studies. She is a recipient of the NSF Career Award."
  },
  {
    "objectID": "presentations/archives/2025-01-16.html",
    "href": "presentations/archives/2025-01-16.html",
    "title": "Placebo Tests Done Right",
    "section": "",
    "text": "Résumé\nIn order to assess the significance of a regressor, applied regression analysts sometimes rerun their regression replacing the regressor of interest with a randomly perturbed — often permuted — version which is expected to be insignificant, i.e., a placebo. While these procedures are often heuristic in their justification, p -values are often reported. We observe that the interpretation of such placebo tests — for observational data — as Fisher test –designed for experimental data– is common bu misleading, and that many such placebo tests and p-values are in fact invalid. In particular, we argue that almost all such tests for multivariate linear regression are invalid. A unified treatment of randomization inference and Fisher tests suggests more robust interpretations and designs of such placebo procedures, and allows us to handle the multivariate linear regression case. We will spend the main part of the presentation producing such a valid test for observational data.\n\n\nBiographie\nDr. Guillaume A. Pouliot is assistant professor at the Harris School of Public Policy and the College at the University of Chicago. Pouliot received his PhD from Harvard University. Previously, he received his B.A. (Honors) in economics as well as his M.S. (concurrent) in statistics from the University of Chicago. His current research focuses on developing statistical methods for nonstandard problems in public policy and economics, as well as the extension of machine learning methods for applications in public policy, and problems at the interface of econometrics and optimization."
  },
  {
    "objectID": "presentations/archives/2025-03-10.html",
    "href": "presentations/archives/2025-03-10.html",
    "title": "A Dynamic Equilibrium Model of Liquidity Risk",
    "section": "",
    "text": "Résumé\nWe present a framework for analyzing the equilibrium implications of liquidity risk dynamics on asset prices. Our model features two risk-averse agents who continuously trade a security to hedge nontraded risks, while facing stochastic transaction costs correlated with their trading needs. We derive explicit solutions for equilibrium prices and traded quantities under small transaction costs, showing that the illiquidity discount increases with the correlation between trading costs and trading needs. Calibrating the model using NYSE and AMEX data, we find that liquid portfolios recover faster from liquidity shocks and exhibit smaller fluctuations, whereas illiquid portfolios are highly sensitive to trading-cost dynamics. For the most illiquid portfolio, the illiquidity discount increases by 12.75% with full correlation comparing to the uncorrelated case.\n\n\nBiographie\nDr. Xiaofei Shi is an assistant professor in the Department of Statistical Sciences at the University of Toronto. Before joining UofT, she worked as a Term Assistant Professor at Columbia University. Xiaofei obtained a PhD in Mathematical Finance at Carnegie Mellon University, under the supervision of Prof. Johannes Muhle-Karbe. Her research interests revolve around stochastic optimization and stochastic differential equations with applications to mathematical finance."
  },
  {
    "objectID": "presentations/archives/2019-10-16.html",
    "href": "presentations/archives/2019-10-16.html",
    "title": "Media and the stock market: Their relationship and abnormal dynamics around earnings announcements",
    "section": "",
    "text": "Résumé\nThis paper investigates the abnormal tone dynamics of media news articles about firms near earnings announcements and how it relates to earnings results and the stock market. We document a post-earnings abnormal tone drift, suggesting inertia in the media opinion. Also, we find that negative cumulative abnormal tone at the earnings events predicts a subsequent stock price reversal. Higher investor’s attention, due to event coverage by the three type of sources (i.e., newswire, newspaper, web publications), amplifies the reversal. Finally, a high level of abnormal share turnover at those events suggests that the reversal is due to overreacting investors.\n\n\nBiographie\nKeven Bluteau has a joint Ph.D. in finance and business economics from the University of Neuchâtel and Vrije Universiteit Brussel. His research focuses on volatility modeling and sentiment analysis applied to finance and business problems. His current work is centered around the field of “Sentometrics”, which lies at the intersection of sentiment analysis and econometrics. Keven is also a proponent of the open-source philosophy. He is the co-author of several R packages that are actively used in the industry, as well as in the research community."
  },
  {
    "objectID": "presentations/archives/2020-11-27.html",
    "href": "presentations/archives/2020-11-27.html",
    "title": "High-Frequency Factor Models and Regressions",
    "section": "",
    "text": "Résumé\nWe consider a nonparametric time series regression model. Our framework allows precise estimation of betas without the usual assumption of betas being piecewise constant. This property makes our framework particularly suitable to study individual stocks. We provide an inference framework for all components of the model, including idiosyncratic volatility and idiosyncratic jumps. Our empirical analysis investigates the largest data set in the high-frequency literature. First, we use all traded stocks from NYSE, AMEX, and NASDAQ stock markets for 1996-2017 to construct the five Fama-French factors and the momentum factor at the 5-minute frequency. Second, we document the key empirical properties across all the stocks and the new factors, and apply the nonparametric time series regression model with the new high-frequency Fama-French factors. We find that this factor model is effective in explaining the systematic component of the risk of individual stocks. In addition, we provide evidence that idiosyncratic jumps are related to idiosyncratic events such as earnings disappointments. This is joint work with Y. Ait-Sahalia and D. Xiu\n\n\nBiographie\nIlze Kalnina is an Assistant Professor of Economics at the Poole College of Management of North Carolina State University (NCSU). She researches nonparametric estimation and inference for volatility with high frequency data. Prior to joining NC State, Kalnina was an assistant professor at the University of Montreal. Kalnina received a doctorate degree in economics from the London School of Economics."
  },
  {
    "objectID": "presentations/archives/2022-02-25.html",
    "href": "presentations/archives/2022-02-25.html",
    "title": "Differentially private inference via noisy optimization",
    "section": "",
    "text": "Résumé\nWe propose a general optimization-based framework for computing differentially private M-estimators and a new method for constructing differentially private confidence regions. Firstly, we show that robust statistics can be used in conjunction with noisy gradient descent or noisy Newton methods in order to obtain optimal private estimators with global linear or quadratic convergence, respectively. We establish local and global convergence guarantees, under both local strong convexity and self-concordance, showing that our private estimators converge with high probability to a nearly optimal neighborhood of the non-private M-estimators. Secondly, we tackle the problem of parametric inference by constructing differentially private estimators of the asymptotic variance of our private M-estimators. This naturally leads to approximate pivotal statistics for constructing confidence regions and conducting hypothesis testing. We demonstrate the effectiveness of a bias correction that leads to enhanced small-sample empirical performance in simulations. We illustrate the benefits of our methods in several numerical examples.\n\n\nBiographie\nDr. Marco Avella est professeur adjoint en prétitularisation conditionnelle au département de statistique de l’Université Columbia. Il a complété un doctorat au sein du Geneva School of Economics and Management (GSEM) à l’Université de Genève sous la direction d’Elvezio Ronchetti et un stage postdoctoral au MIT. Son domaine d’expertise principal se situe à l’intersection de la statistique robuste, l’apprentissage automatique et les données en haute dimension."
  },
  {
    "objectID": "presentations/archives/2021-05-18.html",
    "href": "presentations/archives/2021-05-18.html",
    "title": "Rethinking Introductory Statistics for Life Sciences Programs",
    "section": "",
    "text": "Résumé\nDespite reproducibility concerns in science, an increased awareness of the prevalence of statistical errors in life sciences research, and even a tightening up of quantitative reporting standards in journals, statistical errors continue to permeate life sciences research. Yet, life science students often only need to take one Statistics course in their undergraduate programs, if that. What can we, as educators, do to prepare future life scientists for statistical practice in research when we have such limited time with them?       In a collaboration between the Department of Statistical Sciences and the Human Biology Program at the University of Toronto, a second-year undergraduate course was introduced a few years ago to integrate statistics instruction with research design to improve the quantitative training of life sciences students. A study was conducted in this course to explore students’ perceptions about statistical practice in the life sciences and their preparedness use statistical methods appropriately in research. Student attitudes and self-efficacies for Statistics, as well as their abilities to recognize and handle problems related to statistical practice in life sciences research was assessed by way of surveys administered at the beginning and end of the course. In this talk, we will highlight this course, share key findings of the study, and reflect on how the results can help inform future course offerings and training initiatives to better prepare our students to effectively engage with Statistics in research.  \n\n\nBiographie\nDr. Bethany White holds a BScH in Mathematics and Statistics from Acadia University and a PhD in Statistics/Biostatistics and a M.Math in Statistics, both from the University of Waterloo. She is the Associate Chair for Undergraduate Studies in Statistics and an Associate Professor, Teaching Stream, in the Department of Statistical Sciences at the University of Toronto. Her research interests involve the impact of technology-enhanced and simulation activities on student learning and attitudes toward statistics. She also has a pedagogical interest in the quantitative training of life sciences students. She served on the Statistical Society of Canada (SSC) Statistical Education Section Executive Committee between 2013-2016 (President of the Section for 2014-2015) and is currently on the SSC Board of Directors, and has served on the editorial boards of a couple of statistics education journals and on organizing committees for statistics and science education workshops and conferences in Canada and the US.\nDr. Jasty Singh received her PhD in Immunology from the University of Toronto, where she studied approaches to make immune cells from stem cells. Her postdoctoral fellowship (Institute of Biomaterials and Biomedical Engineering, University of Toronto) enabled her to extend these interests into the stem cell bioengineering space, where she engineered immune cells and developed clinically relevant biomaterials for use in immunotherapy. Currently, Jasty is an Assistant Professor, Teaching Stream with the Department of Immunology at the University of Toronto. Her previous research experiences have guided her pedagogical interests, which, in part, focus on preparing undergraduate/graduate life sciences researchers to engage with statistics in research."
  },
  {
    "objectID": "presentations/archives/2020-01-09.html",
    "href": "presentations/archives/2020-01-09.html",
    "title": "Some statistical applications of generative neural networks",
    "section": "",
    "text": "Résumé\nWe present some examples of how the field of statistics can benefit from the Deep Learning movement. First, using generative neural networks (GNNs), we are now able to produce quasi Monte Carlo samples from “almost any” copula model. For example, we can do this even for mixtures of copulas with singular components. Second, using GNNs, we can now model and, most importantly, forecast multivariate time series without having to restrict ourselves to using only a few parametric copula families to describe the underlying multivariate dependence. We have empirical evidence that a better dependence model does indeed translate into better forecasts.\nThis is joint work with Marius Hofert and Avinash Prasad.\n\n\nBiographie\nMu Zhu is a professor of statistics at the University of Waterloo and a Fellow of the American Statistical Association. A Phi Beta Kappa graduate of Harvard University, he obtained his PhD from Stanford University. His research has received a prestigious Discovery Accelerator Supplement Award from the Natural Sciences and Engineering Research Council of Canada (NSERC). Mu’s main research interests are machine learning, multivariate analysis, and network data analysis, with their applications to bioinformatics, health informatics, and data mining."
  },
  {
    "objectID": "presentations/archives/2025-04-23.html",
    "href": "presentations/archives/2025-04-23.html",
    "title": "An Extended Validity Domain for Constraint Learning",
    "section": "",
    "text": "Résumé\nWe consider embedding a (predictive) machine-learning model within a (prescriptive) optimization problem. In this setting, called constraint learning, the computed optimal solution may be too far from the training data, that is, in a region where the machine-learning predictions are less accurate. To correct for this, researchers have proposed the concept of a validity domain, which further constrains the optimization to stay close to the training data. One common choice forces the decision variable to lie within the convex hull of the data. In this talk, we propose a new validity domain, which uses the convex-hull idea in an extended space. We investigate its properties and compare it empirically with existing techniques on a set of test problems for which the ground-truth optimization problem is known. We also consider our approach within a pricing case study using real-world data. This is joint work with Yilin Zhu at the University of Iowa.\n\n\nBiographie\nSam Burer is the Tippie Rollins Professor in the Department of Business Analytics at the University of Iowa. He received his Ph.D. from Georgia Tech, and his research focuses on convex optimization. He is the recipient of the 2020 INFORMS Computing Paper Prize and the 2023 SIAM Optimization Test of Time Award. His work has been supported by grants from the National Science Foundation, including the CAREER award, and he currently serves as an area editor of Operations Research and as an associate editor for SIAM Journal on Optimization and Mathematical Programming. He also serves as Treasurer of the Mathematical Optimization Society and is a past Vice Chair of the SIAM Activity Group on Optimization. He teaches at all levels of business education, served as the founding faculty director of two Master’s programs, and is the 2022 recipient of the University of Iowa’s President and Provost Award for Teaching Excellence."
  },
  {
    "objectID": "presentations/archives/2020-10-16.html",
    "href": "presentations/archives/2020-10-16.html",
    "title": "Optimizing Post-Disruption Response Operations to Improve Resilience of Critical Infrastructure Systems",
    "section": "",
    "text": "Résumé\nCritical infrastructure systems (CIS) underpin almost every aspect of the modern society by enabling the essential functions through overlaying service networks. After a disruption impacting the CIS, the functionality of the overlaying service networks degrades. Thus, after an extreme event, in order to minimize the negative impact to society, it is crucial to restore the disrupted CIS as soon as possible. In this talk, we focus on disruptions created by natural hazards on transportation CIS and develop methods to efficiently plan the post-disaster response operations. In the aftermath of a natural disaster, the transportation network is disrupted due to the debris blocking the roads and obstructing the flow of relief aid and search-and-rescue teams between critical facilities and disaster sites. In the first few days following a disaster, in order to deliver aid to those in need, blocked roads must be cleared by pushing the debris to the sides. In this context, we define the road network recovery problem (RNRP) as finding a schedule to clear the roads with limited resources such that all the service demanding locations are served in the shortest possible time. First, we address the deterministic RNRP and propose a novel network science inspired measure to quantify the criticality of the components within a disrupted service network and develop a restoration heuristic. Next, we consider RNRP with stochastic demand and propose an approximate dynamic programming approach for identifying an effective policy under uncertainty.\n\n\nBiographie\nDr. Özlem Ergun is a professor in Mechanical and Industrial Engineering at Northeastern University. Dr. Ergun’s research focuses on design and management of large-scale and decentralized networks. She has applied her work on network design, management, and resilience to problems arising in many critical systems including transportation, pharmaceuticals, and healthcare. She has worked with organizations that respond to emergencies and humanitarian crises around the world, including USAID, UNWFP, UNHCR, IFRC, OXFAM America, CARE USA, FEMA, USACE, CDC, AFCEMA, and MedShare International. Dr. Ergun is currently serving as a member of the National Academies Committee on Building Adaptable and Resilient Supply Chains after Hurricanes Harvey, Irma, and Maria. Within INFORMS, Dr. Ergun has been a leader in establishing a strong community of OR/MS professionals with an interest in public programs. She was the President of INFORMS Section on Public Programs, Service and Needs in 2013. She currently serves as the Area Editor at the Operations Research journal for Policy Modeling and the Public Sector Area. Dr. Ergun is also a founding co-chair of the annual Health and Humanitarian Logistics Conference, held annually since 2009. In addition, Dr. Ergun was the Vice President of Membership and Professional Recognition on the INFORMS Board of Directors, 2011 - 2015. Prior to joining Northeastern she was the Coca-Cola Associate Professor in the School of Industrial and Systems Engineering at Georgia Institute of Technology, where she also co-founded and co-directed the Health and Humanitarian Systems Research Center at the Supply Chain and Logistics Institute. She received a B.S. in Operations Research and Industrial Engineering from Cornell University in 1996 and a Ph.D. in Operations Research from the Massachusetts Institute of Technology in 2001."
  },
  {
    "objectID": "presentations/archives/2024-04-30.html",
    "href": "presentations/archives/2024-04-30.html",
    "title": "International environmental agreements in the presence of adaptation and self-image",
    "section": "",
    "text": "Résumé\nWe examine the stability and effectiveness of an international environmental agreement when countries can decide both their emissions and adaptation levels. We assume that adaptation requires a prior irreversible investment and presents the characteristics of a private good by reducing a country’s vulnerability to the impact of pollution, while mitigation policies produce a public good by reducing the total amount of pollution. By using a stylized model capturing the main features of the Paris agreement, we show that investments in adaptation do not ameliorate the participation to the agreement, total emissions by countries are higher than when countries can only emit, and that global welfare is higher with adaptation than without. All this suggests that adaptation is beneficial even if there may be a loss in participation. We also investigate the impact of a regulated adaptation. This is based on joint work with Michèle Breton.\n\n\nBiographie\nDr. Lucia Sbragia est professeure agrégée de sciences économiques à l’école de gestion de l’Université Durham au Royaume-Uni. Ses intérêts de recherches portent sur l’application de concepts de théorie des jeux et de modélisation nonlinéaire dynamique pour l’étude d’enjeux en lien avec l’environnement et les ressources naturelle, notamment la pêche durable et les accords globaux pour la réduction des émissions de polluants. Elle est détentrice d’un doctorat en économie de l’Università Cattolica del Sacro Cuore à Milan."
  },
  {
    "objectID": "presentations/archives/2024-11-11.html",
    "href": "presentations/archives/2024-11-11.html",
    "title": "Locating Facilities for Smart and Sustainable Mobility",
    "section": "",
    "text": "Résumé\nThis talk will explore three facility location problems for smart and sustainable mobility. The first problem focuses on locating and sizing charging stations to support long-distance travel with electric vehicles. A bilevel optimization model is developed where the network planner (leader) minimizes infrastructure costs while meeting probabilistic service requirements for waiting times to charge. Electric vehicle users (followers) minimize their route lengths, affecting charging demand and wait times. The bilevel problem is reduced to a single-level mixed-integer model when stations operate as M/M/c queues and users cooperate. A decomposition-based solution method is developed using a logic-based Benders algorithm. Computational experiments on benchmark and real-world highway networks analyze the impact of route choice, service requirements, and deviation tolerance on decisions. The second problem involves designing hub networks for the strategic deployment of autonomous shuttles. Given a set of passenger trips in an urban area, the goal is to determine the origins and destinations of a fixed number of hub arcs representing shuttle connections to maximize users of the system. This problem is formulated as a maximal covering hub arc location model and solved to optimality using Benders decomposition. Two data-driven clustering-based methodologies are also implemented and compared with the optimization model. Computational experiments using the New York City taxi data compare optimization and data-driven approaches on key performance metrics. The final problem addresses infrastructure design for shared autonomous transportation by locating staging facilities and deploying autonomous lanes. A bi-objective model is developed that minimizes total travel distance as well as non-autonomous lane travel, given a lane deployment budget and the number of staging facilities to locate. Trade-offs are evaluated on benchmark instances.\n\n\nBiographie\nDr. Alumur Alev is a tenured Associate Professor in the Department of Management Science and Engineering at the University of Waterloo. She received her Ph.D. degree in Industrial Engineering from Bilkent University in 2009 and subsequently served as a Post-Doctoral Researcher at the Institute of Operations Research at Karlsruhe Institute of Technology, Germany. Before joining the University of Waterloo in 2014, she worked as an Assistant Professor at TOBB University of Economics and Technology in Ankara. In 2017, she received the Chuck ReVelle Rising Star Award from the Institute for Operations Research and the Management Sciences (INFORMS) as a recognition of her research accomplishments in Location Analysis. In 2020, she received an Outstanding Performance Award at the University of Waterloo and in 2023, she was awarded the Engineering Research Excellence Award in recognition of her outstanding research accomplishments in the Faculty of Engineering. She currently serves as the Senior Associate Editor for the journal Socio-Economic Planning Sciences, an Associate Editor for the journals Transportation Science and Service Science, an Editorial Board Editor for Transportation Research Part B, and an Editorial Board Member for Computers & Operations Research. Her research interests lie in optimizing facility locations and logistics operations, in particular, hub location, hub network design, supply chain and reverse logistics network design."
  },
  {
    "objectID": "presentations/archives/2023-02-08.html",
    "href": "presentations/archives/2023-02-08.html",
    "title": "Court Shopping, Pro-Debtor Bias, and Bankruptcy Outcomes",
    "section": "",
    "text": "Résumé\nThe Bankruptcy Venue Reform Act of 2021 requires large corporations to file for bankruptcy in their principle place of business or where significant assets are located. We study the impact of this bill on the firm’s chances to emerge successfully from bankruptcy, and on the leniency of court judges in grating motions that affect the debtor’s access to cash, access to credit and control of the reorganization plan. We use these three dimensions to quantify the effective pro-debtor bias of Chapter 11 bankruptcy judges based on their decisions within the first 120 days of a voluntary Chapter 11 reorganization of large, publicly held companies between 2004 and 2020. We find a positive association between court shopping and pro-debtor bias, but no effect on firm survival. Our results suggest that the refiling rate can be improved by providing more opportunities to creditors to be involved in the reorganization plan.\n\n\nBiographie\nKris Boudt is professor of finance and econometrics at Ghent University, Vrije Universiteit Brussel and Vrije Universiteit Amsterdam. He is part of the core team at Sentometrics. He teaches the online courses “Introduction to portfolio analysis in R” and “GARCH models in R” at Datacamp. He is also affiliated with the KU Leuven and an invited lecturer at the University of Illinois in Chicago, Renmin University, SWUFE and the University of Aix-Marseille. Kris Boudt obtained his PhD in 2008 for his developments in the modelling and estimation of financial risk under a non-normal distribution. He has published his research in the Journal of Econometrics, Journal of Portfolio Management, Journal of Statistical Software, and the Review of Finance, among others. Kris Boudt received several awards for outstanding research and refereeing and is an active contributor to the open source community."
  },
  {
    "objectID": "presentations/archives/2021-01-22.html#résumé",
    "href": "presentations/archives/2021-01-22.html#résumé",
    "title": "Asymptotic Expansion Formulas for Diffusion Processes Based on the Perturbation Method",
    "section": "Résumé:",
    "text": "Résumé:\nDiffusion processes are a class of models that plays a prominent role in describing the time-continuous evolution of phenomena in the natural and social sciences. However, only in very few cases the stochastic differential equation driving the process can be analytically solved. Based on the perturbation method, we present asymptotic expansion formulas to generate accurate approximations to the solution of arbitrary diffusions. In particular, we expand the characteristic function of the process. Then, the approximated expectation and moments are computed by differentiation, and the approximated transition density is written in terms of Hermite polynomials by applying Fourier transform. The computational efficiency, accuracy, and flexibility of the method are assessed via experiments conducted against closed-form solutions and Monte Carlo simulations in tasks involving density approximation, expectations, moments, filtering, and functionals of generic diffusion processes."
  },
  {
    "objectID": "presentations/archives/2021-01-22.html#biographie-du-conférencier",
    "href": "presentations/archives/2021-01-22.html#biographie-du-conférencier",
    "title": "Asymptotic Expansion Formulas for Diffusion Processes Based on the Perturbation Method",
    "section": "Biographie du conférencier:",
    "text": "Biographie du conférencier:\nEmanuele Guidotti est doctorant en finance à l’Université de Neuchâtel spécialisé dans la tarification empirique des actifs. Il est également chercheur associé au CREST, Japan Science and Technology Agency depuis 2017 au sein du projet YUIMA, une équipe internationale de recherche qui oeuvre à développer un environnement complet pour l’estimation et la simulation d’équations différentielles stochastiques. Il est également partenaire au sein de Algo Finance Sagl, une jeune pousse spécialisé dans la création de logiciels et d’algorithmes en finance pour le secteur de la gestion d’actifs. Emanuele enseigne à titre de professeur associé à l’Université de Milan au sein du programme de maîtrise en science des données et économie. Emanuele détient une licence en physique et une maîtrise en économie et en finance quantitive, toutes deux décernées par l’Université de Milan cum laude."
  },
  {
    "objectID": "presentations/archives/2021-04-07.html",
    "href": "presentations/archives/2021-04-07.html",
    "title": "When does eco-efficiency rebound or backfire? An analytical model",
    "section": "",
    "text": "Résumé\nIt is known that an eco-efficiency strategy, which saves resources in the production process, may be offset by a rebound effect; it may even backfire. Less known are the exact conditions under which eco-efficiency rebounds or backfires. This article fills the gap by providing an analytical model of the rebound and backfire effects. We propose an optimal control framework of dynamic pricing and eco-efficiency investment, for which eco-efficiency reduces the unit production cost and boosts the demand of environmentally concerned consumers. Results, which hold with a general demand formulation, examine the analytic conditions for the rebound and backfire effects. They also highlight the possibility of a reverse rebound effect. Such results pave the way to sounder sustainability strategies.\n\n\nBiographie\nRégis Chevanaz is an associate professor of managerial economics and operations management at KEDGE Business School. His research focuses on operations management, firm micro-economics, and business analytics. He holds a PhD degree from Telecom Paris and an HDR from Université Nice Sophia Antipolis and his research has been featured notably in the European Journal of Operational Research and in Applied Economics."
  },
  {
    "objectID": "presentations/archives/2024-03-15.html",
    "href": "presentations/archives/2024-03-15.html",
    "title": "Skew-symmetric approximations of posterior distributions",
    "section": "",
    "text": "Résumé\nA broad class of regression models that routinely appear in several fields of application can be expressed as partially or fully discretized Gaussian linear regressions. Besides incorporating the classical Gaussian response setting, this class crucially encompasses probit, multinomial probit and tobit models, among others. The relevance of these representations has motivated decades of active research within the Bayesian field. A main reason for this constant interest is that, unlike for the Gaussian response setting, the posterior distributions induced by these models do not seem to belong to a known and tractable class, under the commonly-assumed Gaussian priors. In this seminar, I will review, unify and extend recent advances in Bayesian inference and computation for such a class of models, proving that unified skew-normal (SUN) distributions (which include Gaussians as a special case) are conjugate to the general form of the likelihood induced by these formulations. This result opens new avenues for improved sampling-based methods and more accurate and scalable deterministic approximations from variational Bayes. These results are further extended via a general and provably-optimal strategy to improve, via a simple perturbation, the accuracy of any symmetric approximation of a generic posterior distribution. Crucially, such a novel perturbation is derived without additional optimization steps and yields a similarly-tractable approximation within the class of skew-symmetric densities that provably enhances the finite-sample accuracy of the original symmetric approximation. Theoretical support is provided, in asymptotic settings, via a refined version of the Bernstein–von Mises theorem that relies on skew-symmetric limiting densities.\n\n\nBiographie\nDr. Daniele Durante est professeur adjoint à l’Université Bocconi depuis 2017 et chercheur affilié à l’institut de sciences des données et d’analytique de Bocconi, du centre DONDENA de recherche en dynamique sociale et politique publique et du Laboratoire de recherche sur la crise du coronavirus. Daniele a obtenu son doctorat sous la direction de Bruno Scarpa, co-encadré par David Dunson, à l’Université de Padoue, où ila également complété sa maîtrise et un stage post-doctoral. Il est éditeur associé de Biometrika, du Journal of Computational and Graphical Statistics et du Journal of Multivariate Analysis. Durante travaille sur la recherche disciplinaire et la méthodologie bayésienne; il est le récipiendaire de nombreux prix, dont le prix Laplace, le prix Mitchell, et le prix de la meilleure thèse de doctorat de la Società Italiana di Statistica."
  },
  {
    "objectID": "presentations/archives/2022-11-11.html",
    "href": "presentations/archives/2022-11-11.html",
    "title": "Optimal stopping mean-field games: a linear programming formulation and applications to entry-exit games in electricity markets",
    "section": "",
    "text": "Résumé\nIn this talk, we present recent results on the linear programming approach to stopping mean-field games in a general setting. This relaxed control approach allows to prove existence results under weak assumptions, and lends itself well to numerical implementation. We consider mean-field game problems where the representative agent chooses the optimal time to exit the game, where the instantaneous reward function and the coefficients of the state process may depend on the distribution of the other agents. Furthermore, we establish the equivalence between mean-field games equilibria obtained by the linear programming approach and the ones obtained via other approaches used in the previous literature. We then present a fictious play algorithm to approximate the mean-field game population dynamics in the context of the linear programming approach. Finally, we give an application of the theoretical and numerical contributions introduced in the first part of the talk to an entry-exit game in electricity markets. The talk is based on several works, joint with R. Aïd, G. Bouveret, M. Leutscher and P. Tankov.\n\n\nBiographie\nRoxana Dumitrescu is an associate professor at King’s College London, United Kingdom. She is a leading expert in stochastic control and mean-field games, with publications in leading journals in the field. She has been recently working on optimal stopping mean-field games, a new trend in the literature, and developed together with several co-authors a new approach to solve them based on a linear-programming formulation. Prior to the appointment at King’s College, she has been an associate researcher in the Mathematics Department at Humboldt University in Berlin and a member of the reasearch training group “Stochastic Analysis with Applications in Finance, Physics and Biology” (2015-2016). She defended her PhD in Mathematics at University Dauphine, in Paris (2015). During her PhD studies, she was a researcher in the Financial Mathematics Group at the National French Institute for Research in Computer Science and Automatics Control, INRIA, France."
  },
  {
    "objectID": "presentations/archives/2022-06-10.html",
    "href": "presentations/archives/2022-06-10.html",
    "title": "The Leverage Effect and Propagation",
    "section": "",
    "text": "Résumé\nThis paper proposes a new way to measure the leverage effect and its propagation over time. We also show that, with respect to the newly proposed measure, common volatility models like the GJRGARCH, the Exponential GARCH, and the asymmetric SV can be inaccurate to correctly represent the leverage effect and its propagation for financial time series. We propose to modify the variance recursion of common volatility models by including an auxiliary leverage process which allows for a proper representation of the leverage effect and its propagation over time. Empirical results indicate that the inclusion of the auxiliary leverage process improves both in sample and out of sample.\n\n\nBiographie\nLeopoldo Catania est professeur agrégé au sein de l’école de gestion et de sciences sociales de l’Université d’Aarhus au Danemark. Ses intérêts de recherche gravitent autour de l’économétrie financière et de l’analyse des séries chronologiques, notamment pour le développement et l’estimation de modèles pour la gestion quantitative du risque unidimensionnel et multidimensionnel, l’estimation de la densité prédictive de niveaux de retours, la dépendance temporelle et la volatilité. Il est titulaire d’une maîtrise en finance de l’Université Sapienza de Rome et d’un doctorat en économie et finance de l’Université de Rome II (Tor Vergata)."
  },
  {
    "objectID": "presentations/archives/2024-04-12.html",
    "href": "presentations/archives/2024-04-12.html",
    "title": "Designing Distribution Network Tariffs Under Increased Residential End-user Electrification: Can the North America Learn Something from Europe?",
    "section": "",
    "text": "Résumé\nAs decarbonization policies lead to the electrification of the transportation, buildings, and other end-use sectors, it will be necessary to expand distribution network capacities, at significant cost. Most North American utilities currently recover both energy and network costs via time-invariant (flat) charges for kilowatt-hour (kWh) usage. While energy costs do vary with kWh usage, network costs vary instead with peak kilowatt (kW) demand, so recovering network costs via flat per-kWh charges can provide no incentives to shift peak demand to reduce the need for expensive network expansion. Time-of-use (TOU) tariffs that vary the cost per kWh to reflect changes in generation costs, for instance, give incentives to shift all electric vehicle (EV) charging to low-price periods, potentially raising kW demand in those periods and increasing network expansion costs. Efficiency (and, I will argue, equity) requires separating energy charges from network charges, with appropriate rate designs for each. Accordingly, this talk considers rate designs that unbundle energy and network charges and presents a realistic case study to investigate the implications of combining TOU energy charges with various network tariffs in the face of increased EV penetration. The results provide support for the adoption in North America of ex-ante subscribed capacity tariffs (subscription charges), which have been used in Europe for many years.\n\n\nBiographie\nDr. Tim Schittekatte is a Senior Director at FTI Consulting in London, a senior lecturer at MIT and a part-time assistant professor at the Florence School of Regulation (FSR), where he worked as research fellow for five years. He is a former Research Scientist at the MIT Energy Initiative, where he researched on power market design and regulation. Before joining FSR in May 2016, he was a visiting researcher at the Grid Integration Group of the Lawrence Berkeley National Lab and a junior economist at Microeconomix in Paris. Tim holds a PhD in energy economics from University Paris-Sud XI."
  },
  {
    "objectID": "presentations/archives/2023-02-03.html",
    "href": "presentations/archives/2023-02-03.html",
    "title": "Overview of adaptive stochastic optimization methods.",
    "section": "",
    "text": "Résumé\nRecently a variety of stochastic variants of adaptive methods have been developed and analyzed. These include stochastic step search, trust region and cubicly regularized Newton methods. Such methods adapt the step size parameter and use it to dictate the accuracy required or stochastic approximations. The requirements on stochastic approximations are, thus, also adaptive and in principle can be biased and even inconsistent. The step size parameters in these method can increase and decrease based on the perceived progress, but unlike the deterministic case they are not bounded away from zero. This creates obstacles in complexity analysis of such methods. We will show how by viewing such algorithms as stochastic processes with martingale behavior we can derive bounds on expected complexity that also apply in high probability. We also show that it is possible to derive a lower bound on step size parameters in high probability for the methods in this general framework. We will discuss various stochastic settings, where the framework easily applies, such as expectation minimization, black box and simulation optimization, expectation minimization with corrupt samples, etc.\n\n\nBiographie\nDr. Katya Scheinberg is a Professor and Director of Graduate Studies at the School of Operations Research and Information Engineering at Cornell University. Prior to joining Cornell she was the Harvey E. Wagner Endowed Chair Professor at the Industrial and Systems Engineering Department at Lehigh University. She attended Moscow University for her undergraduate studies and received her PhD degree from Columbia University. She worked at the IBM T.J. Watson Research Center as a research staff member for over a decade before joining Lehigh in 2010. Katya’s main research areas are related to developing practical algorithms (and their theoretical analysis) for various problems in continuous optimization, such as convex optimization, derivative free optimization, machine learning, quadratic programming, etc. She is an Informs Fellow, a recipient of the Lagrange Prize from SIAM and MOS, the Farkas Prize from Informs Optimization Society and the Outstanding Simulation Publication award from Informs Simulation Society. Katya is currently the editor-in-chief of Mathematics of Operations Research, and co-editor of Mathematical Programming. She served as the Chair of SIAM Activity Group on Optimization from 2020 until 2022."
  },
  {
    "objectID": "presentations/archives/2024-04-05.html",
    "href": "presentations/archives/2024-04-05.html",
    "title": "Evaluating a major home energy efficiency retrofit program",
    "section": "",
    "text": "Résumé\nWe quantify realized energy consumption, bill savings, and rebates disbursed from Canada’s energy efficiency retrofit program. We find that retrofits reduce natural gas consumption for up to 10 years in the average participating home by about 21%, representing 61% of model-predicted natural gas savings. Whole-envelope retrofits are predicted to reduce natural gas consumption by 67%, but in practice only half of these savings are realized. Electricity consumption did not appreciably change, and several recommended retrofits save zero energy. Program-induced gas bill savings peak among relatively low wealth households, whereas rebates were disbursed equally across the house wealth distribution. This is joint work with Maya Papineau and Kareman Yassin.\n\n\nBiographie\nNicholas Rivers is Associate Professor at the Graduate School of Public and International Affairs and the Institute of the Environment at the University of Ottawa. His research focuses on the economic evaluation of environmental policies, using econometric and computational methods. He has received awards and grants for his research from the Trudeau Foundation, the Social Science and Humanities Research Council, and the National Science and Engineering Research Council. He serves as an expert panel member for the Canadian Climate Institute and as the Director for graduate programs at the Institute of the Environment. From 2011-2021, he was Canada Research Chair in Climate and Energy Policy. From 2016-2022 he was a co-editor of the Journal of Environmental Economics and Management."
  },
  {
    "objectID": "presentations/archives/2019-11-29.html",
    "href": "presentations/archives/2019-11-29.html",
    "title": "Life at the (grid edge): Evaluating energy decentralization strategies in California",
    "section": "",
    "text": "Résumé\nAvoiding the worst consequences of climate change hinges on the transition to a deeply decarbonized global energy system. Recent studies suggest that the least costly and risky way in which this transition could unfold is through radical electrification that is dominated by low-carbon sources of electricity. Several visions of this future have been put forward, some of which hinge on revolutions at the grid-edge that enable smarter energy management and vehicle electrification.\nThis talk will cover recent and ongoing research on the next wave of technical and policy interventions at the grid-edge that have the potential to transform the electric power system. In California, policy makers are promoting distributed energy resources like residential solar and energy storage and expecting that they will both decarbonize and decentralize the electric power system. This presentation will describe a systematic analysis of the unintended effects of deploying residential energy storage (RES) systems. Our results show that RES systems predominantly increase emissions when users seek to minimize their electricity cost.\nAnother development at the grid-edge is the proliferation of electric vehicles (EV). California’s size and status as an early adopter of regulatory standards has given it influence in establishing trends which the rest of the nation inherits. The role it plays in efforts to decarbonize transportation could prove revelatory: the state accounts for 10% of new vehicle sales and 50% of EV sales in the U.S. However, one obstacle to policy development is the dearth of information about EV adopters’ attitudes towards incentives, charging, and potentially emergent challenges. This talk will present preliminary results from a large survey of EV adopters in Southern California that reveals these attitudes and allows modelers and policy makers to elaborate strategies for managing a grid with high EV penetration.\n\n\nBiographie\nAhmed Abdulla is Assistant Research Professor in the Department of Engineering and Public Policy at Carnegie Mellon University, and a Research Fellow in the Deep Decarbonization Initiative at the University of California, San Diego. His research uses large, empirical data, optimization and decision analysis to model the strategic development and deployment of innovative energy technologies that decarbonize the energy system and increase its resiliency. His work has been supported by the U.S. National Science Foundation, the Alfred P. Sloan Foundation and the John D. and Catherine T. MacArthur Foundation, among others. Results from his research have been published in leading journals, including Nature Climate Change and the Proceedings of the National Academy of Sciences; they have also been featured in the Wall Street Journal, Bloomberg News and The Los Angeles Times. Prior to Carnegie Mellon, Abdulla was an Assistant Research Scientist in the Center for Energy Research at the University of California, San Diego. He holds a PhD in Engineering and Public Policy from Carnegie Mellon University (2014) and a BS in Chemical Engineering from Princeton University (2009)."
  },
  {
    "objectID": "presentations/archives/2022-03-23.html",
    "href": "presentations/archives/2022-03-23.html",
    "title": "Comparative Probability Metrics: Using Posterior Probabilities to Account for Practical Equivalence in A/B tests",
    "section": "",
    "text": "Résumé\nOnline controlled experiments (i.e., A/B tests) have become an extremely valuable tool used by internet and technology companies for purposes of advertising, product development, product improvement, customer acquisition, and customer retention to name a few. The data-driven decisions that result from these experiments are typically informed by null hypothesis significance tests and analyses based on p-values. However, attention has recently been drawn to the shortcomings of hypothesis testing, and an emphasis has been placed on the development of new methodologies that overcome these shortcomings. We propose the use of posterior probabilities to facilitate comparisons that account for practical equivalence and that quantify the likelihood that a result is practically meaningful, as opposed to statistically significant. We call these posterior probabilities comparative probability metrics (CPMs). This Bayesian methodology provides a flexible and intuitive means of making meaningful comparisons by directly calculating, for example, the probability that two variants are practically equivalent, or the probability that one variant is practically superior to another. In this talk we describe a unified framework for constructing and estimating such probabilities, and we discuss a sample size determination methodology that may be used to determine how much data is required to calculate trustworthy CPMs.\n\n\nBiographie\nDr. Nathaniel Stevens est professeur adjoint à l’Université de Waterloo au sein du département de statistique et de science actuarielle. Après avoir complété son doctorat en 2015 à l’Université de Waterloo, il a travaillé de 2015 à 2018 à l’Université de San Francisco, où il a développé le programme de maîtrise en science des données. Nathaniel est un spécialiste des devis expérimentaux et des tests A/B. Il est actuellement directeur du programme BISRG à Waterloo et oeuvre à l’interface de la science des données et des statistiques industrielles. Le côté novateur de ses publications a été récompensé, notamment avec le prix Søren Bisgaard en 2021 pour un article paru dans Quality Engineering. Nathaniel est actuellement président du groupe de Science des données de la Société statistique du Canada."
  },
  {
    "objectID": "presentations/archives/2020-10-09.html",
    "href": "presentations/archives/2020-10-09.html",
    "title": "The Use of Binary Choice Forests to Model and Estimate Discrete Choices",
    "section": "",
    "text": "Résumé\nWe show the equivalence of discrete choice models and the class of binary choice forests, which are random forests based on binary choice trees. This suggests that standard machine learning techniques based on random forests can serve to estimate discrete choice models with an interpretable output. This is confirmed by our data-driven theoretical results which show that random forests can predict the choice probability of any discrete choice model consistently, with its splitting criterion capable of recovering preference rank lists. The framework has unique advantages: it can capture behavioral patterns such as irrationality or sequential searches; it handles nonstandard formats of training data that result from aggregation; it can measure product importance based on how frequently a random customer would make decisions depending on the presence of the product; it can also incorporate price information and customer features. Our numerical results show that using random forests to estimate customer choices represented by binary choice forests can outperform the best parametric models in synthetic and real datasets. The paper can be downloaded from this link.\n\n\nBiographie\nNingyuan Chen is assistant professor of the Department of Management at the University of Toronto, Mississauga. He received his PhD in Operations Research from Columbia University in 2015. His research interests include revenue management and dynamic pricing, networks, and statistics."
  },
  {
    "objectID": "presentations/archives/2023-10-05.html",
    "href": "presentations/archives/2023-10-05.html",
    "title": "Strategic Sector Coupling? Market Power in Heat and Power Markets",
    "section": "",
    "text": "Résumé\nDecarbonisation of the power sector envisages vast uptake of variable renewable energy (VRE) technologies. Coupling between heat and power sectors via combined heat and power (CHP) plants could provide the flexibility needed to mitigate intermittent VRE output. However, firms with CHP plants could use the link between the two energy sectors to manipulate electricity prices. We use a bi-level model to investigate the incentives for such strategic behaviour. At the upper level, a firm with both heat-only and CHP plants determines its heat output and is constrained by power-market operations at the lower level. Such a strategic firm produces more (less) heat from its CHP (heat-only) plant vis-à-vis the social optimum in order to constrain its maximum power output. Thus, it uses its leverage to manufacture scarcity in the power market to boost the electricity price. In order to mitigate welfare losses from such strategic behaviour, we devise a regulatory package consisting of (i) a subsidy on CHP output and (ii) a modification to the heat contract. This is joint work with Sebastian Maier (University College London)\n\n\nBiographie\nDr. Afzal S. Siddiqui est titulaire d’un doctorat en génie industriel et opérations de l’Université de la Californie à Berkeley et est professeur au département de science et systèmes informatiques de l’Université de Stockholm. Il était précédemment professeur d’économie de l’énergie au département des sciences statistique de UCL et professeur affilié à Aalto et HEC Montréal. Ses intérêts de recherche sont dans l’application des méthodes de recherche opérationnelle pour l’analyse de la prise de décision dans un contexte d’incertitude et de compétition dans le secteur énergétique."
  },
  {
    "objectID": "presentations/archives/2023-05-19.html",
    "href": "presentations/archives/2023-05-19.html",
    "title": "Markov Chain-based Policies for Multi-stage Stochastic Integer Linear Programming",
    "section": "",
    "text": "Résumé\nWe introduce a novel aggregation framework to address multi-stage stochastic programs with mixed-integer state variables and continuous local variables (MSILPs). Our aggregation framework imposes additional structure to the integer state variables by leveraging the information of the underlying stochastic process, which is modeled as a Markov chain (MC). We present an exact solution method to the aggregated MSILP, which can also be used in an approximation form to obtain dual bounds and implementable feasible solutions. Moreover, we apply two-stage linear decision rule (2SLDR) approximations and propose MC-based variants to obtain high-quality decision policies with significantly reduced computational effort. We test the proposed methodologies in a novel MSILP model for hurricane disaster relief logistics planning.\n\n\nBiographie\nMerve Bodur is an Assistant Professor in the Department of Mechanical and Industrial Engineering at the University of Toronto. She also held a Dean’s Spark Professorship in the Faculty of Applied Science and Engineering (2018-2021). She is a faculty associate of University of Toronto Transportation Research Institute, Smart Freight Centre, and Centre for Healthcare Engineering at the University of Toronto. Currently, she is the INFORMS Optimization Society Vice Chair of Integer and Discrete Optimization. She obtained her Ph.D. from University of Wisconsin-Madison and did a postdoc at Georgia Institute of Technology. She received her B.S. in Industrial Engineering and B.A. in Mathematics from Bogazici University, Turkey. Her research interests include stochastic programming, integer programming, multiobjective optimization and combinatorial optimization, with applications in a variety of areas such as scheduling, transportation, power systems, healthcare and telecommunications."
  },
  {
    "objectID": "presentations/archives/2020-09-11.html",
    "href": "presentations/archives/2020-09-11.html",
    "title": "Machine Learning for Causal Inference",
    "section": "",
    "text": "Résumé\nGiven advances in machine learning over the past decades, it is now possible to accurately solve difficult non-parametric prediction problems in a way that is routine and reproducible. In this talk, I’ll discuss how machine learning tools can be rigorously integrated into observational study analyses, and how they interact with classical statistical ideas around randomization, semiparametric modeling, double robustness, etc. I’ll also survey some recent advances in methods for treatment heterogeneity. When deployed carefully, machine learning enables us to develop causal estimators that reflect an observational study design more closely than basic linear regression based methods.\n\n\nBiographie\nStefan Wager is an assistant professor of Operations, Information, and Technology at Stanford University. He completed a PhD in statistics at the same university in 2016 with Brad Efron and Guenther Walther, and spent a year as a postdoctoral researcher at Columbia University. His research focuses on adapting ideas from machine learning to statistical problems that arise in scientific applications. His research interests are broad anc include causal inference, non-parametric statistics, uses of subsampling for data analysis, and empirical Bayes methods."
  },
  {
    "objectID": "presentations/archives/2021-11-05.html",
    "href": "presentations/archives/2021-11-05.html",
    "title": "Size Distribution of Firms and Strategic Investments in Large Markets: A Stochastic Mean Field Game Approach",
    "section": "",
    "text": "Résumé\nThis paper analyzes how firm size heterogeneity distribution affects capacity investments and profits in large competitive markets with both idiosyncratic and aggregate uncertainty shocks. We use a mean field game approach where firms’ sizes are heterogeneously distributed and decisions on capacity investments (addition) or disinvestments (withdrawal) are optimally made over time to enhance prospects of earning profits under uncertainty. A mean field game framework allows for consideration of markets with a large population of interacting firms. The dynamic of capacity decisions in the market equilibrium is characterized through a fully coupled system of forward-backward stochastic differential equations (FBSDEs), one evolving forward in time and one evolving backward in time. The equilibrium behavior of firms is solved in closed form and market dashboards are used to illustrate how changes in market uncertainty shocks affect the evolution of the size distribution of firms, the market prices, and the distribution of profits. Numerical simulations of the dynamics of the stochastic competitive market with a large number of firms suggest the followings:\n\nThere is a positive relation between growth rates and the size of firms, with larger firms growing faster.\nThe equilibrium size distribution of all firms exhibits two tails.\nThe equilibrium size distribution of large firms is highly asymmetrical and skewed toward the right.\nThe size distribution for small-sized firms is highly skewed to the left.\nA bimodal shape appears in the size distribution of medium-sized firms.\nThere is a U-shaped relation between the size category and the variance of growth rates in the sense that the variance of large-sized and small-sized firms is greater than the variance of medium-sized firms.\n\n\n\nBiographie\nDr. Justin Johnson Kakeu is an Assistant Professor of Economics at University of Prince Edward Island (UPEI) in Canada. He holds a Ph.D. in Economics from University of Montreal (Canada), a Master’s in Statistics and Economics, and a Master’s in Applied Mathematics and Mechanics. Before joining the University of Prince Edward, he taught at Georgia Institute of Technology and Morehouse College in USA.\nHis research interests include Energy and Environmental Economics, Dynamic Macroeconomics, Sustainable Finance and Investing, Uncertainty in Resource and Climate Change Policies."
  },
  {
    "objectID": "presentations/archives/2024-11-22.html",
    "href": "presentations/archives/2024-11-22.html",
    "title": "Scalable Bayesian design for business innovation",
    "section": "",
    "text": "Résumé\nAbstract: Trustworthy Bayesian studies can be designed to satisfy criteria for operating characteristics of posterior analyses – such as power and the type I error rate. Such studies may inform decision-making processes in business analytics, marketing strategies, and product development. Operating characteristics for posterior analyses are typically assessed by exploring entire sampling distributions of posterior probabilities via simulation. We propose a scalable method to determine optimal sample sizes and decision criteria that maps posterior probabilities to low-dimensional conduits for the data. Our method leverages this mapping and large-sample theory to explore segments of the relevant sampling distributions. This approach prompts consistent sample size recommendations with fewer simulation repetitions than standard methods. We repurpose the posterior probabilities computed in that approach to efficiently investigate various sample sizes and decision criteria using contour plots.\n\n\nBiographie\nDr. Luke Hagar is a postdoctoral fellow at McGill University. He obtained a PhD degree in statistics in 2024 from Waterloo, completed under the supervision of Nathaniel Stevens. Luke works in the field of experimental design: his research interests revolve around efficient sampling techniques, hypothesis testing, Bayesian methods, and computational inference."
  },
  {
    "objectID": "presentations/archives/2023-10-13.html",
    "href": "presentations/archives/2023-10-13.html",
    "title": "Managing Hedge Fund Liquidity Risks",
    "section": "",
    "text": "Résumé\nWe study hedge fund optimal portfolios in the presence of market and funding liquidity risks. We consider a two-period economy with a single hedge fund. The fund has access to cash which is available every period and to an illiquid asset which pays off only at the end of the second period. Funding liquidity risk takes the form of a random proportion of the fund’s assets under management being withdrawn by clients in period one. The fund can then liquidate a part of the illiquid position by bidding on a secondary market where a random haircut on the effective selling price is applied. We solve the allocation problem of the fund and find its optimal portfolio. Whereas the cash buffer is monotonously decreasing in the secondary market liquidity, we show that the fund’s default probability is bell-shaped. Finally, we apply our model in an asset pricing framework for different hedge fund strategies to see how both risks are priced over time. This is joint work with Guillaume Roussellet (McGill University).\n\n\nBiographie\nSerge Darolles est Professeur de Finance à l’Université Paris-Dauphine où il enseigne l’économétrie de la finance et la finance empirique depuis 2012. Avant de rejoindre Dauphine, il a travaillé pour Lyxor Asset Management entre 2000 et 2012, où il a développé des modèles mathématiques pour diverses stratégies d’investissement. Il a également occupé des postes de consultants à la Caisse des Dépôts et Consignations, la Banque Paribas et le Commissariat à l’Energie Atomique. Serge est spécialisée est spécialisé en économétrie de la finance et a écrit de nombreux articles publiés dans de nombreuses revues académiques. Il est également membre du Conseil consultatif scientifique de l’AMF. Serge est titulaire d’un doctorat en mathématiques appliquées de l’Université de Toulouse et d’un DESS de l’ENSAE, Paris."
  },
  {
    "objectID": "presentations/archives/2022-12-06.html",
    "href": "presentations/archives/2022-12-06.html",
    "title": "Stochastic Algorithms in the Large",
    "section": "",
    "text": "Résumé\nIn this talk, I will present a framework, inspired by random matrix theory, for analyzing the dynamics of stochastic optimization algorithms (e.g., stochastic gradient descent (SGD) and momentum (SGD + M)) when both the number of samples and dimensions are large. Using this new framework, we show that the dynamics of optimization algorithms on a least squares problem with random data become deterministic in the large sample and dimensional limit. In particular, the limiting dynamics for stochastic algorithms are governed by a Volterra equation. From this model, we identify a stability measurement, the implicit conditioning ratio (ICR), which regulates the ability of SGD+M to accelerate the algorithm. When the batch size exceeds this ICR, SGD+M converges linearly at a rate of \\(O(1/\\sqrt{\\kappa})\\), matching optimal full-batch momentum (in particular performing as well as a full-batch but with a fraction of the size). For batch sizes smaller than the ICR, in contrast, SGD+M has rates that scale like a multiple of the single batch SGD rate. We give explicit choices for the learning rate and momentum parameter in terms of the Hessian spectra that achieve this performance. Finally we show this model matches performances on real data sets.\n\n\nBiographie\nCourtney Paquette est professeur adjointe à l’université McGill et détentrice d’une chaire en IA Canada. La recherche de Dr. Paquette est grossièrement centrée sur le devis et l’analyse d’algorithmes pour les problèmes d’optimisation à grande échelle, motivés par des applications en science des données. Courtney Paquette a obtenu un doctorat en mathématiques de l’Université de Washington en 2017, avant d’entreprendre un stage postdoctoral à l’Université Lehigh et l’Université de Waterloo, cette fois en qualité de chercheuse NSF. Elle a également occupé un poste de scientifique chez Google Research, Brain Montreal (2019-2020). Sa recherche est financée par le biais de sa chaire CIFAR, par le MILA, le CRSNG et le FRQNT."
  },
  {
    "objectID": "presentations/archives/2023-04-14.html",
    "href": "presentations/archives/2023-04-14.html",
    "title": "Efficient evaluation of prediction rules in semi-supervised settings under stratified sampling",
    "section": "",
    "text": "Résumé\nIn many contemporary applications, large amounts of unlabelled data are readily available while labelled examples are limited. There has been substantial interest in semi-supervised learning (SSL) which aims to leverage unlabelled data to improve estimation or prediction. However, current SSL literature focuses primarily on settings where labelled data are selected uniformly at random from the population of interest. Stratified sampling, while posing additional analytical challenges, is highly applicable to many real-world problems. Moreover, no SSL methods currently exist for estimating the prediction performance of a fitted model when the labelled data are not selected uniformly at random. In this paper, we propose a two-step SSL procedure for evaluating a prediction rule derived from a working binary regression model based on the Brier score and overall misclassification rate under stratified sampling. In step I, we impute the missing labels via weighted regression with nonlinear basis functions to account for stratified sampling and to improve efficiency. In step II, we augment the initial imputations to ensure the consistency of the resulting estimators regardless of the specification of the prediction model or the imputation model. The final estimator is then obtained with the augmented imputations. We provide asymptotic theory and numerical studies illustrating that our proposals outperform their supervised counterparts in terms of efficiency gain. Our methods are motivated by electronic health record (EHR) research and validated with a real data analysis of an EHR-based study of diabetic neuropathy.\n\n\nBiographie\nDr. Jessica Gronsbell est professeure adjointe au département de statistique à l’Université de Toronto. Ses intérêts de recherche sont centrés sur le développement des méthodes statistiques et d’algorithme d’apprentissage automatique pour les dossiers médicaux électroniques et les données médicales mobiles. Avant de rejoindre l’Université de Toronto, Jessica a reçu un baccalauréat en mathématiques appliquées à l’Université de la Californie (Berkeley) et un doctorat en biostatistique sous la direction de Tianxi Cai à l’Université Harvard. Elle a ensuite fait un stage postdoctoral avec Lu Tian à Stanford et passé quelques années comme scientifique des données dans le groupe Mental Health Research and Development group au sein du groupe Verily Life Sciences chez Alphabet."
  },
  {
    "objectID": "presentations/archives/2024-09-24.html",
    "href": "presentations/archives/2024-09-24.html",
    "title": "0DTE Option Pricing",
    "section": "",
    "text": "Résumé\nThe market for ultra short-tenor (zero days-to-expiry or 0DTE) options has grown exponentially over the last few years. In 2023, daily volume in 0DTEs reached over 45% of overall daily option volume. After briefly describing this exploding new market, we present a novel pricing formula designed to capture the shape of the 0DTE implied volatility surface. Pricing hinges on an Edgeworth-like expansion of the conditional characteristic function of the continuous portion of the underlying’s price process. The expansion shifts probability mass from an otherwise locally Gaussian return density by adding time-varying skewness (through leverage) and time-varying kurtosis (through the volatility-of-volatility). The expansion is local in time and, therefore, naturally suited to price ultra short-tenor instruments, like 0DTEs. We document considerable (1) price and (2) hedging improvements as compared to state-of-the-art specifications. We conclude by providing suggestive results on nearly instantaneous predictability by estimating 0DTE-based return/variance risk premia. This is joint work with Federico M. Bandi and Nicola Fusari. The paper is available at SSRN.\n\n\nBiographie\nRoberto Renò is Professor at the IDS Department (Information Systems, Decision Sciences and Statistics) at ESSEC Business School. He is a former professor Quantitative Finance at the University of Siena. He holds a PhD in Financial Mathematics at Scuola Normale Superiore in Pisa, and a Degree in Physics at the University of Pisa. His research focuses on various aspects of econometrics and finance, with specific contributions in asset pricing, high frequency financial econometrics, nonparametric statistics."
  },
  {
    "objectID": "presentations/archives/2022-10-28.html",
    "href": "presentations/archives/2022-10-28.html",
    "title": "Computer Model Emulation Using Deep Gaussian Processes",
    "section": "",
    "text": "Résumé\nComputer models are often used to explore physical systems. Increasingly, there are cases where the model is fast, the code is not readily accessible to scientists, but a large suite of model evaluations is available. In these cases, an \"emulator\" is used to stand in for the computer model. This work was motivated by a simulator for the chirp mass of binary black hole mergers where no output is observed for large portions of the input space and more than \\(10^6\\) simulator evaluations are available. This poses two problems: (i) the need to address the discontinuity when observing no chirp mass; and (ii) performing statistical inference with a large number of simulator evaluations. The traditional approach for emulation is to use a stationary Gaussian process (GP) because it provides a foundation for uncertainty quantification for deterministic systems. We explore the impact of the choices when setting up the deep GP on posterior inference, apply the proposed approach to the real application and propose a sequential design approach for identifying new simulations.\n\n\nBiographie\nDerek Bingham est professeur titulaire et directeur du département de statistique et de sciences actuarielles de l’Université Simon Fraser à Vancouver, où il a complété en 1999 un doctorat en statistique sous la gouverne de Randy Sitter. Dr Bingham est un spécialiste des devis expérimentaux, et travaille sur la calibration, l’émulation et la quantification de l’incertitude pour des modèles Bayésiens. Sa recherche se concentre actuellement sur le développement de modèles statistiques permettant de combiner des observations de modèles physiques avec des simulations par ordinateur à grande échelle. Derek a été professeur au département de statistique de l’Université de Michigan et titulaire d’une chaire de recherche du Canada niveau 2 en statistique industrielle de 2003 à 2013. Il a été le récipiendaire du prix CRM-SSC en statistique en 2013."
  },
  {
    "objectID": "presentations/archives/2025-01-24.html",
    "href": "presentations/archives/2025-01-24.html",
    "title": "A Generalized Logrank-type Test for Comparison of Treatment Regimes in Sequential Multiple Assignment Randomized Trials",
    "section": "",
    "text": "Résumé\nThe sequential multiple assignment randomized trial (SMART) is the gold standard study design for the evaluation of multistage treatment regimes, which comprise sequential decision rules that recommend treatments for a patient at each of a series of decision points based on their evolving characteristics. A common goal is to compare the set of so-called embedded regimes represented in the design on the basis of a primary outcome of interest. In the study of chronic diseases and disorders, this outcome is often a time to an event, and a goal is to compare the distributions of the time-to-event outcome associated with each regime in the set. We present a general statistical framework in which we develop a logrank-type test for comparison of the survival distributions associated with regimes within a specified set based on the data from a SMART with an arbitrary number of stages that allows incorporation of covariate information to enhance efficiency and can also be used with data from an observational study. The framework provides clarification of the assumptions required to yield a principled test procedure, and the proposed test subsumes or offers an improved alternative to existing methods. The methods are applied to a SMART in patients with acute promyelocytic leukemia. This is joint work with Anastasios A. Tsiatis.\n\n\nBiographie\nMarie Davidian is J. Stuart Hunter Distinguished Professor of Statistics at North Carolina State University (NC State) and Adjunct Professor of Biostatistics and Bioinformatics at Duke University. Her research interests include methodology for the design and analysis of clinical trials and observational studies, for causal inference and dynamic treatment regimes, for analysis of missing and mismeasured data, and for longitudinal data analysis. Marie is a Fellow of the American Statistical Association (ASA), Institute of Mathematical Statistics (IMS), and the American Association for the Advancement of Science, and an elected member of the International Statistical Institute. Marie has received several awards and recognitions, including the ASA Award for Outstanding Statistical Application (1993), the Janet L. Norwood Award for Outstanding Achievement by a Woman in the Statistical Science, the Committee of Presidents of Statistical Societies (COPSS) Florence Nightingale David Award, the COPSS George W. Snedecor Award, the ASA Founders Award, and the Marvin Zelen Leadership Award in Statistical Science from the Harvard T. H. Chan School of Public Health; as well as honorary life memberships in the IMS and Interational Biometric Society. She has been Coordinating and Executive Editor of the journal Biometrics, served on several US Food and Drug Administration Advisory Committees, and served on several US National Institutes of Health (NIH) grant review panels, including as Chair of the Biostatistical Methods and Research Design Study Section. Marie has published over 135 refereed papers and several books. She has supervised over 30 PhD students. For two decades, Marie was Program Director for the Summer Institute for Biostatistics (SIBS), which introduced talented US undergraduates to careers in biostatistics and encouraged them to pursue graduate training; and an NIH predoctoral training grant, which placed PhD students at NC State and Duke with cardiovascular disease research projects at Duke Clinical Research Institute, providing them hands-on collaborative experience in health sciences research."
  },
  {
    "objectID": "presentations/archives/2023-01-23.html",
    "href": "presentations/archives/2023-01-23.html",
    "title": "Records Analysis in Climate Attribution",
    "section": "",
    "text": "Résumé\nNumerical climate models are complex and combine a large number of physical processes. They are key tools in quantifying the relative contribution of potential anthropogenic causes (e.g., the current increase in greenhouse gases) on high-impact atmospheric variables like heavy rainfall or temperatures. These so-called climate extreme event attribution problems are particularly challenging in a multivariate context, that is, when the atmospheric variables are measured on a possibly high-dimensional grid. In addition, global climate models like any in sillico numerical experiments are affected by different types of bias. In this talk, I will discuss about how to combine to two statistical theories to assess causality in the context of extreme event attribution. In addition, the question of uncertainties quantification that remains a challenge in any climate attribution analysis will be explored from various directions. In particular, a simple model bias correction step for records will described in details. To illustrate our approach, we infer emergence times in precipitation from the CMIP5 and CMIP6 archives.\nThis is joint work with Anna Kiriliouk, Paula Gonzalez Soulivanh Thao and Julien Worms.\n\n\nBiographie\nPhilippe Naveau est chercheur scientifique senior au Laboratoire des Sciences du Climat et de l’Environnement du CNRS à Gif-sur-Yvette (Saclay, France). Il travaille notamment sur la détection et l’attribution d’événements extrêmes. Phillipe a obtenu un doctorat en statistique de Colorado State University en 1998 sous la direction de R. Tweedie et R. Davis et a travaillé à University of Boulder et NCAR avant de rejoindre le CNRS."
  },
  {
    "objectID": "presentations/archives/2025-03-21.html",
    "href": "presentations/archives/2025-03-21.html",
    "title": "Investigating Two-Stage Robust Optimization and Distributionally Robust Optimization from the Primal Perspective: A Complete and Intuitive Solution Framework",
    "section": "",
    "text": "Résumé\nRobust optimization (RO) and distributionally robust optimization (DRO), as relatively new optimization schemes, have been adopted in many practical systems (e.g., power, logistics and healthcare systems) to support their design, operations, and reliabilities. Conventionally, due to the sophisticated and nested min-max structures, two-stage RO and DRO are often studied using duality-based techniques, aiming to simplify their structures and obtain monolithic reformulations. Nevertheless, research developed from such dual perspective is rather abstract and technically demanding, which is less friendly to build intuitive understanding.\nIn this talk, unlike existing research, we take the primal perspective to analyze RO and DRO, and directly make use of their primal structures to develop computational algorithms. The resulting column-and-constraint generation algorithm and its variants are, overall, simple, intuitive, and application-friendly. Actually, they often drastically outperform existing solution methods. Extensions to handle decision-dependent uncertainty (DDU), which is closely related to the phenomenon of the “induced demand”, will also be discussed. Demonstrations in logistics, production, and energy systems, along with computational results and managerial insights, are presented to help us appreciate RO and DRO and those solution methods in practice.\n\n\nBiographie\nDr. Zeng is an Associate Professor of Industrial Engineering in the Swanson School of Engineering at the University of Pittsburgh where he teaches and conducts research discrete and robust optimization, with applications in logistics, energy, and healthcare systems. Prior to that, Bo obtained his PhD from Purdue in 2007 and worked as an assistant professor of Industrial and Management Systems Engineering at the University of South Florida, from 2009 to 2015.\nThrough his research, Dr. Zeng has developed several analytical operational models and algorithms (e.g., the basic column-and-constraint generation method and its variants) that have been extensively applied in energy, logistics and other critical infrastructure systems, to address real design and operational issues and to hedge against risks and to achieve better reliability and security. He is a professional member of IISE, INFORMS and IEEE."
  },
  {
    "objectID": "presentations/archives/2025-01-31.html",
    "href": "presentations/archives/2025-01-31.html",
    "title": "Robust Loss Reserve Prediction: A Comparative Analysis of Modern Statistical and Machine Learning Methods",
    "section": "",
    "text": "Résumé\nIn the property and casualty (P&C) and health insurance industries, unpaid losses represent the largest liability on insurers’ balance sheets. These losses are typically organized into loss triangles, where rows correspond to accident years, columns denote development years, and cells contain cumulative paid losses up to a given calendar year. The lower triangle represents unpaid losses and requires accurate prediction to ensure solvency and effective risk management. Traditional methods focus on grouped lines of business (LOBs) with similar development patterns, aggregating these predictions to compute the overall reserve. However, such methods often ignore complex dependencies across LOBs—such as shared trends, cross-sectional correlations, and macroeconomic factors like inflation or strategic decisions—leading to non-robust predictions and unstable estimates of risk capital.\nIn this talk, I will present advancements in using recurrent neural networks (RNNs) and generative adversarial networks (GANs) to tackle these challenges. RNNs excel at capturing complex dependencies in loss sequences, while a copula-based GAN generates synthetic data that mirrors the distributional properties of observed loss triangles. These techniques demonstrate significant improvements in predictive accuracy and stability over traditional methods.\nI will also discuss our progress in enhancing seemingly unrelated regression (SUR) models for reserve and risk capital prediction. By incorporating mixed effects to account for heterogeneity across companies and using copulas to model dependencies between LOBs, we improve the robustness of reserve and risk capital predictions. In addition, I will outline an estimation procedure that combines maximum likelihood and residual ranks to refine model parameters. This highlights the potential of a hybrid approach integrating RNNs with a SUR copula mixed model for robust reserve prediction. This is a joint work with Dr. Anas Abdallah and Pengfei Cai.\n\n\nBiographie\nDr. Pratheepa Jeganathan est professeur adjointe au départmeent de mathématiques et de statisitque de l’Unversité McMaster. Ses intérêts de recherche sont en lien avec l’aprentissage multitâche, les modèles génératifs, les résaux de neurones et la modélisation de données dépendantes. Pratheepa a complété un doctorat à l’Université Texas Tech et a effectué un stage postdoctoral à Stanford avec Susan Holmes.\nDr. Jeganathan’s research focuses on developing statistical and computational methods to analyze multi-domain data, especially addressing statistical challenges in microbiome multi-omic and spatial multi-omic data analysis. Current research includes multi-table integration, preprocessing and transformation of high-throughput sequencing data, visualization, hierarchical modeling, Bayesian statistics, statistical inference, block bootstrap method, data mining, and approximation theory in statistics."
  },
  {
    "objectID": "presentations/archives/2024-10-15.html",
    "href": "presentations/archives/2024-10-15.html",
    "title": "Inequality in Discrete Choice Models: Applications in the Mobility Context",
    "section": "",
    "text": "Résumé\nPublic policies are often evaluated using cost-benefit analysis, but their acceptability also hinges on the distribution of winners and losers. In this presentation, we assess inequality within a discrete choice model framework, starting with theoretical insights into how equity can be defined and measured using tools such as the Lorenz curve and the GINI coefficient. We also briefly examine the influence of heterogeneity on welfare outcomes. Applications in the mobility sector are discussed, mainly through the dynamic traffic model METROPOLIS. We briefly introduce the model and clarify its link to the W. Vickrey theoretical approach. Finally, we use this framework to evaluate two policies: Low Emission Zones (LEZ) and dynamic carpooling.\n\n\nBiographie\nDr. André de Palma (Ph.D., HDR) est professeur titulaire de classe exceptionnelle à CY Cergy Paris Université, spécialisé dans l’économie du transport. Il est détenteur d’un doctorat en physique de l’Université Libre de Bruxelles et d’un doctorat en économie de l’Université de Bourgogne et d’une HDR en économie. Il est l’auteur de quatre ouvrages de référence, quatre livres à titre d’éditeur et il a publié près de 250 articles dans des revues arbitrés. Ses domaines d’expertises incluent l’économie urbaine et de transport, le finance bancaire et comportementale, l’organisation industrielle, les modèles à choix discrets et la théorie de la décision."
  },
  {
    "objectID": "presentations/archives/2020-11-05.html",
    "href": "presentations/archives/2020-11-05.html",
    "title": "Developing an Open Energy Outlook for the United States Using Tools for Energy Model Optimization and Analysis (Temoa)",
    "section": "",
    "text": "Résumé\nClimate change coupled with rapid technological innovation is driving large scale change in the global energy system. Computer models of the energy system – referred to as energy optimization system models – provide a way to examine future energy system evolution, test the effects of proposed policy, and explore the role of future uncertainty. In this talk, I will describe Tools for Energy Model Optimization and Analysis (Temoa), an open source energy system optimization model designed to conduct rigorous uncertainty analysis. Temoa is currently being used to develop an Open Energy Outlook for the United States, which aims to apply best practices of policy-focused energy system modeling, ensure transparency, build a networked community, and work towards a common purpose: examining possible U.S. energy system futures to inform energy and climate policy efforts.\n\n\nBiographie\nDr. DeCarolis is a Professor in the Department of Civil, Construction, and Environmental Engineering at North Carolina State University. His research program is focused on addressing energy and environmental challenges at the intersection of engineering, economics, and public policy. His primary focus is the development and application of energy system models to analyze how energy technology and public policy can shape a sustainable future. He received his PhD in Engineering and Public Policy from Carnegie Mellon University in 2004 and is the recipient of an NSF CAREER Award."
  },
  {
    "objectID": "presentations/archives/2024-12-05.html",
    "href": "presentations/archives/2024-12-05.html",
    "title": "Biais de sélection et assurance : une structure de prime équitable pour un portefeuille est-elle globalement équitable sur le marché?",
    "section": "",
    "text": "Résumé\nL’équité est centrée sur les personnes. Dans le domaine de l’assurance, elle devrait s’appliquer à l’ensemble de la population assurée, et non seulement aux clients d’un assureur. Toutefois, le portefeuille de chaque compagnie représente un sous-échantillon souvent non-représentatif. Un modèle ajusté sur ces données, empreintes d’un biais de sélection, se généralise mal à l’ensemble des assurés. Deux biais découlent de la composition du portefeuille : le biais de représentation, lorsque des erreurs importantes sont commises sur les prévisions pour les individus appartenant à des sous-populations rarement observées, et le biais de sélection, lorsque les pratiques de souscription des risques et de marketing rendent le portefeuille différent de la population assurée. Nous examinons comment la composition du portefeuille affecte les méthodologies de primes équitables pour atténuer la discrimination directe et indirecte sur la base d’un attribut protégé. Nous illustrons comment l’atténuation des iniquités basée sur des portefeuilles biaisés ne produit pas un marché équitable pour les assurés. En nous appuyant sur l’inférence causale et sur un indicateur de composition du portefeuille, nous décrivons le mécanisme de sélection et identifions les conditions pour lesquelles chaque biais affecte les primes équitables selon différentes méthodologies. Enfin, nous proposons une méthode pour corriger le biais des primes équitables calculées sur un portefeuille spécifique, en utilisant une estimation sans biais (fournie par un tiers) de la distribution des attributs interdits. Cette approche atténue efficacement le biais de sélection mais conduit à des primes globales qui ne sont pas équilibrées sur le portefeuille.\n\n\n\nBiographie\nMarie-Pier Côté est professeure agrégée à l’École d’actuariat de l’Université Laval depuis 2018, et titulaire de la Chaire de leadership en enseignement en analyse de données massives pour l’actuariat - Intact. Elle détient un doctorat en statistique de l’Université McGill (2018) et est membre Fellow de la Société des actuaires. Ses travaux de recherche sur l’apprentissage statistique en actuariat lui ont valu le prix du meilleur article publié en 2021 dans le North American Actuarial Journal et le prix Patrick Brockett & Arnold Shapiro Award de l’American Risk and Insurance Association. Elle s’intéresse notamment à l’équité algorithmique en actuariat et à la modélisation statistique de la dépendance et des risques actuariels."
  },
  {
    "objectID": "presentations/archives/2019-09-10.html",
    "href": "presentations/archives/2019-09-10.html",
    "title": "Optimal picking policies for e-commerce warehouses",
    "section": "",
    "text": "Résumé\nIn e-commerce warehouses, online retailers increase their efficiency by using a mixed-shelves (or scattered storage) concept, where unit loads are purposefully broken down into single items, which are individually stored in multiple locations. Irrespective of the stock keeping units a customer jointly orders, this storage strategy increases the likelihood that somewhere in the warehouse the items of the requested stock keeping units will be in close vicinity, which may significantly reduce an order picker’s unproductive walking time. In this talk, we optimize picker routing through such mixed-shelves warehouses. Specifically, we introduce a generic exact algorithmic framework that covers a multitude of picking policies, independently of the underlying picking zone layout, and is suitable for real-time applications. This framework embeds a bidirectional layered graph algorithm which provides the best known performance for the simple picking problem with a single depot and no further attributes. We compare three different real-world e-commerce warehouse settings that differ slightly in their application of scattered storage and in their picking policies. Based on these, we derive additional layouts and settings that yield further managerial insights. Our results reveal that the right combination of drop-off points, dynamic batching, the utilization of picking carts, and the picking zone layout can greatly improve the picking performance. In particular, some combinations of policies yield efficiency increases of more than 20%.\n\n\nBiographie\nMaximilian Schiffer is an Assistant Professor of Operations and Supply Chain Management at TUM School of Management, Technical University of Munich. Before joining TU Munich, he was a Visiting Postdoctoral Scholar at Stanford University and a Postdoctoral Scholar at RWTH Aachen University. Dr. Schiffer is an Associate Member of the GERAD. He received a Ph.D. degree in Operations Research from RWTH Aachen University in 2017. Dr. Schiffer’s expertise lies in in the field of Operations Research and Management Science, specifically in Dynamic Programming, (Mixed) Integer Programming, Metaheuristics, Exacts, Robust Optimization, Machine Learning, and Forecasting, applied to a variety of complex application fields, e.g., Transportation Problems, Supply Chains, Production Networks, and Big Data. His research on electric vehicles and logistics networks with intermediate stops has been awarded with numerous prizes, e.g., the INFORMS TSL Dissertation Prize and the GOR Doctoral Dissertation Prize."
  },
  {
    "objectID": "presentations/actuel/2025-08-19.html",
    "href": "presentations/actuel/2025-08-19.html",
    "title": "# CANCELLED # Robust Mean Estimation for Optimization: The Impact of Heavy Tails",
    "section": "",
    "text": "Résumé\n\n\nCANCELLED # Most data-driven decisions formulations in the literature explicitly assume bounded or light-tailed distributions. However, many real-world phenomena exhibit heavy-tailed distributions, characterized by rare but extreme events with may have significant impact. In this work, we investigate the performance of sample average approximation and Wasserstein DRO and show that neither offer adequate protection when the associated losses are bounded from right but regularly varying heavy-tailed from the left. Surprisingly, if the data has finite variance, classical variance regularization does offer such protection but we show that it is generally conservative. Finally, we show that a judiciously scaled Kullback-Leibler DRO is statistically efficient. We do so by developing an upper bound on the probability that the KL DRO decision disappoints out-of-sample (of independent interest) and indicate that it matches a statistical lower bound asymptotically obtained through a change-of-measure argument.\n\n\nBiographie\n\n\nCANCELLED # Bart Van Parys is an associate professor in the stochastics group at the national research institute for mathematics and computer science (CWI) in Amsterdam, the Netherlands. His research is located on the interface between optimization and machine learning. In particular, he develops novel mathematical methodologies and algorithms with which we can turn data into better decisions. Although most of his research is methodological, he does enjoy applications related to problems in renewable energy."
  },
  {
    "objectID": "presentations/brownbag/PHD_seminars_2.html",
    "href": "presentations/brownbag/PHD_seminars_2.html",
    "title": "Quality Issues of Implied Volatilities of Index and Stock Options in the OptionMetrics IvyDB Database",
    "section": "",
    "text": "Subject\nSebastien will be presenting the paper Quality Issues of Implied Volatilities of Index and Stock Options in the OptionMetrics IvyDB Database by Martin Wallmeier. This paper discusses problems related to the OptionMetrics database and how to solve them."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Séminaires à venir",
    "section": "",
    "text": "Nos séminaires sont webdiffusés via la plateforme Zoom. Remplissez le formulaire suivant pour recevoir les informations de connexion par courriel.\nOur seminars are broadcasted live on Zoom. You can sign-up using this form to get Zoom credentials by email.\n\n\n\n\n\n\nDate\n\n\n\nHeure\n\n\n\nInvité(e)\n\n\n\nTitre de l'exposé\n\n\n\nLocal\n\n\n\n\n\n\n\n\njeu., 13 nov. 2025\n\n\n13h30\n\n\nAlan de Genaro\n\n\nProduct Complexity, Investor Experience, and Returns\n\n\nCôte-Sainte-Catherine 1er, Bleu, Hélène-Desmarais) (https://www.hec.ca/campus/cote_sainte_catherine/1er_etage/salles_cours/hdesmarais.html)\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "archives.html",
    "href": "archives.html",
    "title": "Séminaires passés",
    "section": "",
    "text": "Date\n\n\n\nInvité(e)\n\n\n\nTitre de l'exposé\n\n\n\n\n\n\n\n\n3 juin 2025\n\n\nAnastasis Kratsios\n\n\nUniversal Arbitrage-Free Volatility Surfaces\n\n\n\n\n\n\n23 avr. 2025\n\n\nSamuel Burer\n\n\nAn Extended Validity Domain for Constraint Learning\n\n\n\n\n\n\n15 avr. 2025\n\n\nOrla A. Murphy\n\n\nEstimating Extreme Wave Surges in the Presence of Missing Data\n\n\n\n\n\n\n26 mars 2025\n\n\nJean-François Bégin\n\n\nA general option pricing framework for affine fractionally integrated models\n\n\n\n\n\n\n21 mars 2025\n\n\nBo Zeng\n\n\nInvestigating Two-Stage Robust Optimization and Distributionally Robust Optimization from the Primal Perspective: A Complete and Intuitive Solution Framework\n\n\n\n\n\n\n10 mars 2025\n\n\nXiaofei Shi\n\n\nA Dynamic Equilibrium Model of Liquidity Risk\n\n\n\n\n\n\n21 févr. 2025\n\n\nGenevera Allen\n\n\nInference for Interpretable Machine Learning: Feature Importance and Beyond\n\n\n\n\n\n\n31 janv. 2025\n\n\nPratheepa Jeganathan\n\n\nRobust Loss Reserve Prediction: A Comparative Analysis of Modern Statistical and Machine Learning Methods\n\n\n\n\n\n\n24 janv. 2025\n\n\nMarie Davidian\n\n\nA Generalized Logrank-type Test for Comparison of Treatment Regimes in Sequential Multiple Assignment Randomized Trials\n\n\n\n\n\n\n16 janv. 2025\n\n\nGuillaume A. Pouliot\n\n\nPlacebo Tests Done Right\n\n\n\n\n\n\n5 déc. 2024\n\n\nMarie-Pier Côté\n\n\nBiais de sélection et assurance : une structure de prime équitable pour un portefeuille est-elle globalement équitable sur le marché?\n\n\n\n\n\n\n22 nov. 2024\n\n\nLuke Hagar\n\n\nScalable Bayesian design for business innovation\n\n\n\n\n\n\n11 nov. 2024\n\n\nSibel Alumur Alev\n\n\nLocating Facilities for Smart and Sustainable Mobility\n\n\n\n\n\n\n18 oct. 2024\n\n\nBart Van Parys\n\n\nDisciplined Decisions in the Face of Uncertainty and Data\n\n\n\n\n\n\n15 oct. 2024\n\n\nAndré de Palma\n\n\nInequality in Discrete Choice Models: Applications in the Mobility Context\n\n\n\n\n\n\n11 oct. 2024\n\n\nAfzal Siddiqui\n\n\nTransmission planning in an imperfectly competitive power sector with environmental externalities\n\n\n\n\n\n\n24 sept. 2024\n\n\nRoberto Renò\n\n\n0DTE Option Pricing\n\n\n\n\n\n\n29 août 2024\n\n\nLuca Trapin\n\n\nHigh-dimensional approximate dynamic matrix factor models: Estimation via the Kalman smoother and the EM algorithm\n\n\n\n\n\n\n30 avr. 2024\n\n\nLucia Sbragia\n\n\nInternational environmental agreements in the presence of adaptation and self-image\n\n\n\n\n\n\n12 avr. 2024\n\n\nTim Schittekatte\n\n\nDesigning Distribution Network Tariffs Under Increased Residential End-user Electrification: Can the North America Learn Something from Europe?\n\n\n\n\n\n\n11 avr. 2024\n\n\nMichael Grabisch\n\n\nOn the design of public debate in social networks\n\n\n\n\n\n\n5 avr. 2024\n\n\nNicholas Rivers\n\n\nEvaluating a major home energy efficiency retrofit program\n\n\n\n\n\n\n15 mars 2024\n\n\nDaniele Durante\n\n\nSkew-symmetric approximations of posterior distributions\n\n\n\n\n\n\n23 févr. 2024\n\n\nVeronika Ročková\n\n\nAdaptive Bayesian predictive inference\n\n\n\n\n\n\n16 févr. 2024\n\n\nJim Luedtke\n\n\nRecent Advances in Methods for Solving Stochastic Integer Programming Problems\n\n\n\n\n\n\n26 janv. 2024\n\n\nTrevor Campbell\n\n\nAn exploration-agnostic characterization of the ergodicity of parallel tempering\n\n\n\n\n\n\n20 déc. 2023\n\n\nMario Ghoussoub\n\n\nEquilibria in Centralized Insurance Markets: Monopolistic vs. Competitive Pricing\n\n\n\n\n\n\n24 nov. 2023\n\n\nLeilei Zeng\n\n\nState-dependent Sampling in Observational Cohort Studies\n\n\n\n\n\n\n10 nov. 2023\n\n\nRenyuan Xu\n\n\nLearning to Simulate Tail-risk Scenarios\n\n\n\n\n\n\n13 oct. 2023\n\n\nSerge Darolles\n\n\nManaging Hedge Fund Liquidity Risks\n\n\n\n\n\n\n11 oct. 2023\n\n\nBahar Yetis Kara\n\n\nDisaster Resilient Cities: An OR Approach to Disaster Management\n\n\n\n\n\n\n5 oct. 2023\n\n\nAfzal S. Siddiqui\n\n\nStrategic Sector Coupling? Market Power in Heat and Power Markets\n\n\n\n\n\n\n29 sept. 2023\n\n\nRyan Campbell\n\n\nStatistical inference for multivariate extremes via a geometric approach\n\n\n\n\n\n\n12 sept. 2023\n\n\nLambert De Monte\n\n\nMultivariate extremes – A geometric Bayesian inference approach\n\n\n\n\n\n\n19 mai 2023\n\n\nMerve Bodur\n\n\nMarkov Chain-based Policies for Multi-stage Stochastic Integer Linear Programming\n\n\n\n\n\n\n18 mai 2023\n\n\nTamer Başar\n\n\nConsensus and Dissensus in Multi-population Multi-agent Systems\n\n\n\n\n\n\n16 mai 2023\n\n\nJocelyn Martel\n\n\nInstitutional Ownership and the Resolution of Financial Distress\n\n\n\n\n\n\n24 avr. 2023\n\n\nBenjamin Shaby\n\n\nSpatial scale-aware tail dependence modeling for high-dimensional spatial extremes\n\n\n\n\n\n\n14 avr. 2023\n\n\nJessica Gronsbell\n\n\nEfficient evaluation of prediction rules in semi-supervised settings under stratified sampling\n\n\n\n\n\n\n17 mars 2023\n\n\nMohamad Elmasri\n\n\nPredictive inference for travel time on transportation networks\n\n\n\n\n\n\n24 févr. 2023\n\n\nGah-Yi Ban\n\n\nPersonalized Dynamic Pricing with Machine Learning: High Dimensional Features and Heterogeneous Elasticity\n\n\n\n\n\n\n17 févr. 2023\n\n\nSébastien Laurent\n\n\nAutoregressive conditional betas\n\n\n\n\n\n\n8 févr. 2023\n\n\nKris Boudt\n\n\nCourt Shopping, Pro-Debtor Bias, and Bankruptcy Outcomes\n\n\n\n\n\n\n3 févr. 2023\n\n\nKatya Scheinberg\n\n\nOverview of adaptive stochastic optimization methods.\n\n\n\n\n\n\n23 janv. 2023\n\n\nPhilippe Naveau\n\n\nRecords Analysis in Climate Attribution\n\n\n\n\n\n\n20 janv. 2023\n\n\nRosnel Sessinou\n\n\nPrecision Least Squares: Estimation and Inference in High-Dimensional Linear Regression Models\n\n\n\n\n\n\n6 déc. 2022\n\n\nCourtney Paquette\n\n\nStochastic Algorithms in the Large\n\n\n\n\n\n\n25 nov. 2022\n\n\nRebecca Steorts\n\n\n(Almost) All of Entity Resolution\n\n\n\n\n\n\n11 nov. 2022\n\n\nRoxana Dumitrescu\n\n\nOptimal stopping mean-field games: a linear programming formulation and applications to entry-exit games in electricity markets\n\n\n\n\n\n\n2 nov. 2022\n\n\nMarie Lambert\n\n\nEmployee Views of Leveraged Buy-Out Transactions\n\n\n\n\n\n\n28 oct. 2022\n\n\nDerek Bingham\n\n\nComputer Model Emulation Using Deep Gaussian Processes\n\n\n\n\n\n\n14 oct. 2022\n\n\nFrédéric Vrins\n\n\nOptimal and robust combination of forecasts via constrained optimization and shrinkage\n\n\n\n\n\n\n10 juin 2022\n\n\nLeopoldo Catania\n\n\nThe Leverage Effect and Propagation\n\n\n\n\n\n\n10 mai 2022\n\n\nTias Guns\n\n\nLearning and reasoning with constraint solving\n\n\n\n\n\n\n10 mai 2022\n\n\nEmanuele Guidotti\n\n\nEfficient Estimation of Bid-Ask Spreads from Open, High, Low, and Close Prices\n\n\n\n\n\n\n28 avr. 2022\n\n\nMargarida Carvalho\n\n\nToward social welfare and fairness in kidney exchange programs\n\n\n\n\n\n\n23 mars 2022\n\n\nNathaniel Stevens\n\n\nComparative Probability Metrics: Using Posterior Probabilities to Account for Practical Equivalence in A/B tests\n\n\n\n\n\n\n25 févr. 2022\n\n\nMarco Avella Medina\n\n\nDifferentially private inference via noisy optimization\n\n\n\n\n\n\n5 nov. 2021\n\n\nJustin Johnson Kakeu\n\n\nSize Distribution of Firms and Strategic Investments in Large Markets: A Stochastic Mean Field Game Approach\n\n\n\n\n\n\n29 oct. 2021\n\n\nTiffany Timbers\n\n\nOpinionated practices for teaching reproducibility: motivation, guided instruction and practice\n\n\n\n\n\n\n4 oct. 2021\n\n\nLucy Gao\n\n\nSelective inference on trees\n\n\n\n\n\n\n18 mai 2021\n\n\nBethany White, Jastaranpreet Singh\n\n\nRethinking Introductory Statistics for Life Sciences Programs\n\n\n\n\n\n\n6 mai 2021\n\n\nSanjiv R. Das, Daniel N. Ostrov\n\n\nDynamic Optimization for Multi-Goals-Based Wealth Management\n\n\n\n\n\n\n7 avr. 2021\n\n\nRégis Chevanaz\n\n\nWhen does eco-efficiency rebound or backfire? An analytical model\n\n\n\n\n\n\n17 mars 2021\n\n\nStephen P. Boyd\n\n\nConvex Optimization\n\n\n\n\n\n\n19 févr. 2021\n\n\nEmily Hector\n\n\nJoint integrative analysis of multiple data sources with correlated vector outcomes\n\n\n\n\n\n\n22 janv. 2021\n\n\nEmanuele Guidotti\n\n\nAsymptotic Expansion Formulas for Diffusion Processes Based on the Perturbation Method\n\n\n\n\n\n\n4 déc. 2020\n\n\nBruno Feunou\n\n\nRisk-neutral moments based estimation of continuous time jump-diffusion models\n\n\n\n\n\n\n27 nov. 2020\n\n\nIlze Kalnina\n\n\nHigh-Frequency Factor Models and Regressions\n\n\n\n\n\n\n5 nov. 2020\n\n\nJoseph DeCarolis\n\n\nDeveloping an Open Energy Outlook for the United States Using Tools for Energy Model Optimization and Analysis (Temoa)\n\n\n\n\n\n\n23 oct. 2020\n\n\nMadhu Kalimipalli\n\n\nBanking networks, systemic risk, and the credit cycle in emerging markets\n\n\n\n\n\n\n16 oct. 2020\n\n\nOzlem Ergun\n\n\nOptimizing Post-Disruption Response Operations to Improve Resilience of Critical Infrastructure Systems\n\n\n\n\n\n\n9 oct. 2020\n\n\nNingyuan Chen\n\n\nThe Use of Binary Choice Forests to Model and Estimate Discrete Choices\n\n\n\n\n\n\n11 sept. 2020\n\n\nStefan Wager\n\n\nMachine Learning for Causal Inference\n\n\n\n\n\n\n5 mars 2020\n\n\nLinda Mhalla\n\n\nTail risk and style dependence in the fund industry: A multivariate extreme value approach\n\n\n\n\n\n\n31 janv. 2020\n\n\nAna-Maria Staicu\n\n\nLongitudinal functional regression: tests of significance\n\n\n\n\n\n\n9 janv. 2020\n\n\nMu Zhu\n\n\nSome statistical applications of generative neural networks\n\n\n\n\n\n\n29 nov. 2019\n\n\nAhmed Abdulla\n\n\nLife at the (grid edge): Evaluating energy decentralization strategies in California\n\n\n\n\n\n\n16 oct. 2019\n\n\nKeven Bluteau\n\n\nMedia and the stock market: Their relationship and abnormal dynamics around earnings announcements\n\n\n\n\n\n\n11 oct. 2019\n\n\nKris Boudt\n\n\nOptimizing the design of textual indices: Impression management and Sentometrics\n\n\n\n\n\n\n10 sept. 2019\n\n\nMaximilian Schiffer\n\n\nOptimal picking policies for e-commerce warehouses\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "brownbag.html",
    "href": "brownbag.html",
    "title": "Causeries",
    "section": "",
    "text": "Les causeries aux cycles supérieurs sont organisés par les étudiant(e)s du doctorat et de la maîtrise en finance et en ingénierie financière. Pour information, contactez l’organisateur Étienne Bacon.\n\n\n\n\n\n\nDate\n\n\n\nHeure\n\n\n\nInvité(e)\n\n\n\nTitre de l'exposé\n\n\n\nLocal\n\n\n\n\n\n\n\n\nven., 20 oct. 2023\n\n\n13h00\n\n\nSebastien Legros\n\n\nQuality Issues of Implied Volatilities of Index and Stock Options in the OptionMetrics IvyDB Database\n\n\nsalle Cogeco\n\n\n\n\n\n\nven., 20 oct. 2023\n\n\n13h30\n\n\nEtienne Bacon\n\n\nOn general semi-closed-form solutions for VIX derivative pricing\n\n\nsalle Cogeco\n\n\n\n\n\n\nven., 24 nov. 2023\n\n\n13h00\n\n\nCéleste Hardy\n\n\nFund flows and investor concerns\n\n\nsalle Rona\n\n\n\n\n\n\nven., 24 nov. 2023\n\n\n13h30\n\n\nCarlotta Pacifici\n\n\nDynamic tail risk estimation using extreme value theory\n\n\nsalle Rona\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "presentations/brownbag/PHD_seminars_1.html",
    "href": "presentations/brownbag/PHD_seminars_1.html",
    "title": "On general semi-closed-form solutions for VIX derivative pricing",
    "section": "",
    "text": "Subject\nMost pricing methods for VIX futures and European VIX options rely on the existence of the squared VIX moment generating function. Yet this function does not exist for some state-of-the-art option pricing models, which prevents their widespread use. This paper presents semi-closed form solutions for VIX futures and European VIX option prices that rely on the characteristic functions of the squared VIX. These pricing formulas are applicable to a wide class of models—virtually all exponentially affine models in the literature, among others—as the characteristic function always exists. We also test our newly proposed pricing methodologies against usual benchmarks in the literature and report that they lead to more efficient and accurate prices. The preprint is available on SSRN."
  },
  {
    "objectID": "presentations/brownbag/PHD_seminars_3.html",
    "href": "presentations/brownbag/PHD_seminars_3.html",
    "title": "Fund flows and investor concerns",
    "section": "",
    "text": "Subject\nWhile financial performance metrics such as risk-adjusted return have long been identified as the main driver of investment flows into the mutual funds industry, other non-financial performance measures have recently emerged. Recent literature has shown that investors have a multi-attribute utility function and do not only rely on the financial performance but also assess the sustainability performance (environmental and social performance) of their investments when making investment decision. However, these preferences are not stable over time and might vary with the concern presents in the economy. This paper examines the tradeoff between financial and non-financial performance through various levels of concerns in the market. We hypothesize that the preferences for sustainability might be driven by investors’ risk aversion towards social or environmental risks but that this risk aversion might vary according to the dominant concern in the economy."
  },
  {
    "objectID": "presentations/actuel/2025-10-13.html",
    "href": "presentations/actuel/2025-10-13.html",
    "title": "Product Complexity, Investor Experience, and Returns",
    "section": "",
    "text": "Résumé\nWe examine how financial sophistication influences investment outcomes for complex financial products. Using regulatory microdata from Brazil’s structured products market, we analyze how returns vary with investor sophistication and product complexity. We introduce a novel measure of financial sophistication that reflects prior trading experience in other securities markets. Investors with established trading experience systematically outperform inexperienced counterparts when trading complex products, exhibiting both persistent skill and greater learning capabilities. This experience-based measure is more predictive than conventional proxies such as wealth, age, and education. Our findings suggest that complexity can effectively obscure risk from unsophisticated investors, enabling strategic rent extraction through product design. These results question the efficacy of current “qualified investor” standards based on wealth thresholds, suggesting that direct measures of financial market experience provide more effective criteria for accessing complex financial instruments in increasingly sophisticated retail markets.\n\n\nBiographie\nAlan Genaro is an Associate Professor of Finance at the Getúlio Vargas Foundation (EAESP/FGV), Brazil, a position he has held since May 2018. He earned his Ph.D. in Statistics from the University of São Paulo (Institute of Mathematics and Statistics – USP) in 2011 and has held visiting scholar positions at Harvard Business School (2024–2025), the Einaudi Institute for Economics and Finance in Rome (2024), and the Courant Institute of Mathematical Sciences at New York University (2012). His research has been published in leading academic journals, including the Journal of Financial Economics, Journal of Banking and Finance, Journal of International Money and Finance, Economics Letters, and Management Science, among others."
  },
  {
    "objectID": "presentations/archives/2021-05-06.html",
    "href": "presentations/archives/2021-05-06.html",
    "title": "Dynamic Optimization for Multi-Goals-Based Wealth Management",
    "section": "",
    "text": "Résumé\nWe develop a dynamic programming methodology that seeks to maximize investor outcomes over multiple, potentially competing goals (such as upgrading a home, paying college tuition, or maintaining an income stream in retirement), even when financial resources are limited. Unlike Monte Carlo approaches currently in wide use in the wealth management industry, our approach uses investor preferences to dynamically make the optimal determination for fulfilling or not fulfilling each goal and for selecting the investor’s investment portfolio. This can be computed quickly, even for numerous investor goals spread over different or concurrent time periods, where each goal may allow for partial fulfillment or be all-or-nothing. The probabilities of attaining each (full or partial) goal under the optimal scenario are also computed, so the investor can ensure the algorithm accurately reflects their preference for the relative importance of each of their goals. These portfolio prescriptions are consistent with Prospect Theory.\n\n\nBiographie\nSanjiv Das is the William and Janice Terry Professor of Finance at Santa Clara University’s Leavey School of Business. He holds post-graduate degrees in Finance (M.Phil and Ph.D. from New York University), Computer Science (M.S. from UC Berkeley), an MBA from the Indian Institute of Management, Ahmedabad, B.Com in Accounting and Economics (University of Bombay, Sydenham College), and is also a qualified Cost and Works Accountant. He is a senior editor of The Journal of Investment Management, co-editor of The Journal of Derivatives, and Associate Editor of other academic journals. Prior to being an academic, Sanjiv Das worked in the derivatives business in the Asia-Pacific region as a Vice-President at Citibank. His current research interests include: the modeling of default risk, machine learning, social networks, derivatives pricing models, portfolio theory, and venture capital. He has published over eighty articles in academic journals, and has won numerous awards for research and teaching. His recent book “Derivatives: Principles and Practice” was published in May 2010. He currently also serves as a Senior Fellow at the FDIC Center for Financial Research.\nDaniel N. Ostrov is a Professor of Mathematics and Computer Science at Santa Clara University. He holds a PhD in Applied Mathematics and a MS in Engineering from Brown University. His research area is Mathematical Finance, with an emphasis on using techniques from control theory and partial differential equations to analyze questions concerning optimal investment and personal finance."
  },
  {
    "objectID": "presentations/archives/2023-11-24.html",
    "href": "presentations/archives/2023-11-24.html",
    "title": "State-dependent Sampling in Observational Cohort Studies",
    "section": "",
    "text": "Résumé\nObservational cohort studies of chronic disease involve the recruitment and follow-up of a sample of individuals with the goal of learning about the course of the disease, the effect of fixed and time-varying risk factors. Analysis of this information is often facilitated by using multistate models with intensity functions governing transition between disease states. Chronic disease studies often involve conditions for recruitment, for example incident cohort involves individuals who are healthy at accrual, prevalent cohort samples individuals who have already developed the disease, and a length biased sampling includes individual who are alive at the time of recruitment. In this talk we discuss the impact of ignoring state-dependent sampling in life history analysis and the ways of addressing the issue using auxiliary information. A longitudinal study of aging and cognition among religious sisters is used to illustrate the related methodology.\n\n\nBiographie\nDr. Leilei Zeng joined the Department of Statistics and Actuarial Science at the University of Waterloo as an associate professor in 2011, as Graham Trust Chair in Health Statistics (2011-2016). Professor Zeng received her masters and PhD in Biostatistics from the University of Waterloo (1999-2005). She then worked as a postdoctoral fellow and then as an assistant professor (2006-2011) at Simon Fraser University (SFU).\nHer research interests lies in the development of statistical methodologies for public health and medical research. Specific research topics include methods for event history and longitudinal data analysis, multistate models, marginal models, incomplete observed data, design of clinical and epidemiological studies, and model misspecification and evaluation. Her current main research collaborations and applications are related to studies in Rheumatic diseases, dementia and Alzheimer disease, and stress and reproduction in women."
  },
  {
    "objectID": "presentations/archives/2023-01-20.html",
    "href": "presentations/archives/2023-01-20.html",
    "title": "Precision Least Squares: Estimation and Inference in High-Dimensional Linear Regression Models",
    "section": "",
    "text": "Résumé\nAs the least squares estimator can be cast to depend only on the precision matrix, we show that a consistent estimator of the latter can be directly used to obtain an expression of the former, even in high-dimensional regression problems where the number of covariates can be larger than the sample size. Since bias can still occur when using consistent regularized precision matrix estimators, we show how to construct a nearly unbiased least squares estimator irrespective of the sparsity within the data generating process. We call this the precision least squares estimator and show that it is asymptotically Gaussian and delivers uniformly valid inference. We employ precision least squares to estimate the predictive connectedness among the daily asset returns of 88 global banks. We find evidence that such connections drastically decrease during crisis periods. Network density (modularity) is then proposed as an empirical measure of crisis proximity.\n\n\nBiographie\nRosnel Sessinou est actuellement stagiaire postdoctoral à HEC Montréal. Il est détenteur d’un doctorat en sciences économiques de l’Université Aix-Marseille. Ses intérêts de recherche portent sur l’économétrie, la finance, l’apprentissage statistique et la statistique en haute-dimension."
  },
  {
    "objectID": "presentations/archives/2020-03-05.html",
    "href": "presentations/archives/2020-03-05.html",
    "title": "Tail risk and style dependence in the fund industry: A multivariate extreme value approach",
    "section": "",
    "text": "Résumé\nIn this paper, we study the connectedness between extreme losses of hedge funds, a crucial feature for systemic risk management. To do so, we exploit cross-sections of hedge funds monthly returns grouped by investment styles, and build a time- varying measure of tail dependence across styles. Relying on extreme value theory and regression techniques, we study the dynamics of the tail dependencies between fund styles conditional on factors reflecting the economic uncertainty and the stock market performance. The resulting tail dependence measures are used to construct a time-varying network between extreme losses of the various investment styles. We show that during a crisis period, while the extremal dependence between some pairs of investment styles remains stable, other pairs show a striking increase of their extremal connectedness. Our results highlight that a proactive regulatory framework should account for the dynamic nature of the tail dependence and its link with financial stress.\nThis is a joint work with Julien Hambuckers and Marie Lambert.\n\n\nBiographie\nLinda Mhalla is a postdoctoral researcher at HEC Montréal. She did her studies in Mathematics and Statistics at EPFL and completed her PhD in 2018 from the University of Geneva, under the supervision of Prof. Valérie Chavez-Demoulin and Prof. Elvezio Ronchetti. Her research interests are in extreme value theory, tail dependence modelling, and causal inference. She is the recipient of a postdoctoral fellowship from the Swiss National Science Foundation."
  },
  {
    "objectID": "presentations/archives/2025-04-15.html",
    "href": "presentations/archives/2025-04-15.html",
    "title": "Estimating Extreme Wave Surges in the Presence of Missing Data",
    "section": "",
    "text": "Résumé\nThe block maxima approach, which consists of dividing a series of observations into equal sized blocks to extract the block maxima, is commonly used for identifying and modelling extreme events using the generalized extreme value (GEV) distribution. In the analysis of coastal wave surge levels, the underlying data which generate the block maxima typically have missing observations. Consequently, the observed block maxima may not correspond to the true block maxima yielding biased estimates of the GEV distribution parameters. Various parametric modelling procedures are proposed to account for the presence of missing observations under a block maxima framework. The performance of these estimators is compared through an extensive simulation study and illustrated by an analysis of extreme wave surges in Atlantic Canada. # Biographie\nDr. Orla Murphy is an Assistant Professor in the Department of Mathematics and Statistics at Dalhousie University in Halifax. She completed a PhD at McGill in statistics with Johanna Nešlehová and Christian Genest before moving to McMaster for a postdoctoral fellowship under the guidance of Paul McNicholas. Her research interests revolve around multivariate statistics, notably for clustering and mixture models, dependence modelling and extreme value analysis."
  },
  {
    "objectID": "presentations/archives/2024-02-16.html",
    "href": "presentations/archives/2024-02-16.html",
    "title": "Recent Advances in Methods for Solving Stochastic Integer Programming Problems",
    "section": "",
    "text": "Résumé\nStochastic integer programming (SIP) problems combine the power of integer decision variables for modeling discrete decisions and logical relationships with the ability of stochastic programming to manage uncertainty when operating, planning, and designing systems. Because of this combination, SIP can be useful in a wide range of applications including power grid operation, employee staffing, and supply chain network design. This combination of features also leads to models that can be extremely difficult so solve. We present recent work in solving these types of problems, including the enhancements to the branch-and-cut method for solving a single instance and techniques for accelerating the solution of multiple instances, as is required when using the sample average approximation technique. This is based on work with Rui Chen and Harshit Kothari.\n\n\nBiographie\nDr. Jim Luedtke est professeur titulaire au département d’ingénierie industrielle et système de l’Université de Wisconsin-Madison. Il a obtenu son doctorat de Georgia Institute of Technology en 2007. Spécialiste de l’optimisation nonlinéaire, stochastique et linéaire avec entiers mixtes, il est actif au sein de la société INFORMS d’optimisation et rédacteur adjoint de Mathematical Programming Computation.\nJim Luedtke is a Professor in the department of Industrial and Systems Engineering at the University of Wisconsin-Madison. Luedtke earned his Ph.D. at Georgia Tech and did postdoctoral work at the IBM T.J. Watson Research Center. Luedtke’s research is focused on methods for solving stochastic and mixed-integer optimization problems, as well as applications of such models. Luedtke is a recipient of an NSF CAREER award, was a finalist in the INFORMS JFIG Best Paper competition, and was awarded the INFORMS Optimization Society Prize for Young Researchers. Luedtke serves on the editorial board of Mathematical Programming Computation, is chair of the Mathematical Optimization Society Publications Committee, and serves as Vice-Chair for Optimization under Uncertainty for the INFORMS Optimization Society."
  },
  {
    "objectID": "presentations/archives/2021-02-19.html",
    "href": "presentations/archives/2021-02-19.html",
    "title": "Joint integrative analysis of multiple data sources with correlated vector outcomes",
    "section": "",
    "text": "Résumé\nWe consider the joint estimation of regression parameters from multiple potentially heterogeneous data sources with correlated vector outcomes. The primary goal of this joint integrative analysis is to estimate covariate effects on all vector outcomes through a marginal regression model in a statistically and computationally efficient way. We present a general class of distributed estimators that can be implemented in a parallelized computational scheme. Modelling, computational and theoretical challenges are overcome by first fitting a local model within each data source and then combining local results while accounting for correlation between data sources. This approach to distributed estimation and inference is formulated using Hansen’s generalized method of moments but implemented via an asymptotically equivalent and communication-efficient meta-estimator. We show both theoretically and numerically that the proposed method yields efficiency improvements and is computationally fast. We illustrate the proposed methodology with the joint integrative analysis of metabolic pathways in a large multi-cohort study.\n\n\nBiographie\nEmily Hector is an Assistant Professor of Statistics at North Carolina State University. She earned a B.Sc. in mathematics (Honours Probability and Statistics) from McGill University and a PhD in Biostatistics at the University of Michigan, working under the supervision of Peter X.-K. Song. Her current methodological interests revolve around data integration, especially of correlated, heterogeneous, high-dimensional data, estimating equations and the generalized method of moments and methods that leverage recent computing and algorithmic developments, with applications in metabolomics, neuroimaging and wearable devices."
  },
  {
    "objectID": "presentations/archives/2022-10-14.html",
    "href": "presentations/archives/2022-10-14.html",
    "title": "Optimal and robust combination of forecasts via constrained optimization and shrinkage",
    "section": "",
    "text": "Résumé\nWe introduce various methods that combine forecasts using constrained optimization with penalty. A non-negativity constraint is imposed on the weights, and several penalties are considered, taking the form of a divergence from a reference combination scheme. In contrast with most of the existing approaches, our framework performs forecast selection and combination in one step, allowing for potentially sparse combining schemes. Moreover, by exploiting the analogy between forecasts combination and portfolio optimization, we provide the analytical expression of the optimal penalty strength when penalizing with the L2-divergence from the equally-weighted scheme. An extensive simulation study and two empirical applications allow us to investigate the impact of the divergence function, the reference scheme, and the non-negativity constraint on the predictive performance. Our results suggest that the proposed models outperform those considered in previous studies.\n\n\nBiographie\nFrédéric Vrins a obtenu un PhD de l’École polytechnique de Louvain (UCLouvain) en 2007 dans le domaine du traitement de signaux adaptifs. Il a travaillé sur des méthodes de séparations de signaux dans le secteur biomédical, avant de se réorienter dans le secteur bancaire où il a travaillé comme analyste quantitatif pendant 7 ans. Depuis 2014, il est professeur titulaire au sein de l’institut de finance quantitative de l’École de gestion de Louvain, en plus d’être professeur affilié à HEC Montréal."
  },
  {
    "objectID": "presentations/archives/2024-10-11.html",
    "href": "presentations/archives/2024-10-11.html",
    "title": "Transmission planning in an imperfectly competitive power sector with environmental externalities",
    "section": "",
    "text": "Résumé\nPolicymakers face the challenge of integrating intermittent output from variable renewable energy (VRE). Even in a well-functioning power sector with flexible generation, producers’ incentives may not align with society’s welfare-maximisation objective. At the same time, political pressure can obstruct policymakers from pricing damage from CO2 emissions according to its social costs. In facilitating decarbonisation, transmission planning will have to adapt to such economic and environmental distortions. Using a Stackelberg model of the Nordic power sector, we find that a first-best transmission-expansion plan involves better resource sharing between zones, which actually reduces the need for some VRE adoption. Next, we allow for departures from perfect competition and identify an extended transmission-expansion plan under market power by nuclear plants. By contrast, temporal arbitrage by hydro reservoirs does not necessitate transmission expansion beyond that of perfect competition because it incentivises sufficient VRE adoption using existing lines. Meanwhile, incomplete CO2 pricing under perfect competition requires a transmission plan that matches hydro-rich zones with sites for VRE adoption. However, since incomplete CO2 pricing leaves fossil-fuelled generation economically viable, it reduces the leverage of strategic producers, thereby catalysing less (more) extensive transmission expansion under market power by nuclear (hydro) plants.\n\n\nBiographie\nDr. Afzal S. Siddiqui est titulaire d’un doctorat en génie industriel et opérations de l’Université de la Californie à Berkeley et est professeur au département de science et systèmes informatiques de l’Université de Stockholm. Il était précédemment professeur d’économie de l’énergie au département des sciences statistique de UCL et professeur affilié à Aalto et HEC Montréal. Ses intérêts de recherche sont dans l’application des méthodes de recherche opérationnelle pour l’analyse de la prise de décision dans un contexte d’incertitude et de compétition dans le secteur énergétique."
  },
  {
    "objectID": "presentations/archives/2025-03-26.html",
    "href": "presentations/archives/2025-03-26.html",
    "title": "A general option pricing framework for affine fractionally integrated models",
    "section": "",
    "text": "Résumé\nIn this presentation, we study the impact of fractional integration on volatility modelling and option pricing. We propose a general discrete-time pricing framework based on affine multi-component volatility models that admit ARCH(\\(\\infty\\))) representations. This not only nests a large variety of option pricing models from the literature, but also allows for the introduction of novel covariance-stationary long-memory affine GARCH pricing models. Using an infinite sum characterization of the log-asset price’s cumulant generating function, we derive semi-explicit expressions for the valuation of European-style derivatives under a general variance-dependent stochastic discount factor. Moreover, we carry out an extensive empirical analysis using returns and S&P 500 options over the period 1996–2019. Overall, we find that once the informational content from options is incorporated into the parameter estimation process, the inclusion of fractionally integrated dynamics in volatility is beneficial for improving the out-of-sample option pricing performance. The largest improvements in the implied volatility root-mean-square errors occur for options with maturities longer than one year, reaching 28% and 18% when compared to standard one- and two-component short-memory models, respectively. This research is joint work with Maciej Augustyniak, Alexandru Badescu, and Sarath Kumar Jayaraman.\n\n\nBiographie\nDr. Jean-François Bégin is an Associate Professor in the Department of Statistics and Actuarial Science at Simon Fraser University and a fellow of the Society of Actuaries and of the Canadian Institute of Actuaries. His research interests include probabilistic modelling in finance and insurance, financial econometrics, filtering methods, credit risk, option pricing, pension economics, mortality, and climate risk. Before joining SFU, he completed a PhD in financial engineering at HEC Montréal under the supervision of Geneviève Gauthier."
  },
  {
    "objectID": "presentations/archives/2025-06-03.html",
    "href": "presentations/archives/2025-06-03.html",
    "title": "Universal Arbitrage-Free Volatility Surfaces",
    "section": "",
    "text": "Résumé\nThe undeniable centrality of volatility surfaces (VSs) in modern quantitative finance has inspired many hand-crafted arbitrage-free VS models; each designed to capture specific stylized features of empirically observed VSs; nevertheless, there remains no generic VS framework devoid of static arbitrage. Though deep learning pipelines provide hope for generic VS modelling, these AI-based approaches presently fail as they typically violate static no-arbitrage conditions. We close this gap in the literature. Our analysis begins with the newly uncovered observation that empirical VS data exhibits log-complete monotonicity in their strike price. We then construct a universal two-layer deep learning model representative of this class of arbitrage-free VSs. The structured weights and biases constraint and the very-specific choices of activation functions allow us to guarantee that our models are always free of static arbitrage and the observed log-complete monotonicity in the strike observed in real-world VS data, while maintaining universality. Additionally, our deep learner’s structure grants us access to a Fourier-analytic training procedure, which is much more efficient than out-of-the-box (stochastic) gradient-descent-based optimization. Our method is implemented on real-world volatility data, where we achieve state-of-the-art reconstruction performance.\nJoint work: Yannick Limmer and Blanka Horvath (Oxford)\n\n\nBiographie\nAnastasis is an assistant professor at McMaster University working on the analytic and statistical foundations of AI and theoretically-guided deep learning in finance. His work aims at understanding the behaviour and properties of these models from the ground up, and using his findings to develop new principled AI which can leverage structures in finance, stochastic processes, games, and PDEs problems to obtain guaranteed scalable and predictive computational pipelines.\nAnastasis did his PhD at Concordia university in mathematical finance and his Masters at l’Université de Montréal in mathematics, before moving to ETH Zürich for his postdoc in deep learning for finance and then to the University of Basel to work on geometric deep learning. Since starting his faculty position, Anastasis has supervised 10 students including postdoc and PhD students to honors undergraduate students."
  },
  {
    "objectID": "presentations/archives/2024-01-26.html",
    "href": "presentations/archives/2024-01-26.html",
    "title": "An exploration-agnostic characterization of the ergodicity of parallel tempering",
    "section": "",
    "text": "Résumé\nNon-reversible parallel tempering (NRPT) is an effective algorithm for sampling from target distributions with complex geometry, such as those arising from posterior distributions of weakly identifiable and high-dimensional Bayesian models. In this talk I will establish the uniform geometric ergodicity of NRPT under an efficient local exploration hypothesis, which avoids the intricacies of dealing with kernel-specific properties. The rates that we obtain are bounded in terms of an easily-estimable divergence, the global communication barrier (GCB), that was recently introduced in the literature. We obtain analogous ergodicity results for classical reversible parallel tempering, providing new evidence that NRPT dominates its reversible counterpart. I will also present some general properties of the GCB and bound it in terms of the total variation distance and the inclusive/exclusive Kullback-Leibler divergences. I will conclude the talk with simulations that validate the new theoretical analysis.\nThis is based on joint work with Nikola Surjanovic, Saifuddin Syed, and Alexandre Bouchard-Côté.\n\n\nBiographie\nDr. Trevor Campbell est professeur agrégé au département de statistique de l’Université de la Colombie-Britannique à Vancouver. Ses recherches portent sur l’inférence Bayésienne, notamment le développement d’algorithmes flexible et automatisés, les données en lignes, la théorie bayésienne et les méthodes bayésiennes nonparamétriques. Trevor a complété son doctorat au Laboratoire d’information et de systèmes de décision au MIT sous la tutelle de Jonathan How et un stage postdoctoral avec Tamara Broderick."
  },
  {
    "objectID": "presentations/archives/2022-11-25.html",
    "href": "presentations/archives/2022-11-25.html",
    "title": "(Almost) All of Entity Resolution",
    "section": "",
    "text": "Résumé\nWhether the goal is to estimate the number of people that live in a congressional district, to estimate the number of individuals that have died in an armed conflict, or to disambiguate individual authors using bibliographic data, all these applications have a common theme — integrating information from multiple sources. Before such questions can be answered, databases must be cleaned and integrated in a systematic and accurate way, commonly known as record linkage, de-duplication, or entity resolution. In this article, we review motivational applications and seminal papers that have led to the growth of this area. Specifically, we review the foundational work that began in the 1940’s and 50’s that have led to modern probabilistic record linkage. We review clustering approaches to entity resolution, semi- and fully supervised methods, and canonicalization, which are being used throughout industry and academia in applications such as human rights, official statistics, medicine, citation networks, among others. Finally, we discuss current research topics of practical importance. This is joint work with Olivier Binette.\n\n\nBiographie\nRebecca Steorts est professeure adjointe au département de sciences statistiques à l’Université Duke depuis 2015 et est affiliée au U.S. Census Bureau, où elle est mathématicienne statisticienne et responsable des accords coopératifs sur le recensement pour la résolution d’entités et la fusion d’identifiants. Elle a obtenu un doctorat en statistique à l’Université de la Floride encadrée par Malay Ghosh et a été professeure adjointe en visite à Carnegie Mellon entre 2012 et 2015, chapeautée par Stephen Fienberg. Dr. Steorts a publié une trentaine d’articles dans des revues arbitrées."
  },
  {
    "objectID": "presentations/archives/2024-08-29.html",
    "href": "presentations/archives/2024-08-29.html",
    "title": "High-dimensional approximate dynamic matrix factor models: Estimation via the Kalman smoother and the EM algorithm",
    "section": "",
    "text": "Résumé\nHigh-dimensional matrix-variate time series data are becoming increasingly popular in economics and finance. This has stimulated the development of matrix factor models to achieve significant dimension reduction. This paper proposes an approximate dynamic matrix factor model that accounts for the time series nature of the data, and develops an estimator of the model parameters based on the EM algorithm and the Kalman smoother that allows to handle arbitrary patterns of missing data. We establish the consistency of the estimated loadings and factor matrices. The finite sample properties of the estimators are assessed through a large simulation study and an application to a financial dataset. This is joint work with Matteo Barigozzi (University of Bologna).\n\n\nBiographie\nDr. Luca Trapin is an associate professor of economic statistics in the Department of Statistics at the University of Bologna. Before moving to Bologna, he was Assistant Professor of Econometrics at Università Cattolica del Sacro Cuore, Post-doctoral Fellow in the Quantitative Finance Group at Scuola Normale Superiore in Pisa, under the supervision of Prof. Davide Pirino, and Post-doctoral Fellow at the Department of Decision Sciences at HEC Montréal, under the supervision of Prof. Debbie Dupuis. Luca received my Ph.D. in Economics from IMT School for Advanced Studies Lucca, under the supervision of Prof. Massimo Riccaboni and Prof. Marco Bee. His research interests are in statistical analysis of financial data, extreme value analysis for policy decisions, economics and business forecasting."
  },
  {
    "objectID": "presentations/archives/2023-05-18.html",
    "href": "presentations/archives/2023-05-18.html",
    "title": "Consensus and Dissensus in Multi-population Multi-agent Systems",
    "section": "",
    "text": "Résumé\nThe talk will start with a general overview of mean field games approach to decision making in multi-agent dynamical systems in both model-based and model-free settings and discuss connections to finite-population games. Following this general introduction, the talk will focus on the structured setting of discrete-time infinite-horizon linear-quadratic-Gaussian dynamic games, where the players are partitioned into finitely-many populations with an underlying graph topology — a framework motivated by paradigms where consensus and dissensus co-exist. Mean field games approach will be employed to arrive at approximate Nash equilibria, and learning algorithms will be presented for the model-free setting, along with sample complexity analysis.\n\n\nBiographie\nTamer Başar est titulaire émérite de la chaire dotée Swanlund et professeur émérite au Center for Advanced Study du département d’ingénierie électrique et informatique de l’Université de l’Illinois à Urbana-Champaign. Il est également professeur de recherche au Coordinated Science Laboratory et au Information Trust Institute et directeur exécutif de Illinois at Singapore Pte. Ltd. Dr. Başar est détenteur d’un Ph.D. de l’Université Yale. Il est le (co-)auteur de près de 1000 publications dans les domaines du contrôle optimal, robuste et adaptif, stochastique, de la théorie de l’estimation et des jeux dynamiques. Il est également l’auteur de l’ouvrage Dynamic Noncooperative Game Theory, coauteur de Game Theory for Next Generation Wireless and Communication Networks: Modeling, Analysis, and Design et (co-)éditeur de plusieurs ouvrages de référence dans le domaine. Tamer Başar a reçu de nombreux prix durant sa carrière: il est notamment membre de l’académie nationale d’ingénierie des États-Unis et fellow de l’IEEE depuis 1983. Une biographie complète est disponible en ligne."
  },
  {
    "objectID": "presentations/archives/2023-10-11.html",
    "href": "presentations/archives/2023-10-11.html",
    "title": "Disaster Resilient Cities: An OR Approach to Disaster Management",
    "section": "",
    "text": "Résumé\nThere have been decades of research in the field of humanitarian logistics, and academics in the field of logistics are becoming more and more interested in it. That being said, we still acquire insight and identify new problems with each disaster.\nWe have also observed various humanitarian logistics applications during Covid-19, e.g. for PCR test sites and vaccination centers. Unfortunately, the recent earthquake in Turkey has led us to re-evaluate the response cycle of disaster management. Close inspection reveals that this response phase actually leads to a variety of new applications of distribution logistics problems.\nWe have conducted many meetings and workshops with municipalities that were very active during the response of the Maraş Earthquakes. Many municipalities aim to have “earthquake resistant cities” with correct action plans and being ready for potential disasters. Based on our discussions with these municipalities, we have developed an “ideal action plan”. We also investigated the potential decision problems and linked these problems with OR literature.\n\n\nBiographie\nDr. Bahar Y. Kara est professeure titulaire au département de génie industriel de l’Université Bilkent, où elle a effectué toutes ses études et de laquelle elle a obtenu un doctorat sous la direction de B.C. Tansel en 1999. Elle a effectué un postdoctorat en gestion à McGill avant de retourner en Turquie. Ses intérêts de recherches sont en logistique, notamment logistique humanitaire et distribution, construction de réseau et problèmes de routages. Dr. Kara est membre fondatrice du comité exécutive EURO pour les opérations humanitaires (HOpe). Elle est également membre du conseil exécutif de l’Université Bilkent et la société turque de recherche opérationnelle. Dr. Kara a édité quatre ouvrages de référence et publié plus de 80 articles. Elle est rédactrice adjointe de Transportation Research-Part B, du Journal of Operational Research Society, de IIE Transactions, et de Socio Economic Planning and Science."
  },
  {
    "objectID": "presentations/archives/2022-05-10b.html",
    "href": "presentations/archives/2022-05-10b.html",
    "title": "Efficient Estimation of Bid-Ask Spreads from Open, High, Low, and Close Prices",
    "section": "",
    "text": "Résumé\nThis paper formally derives an efficient estimator of the bid-ask spread from open, high, low, and close prices. The estimator is asymptotically unbiased and optimally combines the full set of price data to minimize the estimation variance. In absence of quote data, it delivers the most accurate estimates of bid-ask spreads theoretically, numerically, and empirically. The estimator is easy to calculate and has a broad applicability in empirical finance.\n\n\nBiographie\nEmanuele Guidotti est doctorant en finance à l’Université de Neuchâtel spécialisé dans la tarification empirique des actifs. Il est également chercheur associé au CREST, Japan Science and Technology Agency depuis 2017 au sein du projet YUIMA, une équipe internationale de recherche qui oeuvre à développer un environnement complet pour l’estimation et la simulation d’équations différentielles stochastiques. Il est également partenaire au sein de Algo Finance Sagl, une jeune pousse spécialisé dans la création de logiciels et d’algorithmes en finance pour le secteur de la gestion d’actifs. Emanuele enseigne à titre de professeur associé à l’Université de Milan au sein du programme de maîtrise en science des données et économie. Emanuele détient une licence en physique et une maîtrise en économie et en finance quantitive, toutes deux décernées par l’Université de Milan cum laude."
  },
  {
    "objectID": "presentations/archives/2021-03-17.html",
    "href": "presentations/archives/2021-03-17.html",
    "title": "Convex Optimization",
    "section": "",
    "text": "Résumé\nConvex optimization has emerged as useful tool for applications that include data analysis and model fitting, resource allocation, engineering design, network design and optimization, finance, and control and signal processing. After an overview of the mathematics, algorithms, and software frameworks for convex optimization, we turn to common themes that arise across applications, such as sparsity and relaxation. We describe recent work on real-time embedded convex optimization, in which small problems are solved repeatedly in millisecond or microsecond time frames.\n\n\nBiographie\nStephen P. Boyd is the Samsung Professor of Engineering, Professor of Electrical Engineering in the Information Systems Laboratory, and chair of the Electrical Engineering Department at Stanford University. His current research focus is on convex optimization applications in control, signal processing, machine learning, and finance.\nProfessor Boyd received a PhD in EECS from U. C. Berkeley in 1985. In 1985 he joined the faculty of Stanford’s Electrical Engineering Department. He is the author of many research articles and four books. His group has produced many open source tools, including CVX (with Michael Grant), CVXPY (with Steven Diamond), Convex.jl (with Madeleine Udell and others), and CVXR (with Anqi Fu and A. Narasimhan), widely used parser-solvers for convex optimization. His group’s CVXGEN software is used in many applications, including the SpaceX Falcon 9 landing system. Stephen P. Boyd has received many awards and honors for his research in control systems engineering and optimization, including an ONR Young Investigator Award, a Presidential Young Investigator Award, and the AACC Donald P. Eckman Award. In 2013, he received the IEEE Control Systems Award, given for outstanding contributions to control systems engineering, science, or technology. In 2012, Michael Grant and he were given the Mathematical Optimization Society’s Beale-Orchard-Hays Award, given every three years for excellence in computational mathematical programming. He is a Fellow of the IEEE, SIAM, and INFORMS, a Distinguished Lecturer of the IEEE Control Systems Society, a member of the US National Academy of Engineering (NAE), a foreign member of the Chinese Academy of Engineering (CAE), and a foreign member of the National Academy of Engineering of Korea (NAEK). He has been invited to deliver more than 90 plenary and keynote lectures at major conferences in control, optimization, signal processing, and machine learning."
  },
  {
    "objectID": "presentations/archives/2024-04-11.html",
    "href": "presentations/archives/2024-04-11.html",
    "title": "On the design of public debate in social networks",
    "section": "",
    "text": "Résumé\nWe propose a model of the joint evolution of opinions and social relationships in a setting where social influence decays over time. The dynamics are based on bounded confidence: social connections between individuals with distant opinions are severed while new connections are formed between individuals with similar opinions. Our model naturally gives raise to strong diversity, i.e., the persistence of heterogeneous opinions in connected societies, a phenomenon that most existing models fail to capture. The intensity of social interactions is the key parameter that governs the dynamics. First, it determines the asymptotic distribution of opinions. In particular, increasing the intensity of social interactions brings society closer to consensus. Second, it determines the risk of polarization, which is shown to increase with the intensity of social interactions. Our results allow to frame the problem of the design of public debates in a formal setting. We hence characterize the optimal strategy for a social planner who controls the intensity of the public debate and thus faces a trade-off between the pursuit of social consensus and the risk of polarization. We also consider applications to political campaigning and show that both minority and majority candidates can have incentives to lead society towards polarization. This is joint work with A. Mandel and A. Rusinowska.\n\n\nBiographie\nDr. Michel Grabisch est professeur titulaire de sciences informatiques à l’Université Paris I Panthéon-Sorbonne et à l’École d’économie de Paris. Il a obtenu un doctorat en traitement de signaux de l’ENSIEG et son HDR de l’Université Paris VI. Après 15 ans comme ingénieur dans des instituts recherche, il s’est joint à l’Université Paris VI pour ensuite devenir professeur titulaire à Paris I. Il est coéditeur en chef du Quarterly Journal of Operations Research et éditeur adjoint de plusieurs revues. Michel a publié cinq livres, 141 articles dans des journaux avec comités d’arbitres et 114 articles dans des comptes rendus arbitrés de congrès. Ses intérêts de recherche portent sur la prise de décision, la théorie des jeux coopératifs, les ensembles ordonnées, les mathématiques discrètes et la recherche opérationnelle. Michel est membre senior honoraire de l’Institut universitaire de France."
  },
  {
    "objectID": "presentations/archives/2022-04-28.html",
    "href": "presentations/archives/2022-04-28.html",
    "title": "Toward social welfare and fairness in kidney exchange programs",
    "section": "",
    "text": "Résumé\nMatching markets are part of our daily lives, appearing on online platforms, school admissions and health systems. Their study attracts the interest of optimizers and game theorists. In this talk, we will focus on a particular matching market, the kidney exchange program (KEP), where combinatorial optimization and game theory play an important role.\nA patient in need of a kidney transplant who has an incompatible donor can register on a KEP. The program seeks compatible donor exchanges between these patient-donor pairs to maximize patient benefit. KEPs resulting from the combination of patient-donor pools from different agents (which can be transplant centers, regions or countries) are currently being formed, requiring a game theoretical analysis. In this talk, we will recall literature on multi-player KEPs and see the extension of graph theoretical results to this game. Then, through computational experiments inspired by the Canadian KEP, we will investigate the social welfare generated by a non-cooperative multi-player KEP. These experiments will reveal the multiplicity of socially optimal solutions, leading to a new research question regarding individual patient fairness: Given multiple socially optimal matching plans, how should we select among them? We will end this talk with a proposition to tackle this question as well as an optimization methodology to efficiently implement it in practice.\n\n\nBiographie\nDr. Margarida Carvalho est une mathématicienne portugaise spécialisée dans la programmation entière mixte, la théorie des jeux algorithmique et la complexité calculatoire. Elle est professeure adjointe au Département d’informatique et de recherche opérationnelle de l’Université de Montréal depuis 2018 et est détentrice de la chaire de recherche FRQ-IVADO en science des données pour la théorie des jeux combinatoires. Margarida Carvalho est détentrice d’un doctorat en sciences informatiques, pour lequel elle a obtenu le prix EURO Doctoral Dissertation Award 2018. Elle a effectué un stage postdoctoral au sein de la Chaire d’excellence en recherche du Canada sur la science des données pour la prise de décision en temps réel suite à ses études avant d’être embauchée à l’UdeM. Margarida est actuellement rédactrice adjointe des revues INFORMS Journal on Computing, OR Spectrum et Dynamic Games and Applications."
  },
  {
    "objectID": "presentations/archives/2023-02-17.html",
    "href": "presentations/archives/2023-02-17.html",
    "title": "Autoregressive conditional betas",
    "section": "",
    "text": "Résumé\nThis paper introduces an autoregressive conditional beta (ACB) model that allows regressions with dynamic betas (or slope coefficients) and residuals with GARCH conditional volatility. The model fits in the quasi score-driven approach recently pro- posed in the literature, and it is semi-parametric in the sense that the distributions of the innovations are not necessarily specified. The time-varying betas are allowed to depend on past shocks and exogenous variables. We establish the existence of a stationary solution for the ACB model, the invertibility of the score-driven filter for the time-varying betas, and the asymptotic properties of one-step and multistep QMLEs for the new ACB model. The finite sample properties of these estimators are studied by means of an extensive Monte Carlo study. Finally, we also propose a strategy to test for the constancy of the conditional betas. In a financial application, we find evidence for time-varying conditional betas and highlight the empirical relevance of the ACB model in a portfolio and risk management empirical exercise. This is joint work with Francisco Blasques and Christian Francq.\n\n\nBiographie\nDr. Sébastien Laurent est professeur titulaire en économétrie à l’école d’économie d’Aix-Marseille (AMSE) et membre de l’Institut d’administration des entreprises (IAE). Son expertise de recherche est en économétrie financière et computationnelle. Il est détenteur d’une maîtrise et d’un doctorat en économétrie financière de l’Université Maastricht. Il est l’auteur de 42 publications dans des revues avec comités de relecture et co-éditeur du Handbook of Volatility Models and Their Applications paru en 2012."
  },
  {
    "objectID": "presentations/archives/2020-10-23.html",
    "href": "presentations/archives/2020-10-23.html",
    "title": "Banking networks, systemic risk, and the credit cycle in emerging markets",
    "section": "",
    "text": "Résumé\nWe undertake a large-scale empirical examination of systemic risk among 1048 financial institutions in a large sample of 23 emerging markets, broken down into 5 regions. This work extends the large literature on systemic risk in the US, Europe, and other developed countries to emerging markets, which is relatively under-researched. We present a novel systemic risk score for each financial system by region, across time. This measure is additively decomposable and attributable to each financial institution, and may be used as an objective and quantifiable measure of whether a bank is a SIFI (systemically important financial institution). The level and timing of systemic risk is heterogenous across the 5 regions, and this risk is concentrated in a few banks, more so pre-crisis than post-crisis. Credit and network effects account for over 2/3 of systemic risk (and in some cases, almost all of the risk), and the remaining comes from individual bank variables. Spillovers of systemic risks across regions are mostly contemporaneous within the quarter. A primary principal component (default level) accounts for 1/2 of the variation in systemic risk across the regions with the next two principal components (policy uncertainty and liquidity) accounting for 1/5 each. Aggregate default risk in a region is statistically predictable using our systemic risk metric, thereby supporting timely macro-prudential policy-making.\n\n\nBiographie\nMadhu Kalimipalli is an Equitable Life of Canada Fellow and Full Professor in Finance at the Lazaridis School of Business and Economics, Wilfrid Laurier University, Waterloo, Canada, and the Director of PhD and Research-based Master’s Programs in Management. Prior to joining Laurier in 2000, he was a visiting Assistant professor at the Faculty of Management in McGill University. He holds a PhD in Finance from the Bauer College of Business, University of Houston, and MA degrees in Economics from Rutgers University and the Gokhale Institute of Politics and Economics, Pune, India. His research has appeared in premier finance journals such as Journal of Financial Economics, Journal of Financial Intermediation, Journal of Banking and Finance, Journal of Empirical Finance, among others."
  },
  {
    "objectID": "presentations/archives/2025-02-21.html",
    "href": "presentations/archives/2025-02-21.html",
    "title": "Inference for Interpretable Machine Learning: Feature Importance and Beyond",
    "section": "",
    "text": "Résumé\nMachine Learning (ML) systems are being used to make critical societal, scientific, and business decisions. To promote trust, transparency, and accountability in these systems, many advocate making them interpretable or explainable. In response, there has been dramatic growth in techniques to provide human understandable interpretations of black-box techniques. Yet we ask: Can we trust these ML interpretations? How do we know if they are correct? Unlike for prediction tasks, it is difficult to directly test the veracity of ML interpretations. In this talk, we focus on interpreting predictive models to understand important features and important feature patterns. We first present motivating results from a large-scale empirical stability study illustrating that feature interpretations are generally unreliable and far less reliable than predictions. To address these issues, we present a new statistical inference framework for quantifying the uncertainty in feature importance and higher-order feature patterns. Our approach allows one to test whether a feature significantly contributes to any ML model’s predictive ability in a completely distribution free manner, thus promoting trust in ML feature interpretations. We highlight our inference for interpretable ML approaches via real scientific case studies and a fun illustrative example.\n\n\nBiographie\nDr. Genevera Allen is a professor of statistics at Columbia University and an investigator at the Zuckerman Institute. Before joining Columbia, she served as assistant and associate professor of Electrical and Computer Engineering, Statistics, and Computer Science at Rice University and as investigator at the Jan and Dan Duncan Neurological Research Institute at Texas Children’s Hospital and Baylor College of Medicine. She is also the Founding Director of the Rice Center for Transforming Data to Knowledge. Dr. Allen received her Ph.D. in statistics in 2010 from Stanford University under the mentorship of Prof. Robert Tibshirani.\nDr. Allen’s research develops new statistical machine learning tools to help people make reliable data-driven discoveries. She is known for her methods and theory work in the areas of unsupervised learning, interpretable machine learning, data integration, graphical models, and high-dimensional statistics. Dr. Allen is the recipient of several honors for both her research and educational efforts including a National Science Foundation Career Award, Rice University’s Duncan Achievement Award for Outstanding Faculty, the Curriculum Innovation Award, and the School of Engineering’s Research and Teaching Excellence Award. In 2014, she was named to the “Forbes ’30 under 30′: Science and Healthcare” list. She is also an elected member of the International Statistics Institute and an elected fellow of the American Statistical Association and of the Institute for Mathematical Statistics."
  },
  {
    "objectID": "presentations/archives/2023-09-12.html",
    "href": "presentations/archives/2023-09-12.html",
    "title": "Multivariate extremes – A geometric Bayesian inference approach",
    "section": "",
    "text": "Résumé\nMultivariate extreme value theory (MEVT) is a branch of probability and statistics concerned with the characterisation of the extremes of finite-dimensional random vectors and the estimation of the probability of joint, rare events. Of particular interest is the task of extrapolating beyond the range of observed data; common environmental applications include modelling extreme hydrological events linked with flooding, harmful and damaging wind gusts as well as heatwaves intensity or duration and their impacts on livelihood.\nThe lack of natural ordering of vectors and the schism between the classes of asymptotically dependent and asymptotically independent random vectors gave rise to various theoretical and modelling frameworks. The geometric approach to MEVT arises through the study of suitably scaled sample clouds―or suitably scaled independent observations from random vectors―and their convergence in probability onto compact and star-shaped limit sets. The estimation of the shape of these limit sets and their associated gauge functions respectively allows for the estimation of well-known coefficients of extremal dependence and of the probability of rare or extreme events. Using a radial-angular decomposition of the random vector of interest, we consider the distribution of radial exceedances of high quantiles of the distribution of radii given angles. Using a limiting Poisson point process likelihood, we present a method using information both from the distribution of the radial exceedances and from the distribution of the angles along which the exceedances occur. We adopt a Bayesian approach in which special emphasis is placed on Hilbert space projections of Gaussian Markov random fields on spheres as flexible non-parametric models for the prior distribution of the gauge function and for the prior distribution of angles. We showcase our method with a simulation study and two case studies of river flow and sea level extremes. Joint work with Ioannis Papastathopoulos, Ryan Campbell, Håvard Rue.\n\n\nBiographie\nLambert De Monte est un doctorant en statistique à l’Université d’Édimbourg spécialisé dans l’analyse de valeurs extrêmes et les méthodes probabilistes de prévisions. Il est encadré par Ioannis Papastathopoulos. Originaire de Montréal, il est détenteur d’une maîtrise en mathématiques de l’Université McGill et d’un B.Sc. en mathématiques et informatique."
  },
  {
    "objectID": "presentations/archives/2023-12-20.html",
    "href": "presentations/archives/2023-12-20.html",
    "title": "Equilibria in Centralized Insurance Markets: Monopolistic vs. Competitive Pricing",
    "section": "",
    "text": "Résumé\nThree fundamental concerns of any risk-sharing market are the willingness of agents to engage in an exchange, Pareto efficiency of allocations of the aggregate risk, and a characterization of equilibria in the market. In sequential-move insurance markets with monopolistic pricing, the notion of a Stackelberg Equilibrium (SE) has gained recent popularity as an equilibrium concept. In this talk, I will go over some of our recent work on the characterization of these equilibria, the examination of their relationship with Pareto-efficient (PE) allocations, as well as some extensions thereof. Specifically, while we show that SE lead to PE allocations, we also show that only those PE allocations that make the policyholder(s) indifferent between suffering the loss and entering into the market can be decentralized through a SE. We interpret the latter result as indicative of the limitations of SE as an equilibrium concept in this literature. We then extend this market structure by introducing strategic price competition on the supply side between several insurers. We argue that the notion of a Subgame Perfect Nash Equilibrium (SPNE) is the appropriate solution concept for analyzing equilibria in that market, and that it is an extension of SE to the case of multiple insurer. We characterize SPNE and show that they lead to PE allocations. Additionally, we show that under mild conditions, the policyholder realizes a strict welfare gain, which addresses the aforementioned concerns with SE and thereby ultimately reflects the benefit to the policyholder of competition on the supply side.\n\n\nBiographie\nMario Ghoussoub est professeur agrégé au départment de statistique et science actuarielle de l’Université de Waterloo, où il a obtenu son doctorat en 2011 avant de faire un stage postdoctoral à l’Université de Montréal. Après un passage à Imperial College, il est revenu à Waterloo. Ses intérêts de recherches portent principalement sur l’incertitude entourant les modèles et la théorie du choix sous incertitude et ambiguité, avec applications en assurance, mesures de risques et gestion, finance comportementale quantitative et la théorie du partage du risque."
  },
  {
    "objectID": "presentations/archives/2023-02-24.html",
    "href": "presentations/archives/2023-02-24.html",
    "title": "Personalized Dynamic Pricing with Machine Learning: High Dimensional Features and Heterogeneous Elasticity",
    "section": "",
    "text": "Résumé\nWe consider a seller who can dynamically adjust the price of a product at the individual customer level, by utilizing information about customers’ characteristics encoded as a \\(d\\)-dimensional feature vector. We assume a personalized demand model, parameters of which depend on \\(s\\) out of the \\(d\\) features. The seller initially does not know the relationship between the customer features and the product demand, but learns this through sales observations over a selling horizon of \\(T\\) periods. We prove that the seller’s expected regret, i.e., the revenue loss against a clairvoyant who knows the underlying demand relationship, is at least of order \\(s\\sqrt{T}\\)% under any admissible policy. We then design a near-optimal pricing policy for a “semi-clairvoyant” seller (who knows which \\(s\\) of the \\(d\\) features are in the demand model) that achieves an expected regret of order \\(s\\sqrt{T}\\log T\\). We extend this policy to a more realistic setting where the seller does not know the true demand predictors, and show that this policy has an expected regret of order \\(s\\sqrt{T} (\\log d＋\\log T)\\), which is also near-optimal. Finally, we test our theory on simulated data and on a data set from an online auto loan company in the United States. On both data sets, our experimentation-based pricing policy is superior to intuitive and/or widely-practiced customized pricing methods such as myopic pricing and segment-then-optimize policies. Furthermore, our policy improves upon the loan company’s historical pricing decisions by 47% in expected revenue over a six-month period.\n\n\nBiographie\nDr. Gah-Yi Ban est professeur agrégée à l’école de gestion de Imperial College à Londres et anciennement de la London Business School et de l’Université du Maryland. Elle détient un doctorat en recherche opérationnelle de l’Université de Californie à Berkeley (UCB) et sert en qualité de rédactrice ajointe de Management Science, M&SOM et de Operations Research Letter. Ses intérêts de recherche sont en analytique des mégadonnées, plus spécialement la prise de décision dans des environnements incertains en haute-dimension avec des applications en gestion. Ses recherches ont été couronnées du prix Best OM Paper in Operations Research award en 2021."
  },
  {
    "objectID": "presentations/archives/2023-09-29.html",
    "href": "presentations/archives/2023-09-29.html",
    "title": "Statistical inference for multivariate extremes via a geometric approach",
    "section": "",
    "text": "Résumé\nA recent addition to the set of modelling tools for multivariate extremes uses a geometric approach via the use of the so-called gauge function. The gauge function is intrinsically linked to the limit set, which itself is obtained by appropriately scaling random vectors whose marginal distributions are light-tailed, and allowing the sample size to grow arbitrarily large. The geometric shape of this limit set, and thus the gauge function, allows for a simple characterisation of the underlying dependence structure. Compared to other approaches, the geometric approach leads to statistical inference that is easier to interpret and has the ability to accurately estimate probabilities in extremal regions where other methods fail. The primary method proposed in this work relies on the use of parametric forms of gauge functions derived from known copulas. Time permitting, a more flexible semi-parametric Bayesian method to obtain a posterior fit of the limit set is presented. This allows for more accurate extremal probability estimates in dependence structures where non-Bayesian methods struggle. This is joint work with Jennifer Wadsworth (Lancaster University); the semi-parametric Bayesian work is done in collaboration with Ioannis Papastathopoulos and Lambert de Monte (University of Edinburgh).\n\n\nBiographie\nRyan Campbell est doctorant sous la tutelle de Jenny Wadsworth à l’Université Lancaster et spécialisé dans l’analyse de valeurs extrêmes. Originaire de Montréal, il est détenteur d’une maîtrise en mathématiques de l’Université McGill et a travaillé brièvement comme scientifique des données chez Desjardins Assurances."
  },
  {
    "objectID": "presentations/archives/2020-12-04.html",
    "href": "presentations/archives/2020-12-04.html",
    "title": "Risk-neutral moments based estimation of continuous time jump-diffusion models",
    "section": "",
    "text": "Résumé\nThis paper provides a novel methodology for estimating option pricing models based on risk‐neutral moments. We synthesize the distribution extracted from a panel of option prices and exploit linear relationships between risk‐neutral cumulants and latent factors within the continuous time affine stochastic volatility framework. We find that fitting the Andersen et al. (Journal of Financial Economics, 2015, 117(3), 558–584) option valuation model to risk‐neutral moments captures the bulk of the information in option prices. Our estimation strategy is effective, easy to implement, and robust, as it allows for a direct linear filtering of the latent factors and a quasi‐maximum likelihood estimation of model parameters. From a practical perspective, employing risk‐neutral moments instead of option prices also helps circumvent several sources of numerical errors and substantially lessens the computational burden inherent in working with a large panel of option contracts.\n\n\nBiographie\nBruno Feunou is a Research Advisor at the Bank of Canada’s Financial Markets Department. Before this position at the Bank of Canada, he worked at Duke University as a post-doc associate. He completed his Ph.D degree at the University of Montreal. Bruno Feunou research interest are option and bond pricing, the linkages between term structure of interest rate and the macroeconomy."
  },
  {
    "objectID": "presentations/archives/2022-11-02.html",
    "href": "presentations/archives/2022-11-02.html",
    "title": "Employee Views of Leveraged Buy-Out Transactions",
    "section": "",
    "text": "Résumé\nThis paper offers a comprehensive view of employee satisfaction: we make use of 700,000 ratings, in addition to 500,000 written reviews posted by employees of all ranks, in different industries, types of companies, and in companies that underwent different types of ownership changes. Employee satisfaction is lower following a company acquisition; more so when it is a Leveraged Buy-Out (LBO). However, there is significant heterogeneity. Our first key finding is that previous ownership type is the main source of heterogeneity for employee satisfaction. A large decrease is observed for Public-to-Private transactions only. The employee drop in satisfaction in these types of transactions is four times larger than in a traditional M&A. We do not find however any difference between Private-to-Private transactions and regular M&As. We conducted a Latent Dirichlet Allocation analysis on about 500,000 written cons reviews and show that for Public-to-Private transactions, employees complain specifically about layoffs, restructuring and cost cutting as well as about the management team. However, these employees also complain less about the operations or their benefits which were a problem before the transaction.\n\n\nBiographie\nMarie Lambert has a joint Ph.D. in Finance from the Universities of Liège and Luxembourg (2010). She is Full Professor and Vice-Dean for Research at HEC Liège - Management School of the University of Liège. She leads the track “Banking and Asset Management” of the master in management and teaches courses on Asset Management, Alternative Investments, Corporate Finance and Financial Modeling. Marie is also Affiliate Professor at EDHEC Business School (Nice) and at Paris Dauphine as well as a Research Associate at the EDHEC Risk Institute and a research fellow of the Quantitative Management Initiative. She is an Associate Editor of Global Finance Journal, Credit and Capital Markets, and Applied Finance Letters. She sits at the management board of the BENELUX Corporate Finance Network. Marie has developed a research expertise in asset pricing models, market anomalies, investment styles (value, growth investing), and hedge funds. Her work has been published in international peer-reviewed journals and has been presented to leading academic and professional conferences. Her current research interests lie in sustainable finance (e.g., climate risk pricing, disagreement in the measurement of firm sustainable ratings, performance analysis of sustainable investments) and private equity."
  },
  {
    "objectID": "presentations/archives/2023-05-16.html",
    "href": "presentations/archives/2023-05-16.html",
    "title": "Institutional Ownership and the Resolution of Financial Distress",
    "section": "",
    "text": "Résumé\nWe analyze the distressed firm’s decision between Chapter 11 and an exchange offer. We construct a comprehensive data set on the financial characteristics and capital structure of 269 distressed firms, which, unlike previous studies, uses quarterly information and includes exhaustive data on equity and bond ownership by institutional investors. Logit regressions confirm that the firm’s restructuring decision depends on its financial characteristics as well as the equity and bond ownership of institutional investors. The impact of ownership varies across categories of investors and according to whether they hold equity or bonds. In general, equity ownership favours exchange offers while we find mixed evidence for bond ownership. In particular, hedge funds have a positive impact on the likelihood of exchange offers through their equity holdings but none through their bond holdings. VC/PE firm holdings are also unique in that higher equity holdings are associated with exchange offers and higher bond holdings with Chapter 11. Finally, the magnitude of equity and bond institutional ownership depend on the quarter at which they are measured, confirming the importance of using quarterly data. This is joint work with Timothy C.G. Fisher and Lorenzo Naranjo.\n\n\nBiographie\nDr. Jocelyn Martel est professeur titulaire au département de finance de l’école de gestion ESSEC depuis 2012 et spécialiste de l’économétrie financière. Son champ d’expertise est en théorie financière, faillites, restructurations et évaluations d’entreprises. Il est co-directeur de la chaire Amundi en gestion du risque et des actifs. Il a défendu son habilitation à diriger des recherches à Cergy-Pontoise en 2008 et a obtenu une maîtrise et un doctorat en économie de l’Université de Montréal."
  },
  {
    "objectID": "presentations/archives/2023-04-24.html",
    "href": "presentations/archives/2023-04-24.html",
    "title": "Spatial scale-aware tail dependence modeling for high-dimensional spatial extremes",
    "section": "",
    "text": "Résumé\nExtreme events over large spatial domains like the contiguous United States may exhibit highly heterogeneous tail dependence characteristics, yet most existing spatial extremes models yield only one dependence class over the entire spatial domain. To accurately characterize `storm dependence’ in analysis of extreme events, we propose a mixture model that achieves flexible dependence properties and allows high-dimensional inference for extremes of spatial processes. We modify the popular random scale construction that multiplies a (transformed) Gaussian random field by a single radial variable; that is, we add non-stationarity to the Gaussian process while allowing the radial variable to vary smoothly across space. As the level of extremeness increases, this single model exhibits both long-range asymptotic independence and short-range weakening dependence strength that can lead to either asymptotic dependence or independence.\n\n\nBiographie\nDr. Ben Shaby est professeur aggrégé au département de statistique du Colorado State University depuis 2019, en provenance de Pennsylvania State. Ses intérêts de recherche gravitent autour des modèles Bayésiens pour les données spatio-temporelles extrêmes. Dr. Shaby a reçu un doctorat de Cornell encadré par David Ruppert et Marty Wells avant de compléter des stages postdoctoraux à SAMSI, Duke et à l’Université de la Californie à Berkeley avec Cari Kaufman. Ben a reçu un CAREER Award de la NSF en 2019."
  },
  {
    "objectID": "presentations/archives/2023-11-10.html",
    "href": "presentations/archives/2023-11-10.html",
    "title": "Learning to Simulate Tail-risk Scenarios",
    "section": "",
    "text": "Résumé\nThe estimation of loss distributions for dynamic portfolios requires the simulation of scenarios representing realistic joint dynamics of their components. Scalability to large or heterogeneous portfolios involving multiple asset classes is particularly challenging, as is the accurate representation of tail risk.\nWe propose a novel data-driven approach for the simulation of realistic multi-asset scenarios with a particular focus on the accurate estimation of tail risk for a given class of static and dynamic portfolios selected by the user. By exploiting the joint elicitability property of Value-at-Risk (VaR) and Expected Shortfall (ES), we design a Generative Adversarial Network (GAN) architecture capable of learning to simulate price scenarios that preserve tail risk features for these benchmark trading strategies, leading to consistent estimators for their VaR and ES.\nFrom a theoretical perspective, we show that different choices of score functions lead to different optimization landscapes and different complexities in GAN training. In addition, we prove that the generator in our GAN architecture enjoys a universal approximation property under the criteria of tail risk measures. In addition, we prove the bi-level optimization formulation between the generator and the discriminator is equivalent to a max-min game, leading to a more effective and practical formulation for training. From an empirical perspective, we demonstrate the accuracy and scalability of our method via extensive simulation experiments using synthetic and market data. Our results show that, in contrast to other data-driven scenario generators, our proposed scenario simulation method correctly captures tail risk for both static and dynamic portfolios in the input datasets.\nThis is based on joint work with Rama Cont, Mihai Cucuringu, and Chao Zhang (Oxford).\n\n\nBiographie\nDr. Renyuan Xu est professeure adjointe WiSE Gabilan au départment de génie industriel et des systèmes à l’Université de la Californie du Sud. Avant de se joindre à USC, elle a passé deux années à Oxford avec Rama Cont comme fellow de recherche. Renyuan a obtenu son doctorat de l’Université de la Californie à Berkeley en 2019. Ses intérêts de recherche sont en théorie du contrôle et des jeux stochastiques, l’apprentissage par renforcement et l’apprentissage automatique avec application en finance et dans les transactions à haute fréquence."
  },
  {
    "objectID": "presentations/archives/2019-10-11.html",
    "href": "presentations/archives/2019-10-11.html",
    "title": "Optimizing the design of textual indices: Impression management and Sentometrics",
    "section": "",
    "text": "Résumé\nTextual sentiment analysis requires to select relevant texts and design weighting schemes to aggregate the intra-textual, across-text and across-time polarity expressed in textual communications. The selection and weighting decisions need to be optimized for the intended use case and account for the underlying data features like impression management by the author and the non i.i.d. nature of the textual data. The seminar will combine an overview of methodology together with application to CEO letters and media articles.\n\n\nBiographie\nKris Boudt is professor of finance and econometrics at Ghent University, Vrije Universiteit Brussel and Vrije Universiteit Amsterdam. He is part of the core team at Sentometrics. He teaches the online courses “Introduction to portfolio analysis in R” and “GARCH models in R” at Datacamp. He is also affiliated with the KU Leuven and an invited lecturer at the University of Illinois in Chicago, Renmin University, SWUFE and the University of Aix-Marseille. Kris Boudt obtained his PhD in 2008 for his developments in the modelling and estimation of financial risk under a non-normal distribution. He has published his research in the Journal of Econometrics, Journal of Portfolio Management, Journal of Statistical Software, and the Review of Finance, among others. Kris Boudt received several awards for outstanding research and refereeing and is an active contributor to the open source community."
  },
  {
    "objectID": "presentations/archives/2022-05-10.html",
    "href": "presentations/archives/2022-05-10.html",
    "title": "Learning and reasoning with constraint solving",
    "section": "",
    "text": "Résumé\nIndustry and society are increasingly automating processes, which requires solving constrained optimisation problems. This includes vehicle routing, demand-response planning, rostering and more. To find not just optimal solutions, but also ‘desirable’ solution by the end user, it is increasingly important to offer tools that automatically learn from the user and the environment and that support the constraint modelling in interpretable ways.\nIn this talk I will provide an overview of three different ways in which part of the problem specification can be learned from data. This includes learning from the user (preference learning in VRP), learning from the environment (end-to-end decision focussed learning) and explanation generation, that sit at the intersection of learning and reasoning.\nAs part of this work, we are building a modern constraint programming language called CPMpy(http://cpmpy.readthedocs.io) that eases integration of multiple constraint solving paradigms with machine learning and other scientific python libraries. I will shortly highlight its possibilities beyond the above cases, as well as our larger vision of conversational human-aware technology for optimisation.\n\n\nBiographie\nTias Guns is Associate Professor at the DTAI lab of KU Leuven, in Belgium. His research is at the intersection of machine learning and combinatorial optimisation.\nTias’ expertise is in the hybridisation of machine learning systems with constraint solving systems, more specifically building constraint solving systems that reason both on explicit knowledge as well as knowledge learned from data. For example learning the preferences of planners in vehicle routing, and solving new routing problems taking both operational constraints and learned human preferences into account; or building energy price predictors specifically for energy-aware scheduling, and planning maintenance crews based on expected failures. He was awarded a prestigious EU ERC Consolidator grant in 2021 to work on conversational human-aware technology for optimisation."
  },
  {
    "objectID": "presentations/archives/2024-02-23.html",
    "href": "presentations/archives/2024-02-23.html",
    "title": "Adaptive Bayesian predictive inference",
    "section": "",
    "text": "Résumé\nBayesian predictive inference provides a coherent description of entire predictive uncertainty through predictive distributions. We examine several widely used sparsity priors from the predictive (as opposed to estimation) inference viewpoint. Our context is estimating a predictive distribution of a high-dimensional Gaussian observation with a known variance but an unknown sparse mean under the Kullback–Leibler loss. First, we show that LASSO (Laplace) priors are incapable of achieving rate-optimal performance. This new result contributes to the literature on negative findings about Bayesian LASSO posteriors. However, deploying the Laplace prior inside the Spike-and-Slab framework (for example with the Spike-and-Slab LASSO prior), rate-minimax performance can be attained with properly tuned parameters (depending on the sparsity level sn). We highlight the discrepancy be- tween prior calibration for the purpose of prediction and estimation. Going further, we investigate popular hierarchical priors which are known to attain adaptive rate-minimax performance for estimation. Whether or not they are rate-minimax also for predictive inference has, until now, been unclear. We answer affirmatively by showing that hierarchical Spike-and-Slab priors are adaptive and attain the minimax rate without the knowledge of sn. This is the first rate-adaptive result in the literature on predictive density estimation in sparse setups. This finding celebrates benefits of a fully Bayesian inference.\n\n\nBiographie\nVeronika Ročková est professeure titulaire d’économétrie et de statistique à l’École de gestion Booth de l’Université de Chicago. Elle détient un doctorat en biostatistiques de l’université Erasmus Rotterdam, une maîtrise en mathématiques statistiques de l’Université Charles de Prague et une maîtrise en biostatistique de Hasselt en Belgique. Avant de venir à Chicago, Dre Ročková a complété un stage postdoctoral à l’école Warton de l’Université de Pennsylvanie. Son expertise se situe à l’intersection des méthodes bayésiennes et fréquentistes, notamment en ce qui a trait à la sélection de variables, la quantification de l’incertitude, les méthodes nonparamétriques bayésiennes, les modèles factoriels et dynamiques, la théorie de la décision en haute dimension. Dre Ročková a reçu le prix des Leaders émergents du COPPS en 2023 et une subvention CAREER en 2020. Elle siège sur le comité éditorial des revues Annals of Statistics, Journal of the American Statistical Association et du Journal of Royal Statistical Society (Series B)."
  },
  {
    "objectID": "presentations/archives/2021-10-04.html",
    "href": "presentations/archives/2021-10-04.html",
    "title": "Selective inference on trees",
    "section": "",
    "text": "Résumé\nThe ever-increasing scope and scale of data collection has shifted the focus of data collection away from testing pre-specified hypotheses and towards hypothesis generation. Researchers are often interested in performing exploratory data analysis on a data set to generate hypotheses, then to validate those hypotheses in that same data via tests of significance. Unfortunately, this type of “double-dipping” can lead to extremely inflated type I error rates.\nIn this talk, I will consider double-dipping on trees. First, I will focus on trees generated by hierarchical clustering, and consider testing for differences between clusters obtained by cutting the tree. I will propose a selective inference approach to test for a difference in means between two clusters that properly accounts for the fact that the choice of null hypothesis was made based on the data. Second, I will consider trees generated using the CART algorithm, and will again use a selective inference approach to conduct inference on the means of the terminal nodes. Applications include single-cell RNA-sequencing data and the Box Lunch Study.\nThis is joint work with Jacob Bien (University of Southern California), Daniela Witten (University of Washington), and Anna Neufeld (University of Washington).\n\n\nBiographie\nLucy Gao est professeur adjointe au département de statistique et de sciences actuarielles de l’Université de Waterloo. Elle est détentrice d’un doctorat en biostatistique de l’Université de Washington où elle a étudiée sous la gouverne de Daniela Witten.\nDr. Gao est intéressée par le développement de méthodologie statistique en lien avec la statistique multidimensionnelle, l’apprentissage statistique et l’inférence sélective, appliquée à des problèmes scientifiques en biologie et en sciences de la santé. Elle travaille également sur la planification optimale de devis expérimentaux."
  },
  {
    "objectID": "presentations/archives/2023-03-17.html",
    "href": "presentations/archives/2023-03-17.html",
    "title": "Predictive inference for travel time on transportation networks",
    "section": "",
    "text": "Résumé\nRecent statistical methods fitted on large-scale GPS data can provide accurate estimations of the expected travel time between two points. However, little is known about the distribution of travel time, which is key to decision-making across a number of logistic problems. With sufficient data, single road-segment travel time can be well approximated. The challenge lies in understanding how to aggregate such information over a route to arrive at the route-distribution of travel time. We develop a novel statistical approach to this problem. We show that, under general conditions, without assuming a distribution of speed, travel time divided by route distance follows a Gaussian distribution with route-invariant population mean and variance. We develop efficient inference methods for such parameters and propose asymptotically tight population prediction intervals for travel time. Using road-level information (e.g., traffic flow), we further develop a trip-specific Gaussian-based predictive distribution, resulting in tight prediction intervals for short and long trips. Our methods, implemented in an R package, are illustrated in a real-world case study using mobile GPS data, showing that our trip-specific and population intervals both achieve the 95% theoretical coverage levels. Compared to alternative approaches, our trip-specific predictive distribution achieves (a) the theoretical coverage at every level of significance, (b) tighter prediction intervals, (c) less predictive bias, and (d) more efficient estimation and prediction procedures. This makes our approach promising for low-latency, large-scale transportation applications.\n\n\nBiographie\nDr. Mohamad Elmasri est stagiaire postdoctoral au sein du département de statistique de l’Université de Toronto, financé par une bourse du CRSNG. Il était précédemment chercheur au sein de l’équipe ETA & Routing chez Lyft et postdoctorant au MILA et à HEC Montréal. Ses intérêts de recherche sont en statistique bayésienne et computationnelle, notamment pour l’analyse de réseaux et les modèles graphiques. Mohamad a complété son doctorat à McGill avec David Stephens et travaillé au sein de l’UNESCO."
  },
  {
    "objectID": "presentations/archives/2021-10-29.html",
    "href": "presentations/archives/2021-10-29.html",
    "title": "Opinionated practices for teaching reproducibility: motivation, guided instruction and practice",
    "section": "",
    "text": "Résumé\nIn the data science courses at the University of British Columbia, we define data science as the study, development and practice of reproducible and auditable processes to obtain insight from data. While reproducibility is core to our definition, most data science learners enter the field with other aspects of data science in mind, for example predictive modelling, which is often one of the most interesting topic to novices. This fact, along with the highly technical nature of the industry standard reproducibility tools currently employed in data science, present out-ofthe gate challenges in teaching reproducibility in the data science classroom. Put simply, students are not as intrinsically motivated to learn this topic, and it is not an easy one for them to learn. What can a data science educator do? Over several iterations of teaching courses focused on reproducible data science tools and workflows, we have found that providing extra motivation, guided instruction and lots of practice are key to effectively teaching this challenging, yet important subject. Here we present examples of how we deeply motivate, effectively guide and provide ample practice opportunities to data science students to effectively engage them in learning about this topic.\n\n\nBiographie\nTiffany Timbers est professeure adjointe volet enseignement au département de statistique de l’Université de la Colombie-Britannique (UBC) à Vancouver et co-directrice du programme de maîtrise en science des données (Vancouver) de UBC. Dans le cadre de son travail, elle enseigne et supervise le développement du cursus pour une application responsable de la science des données appliquée à des problèmes pratiques. Parmi ses cours favoris figure un cours des cycles supérieurs sur le développement de logiciels collaboratifs, qui traite de création de paquets R et Python à l’aide des outils et d’un flux de travail moderne."
  },
  {
    "objectID": "presentations/archives/2024-10-18.html",
    "href": "presentations/archives/2024-10-18.html",
    "title": "Disciplined Decisions in the Face of Uncertainty and Data",
    "section": "",
    "text": "Résumé\nProblem uncertainty typically limits how well decisions can be tailored to the problem at hand but often can not be avoided. The availability of large quantities of data in modern applications however presents an exciting opportunity to nevertheless make better informed decisions. Capitalizing on this opportunity requires developing novel tools on the intersection between operations research, stochastics as well as data science. In a modern setting the primitive describing uncertainty is often messy data rather than classical distributions. Simply quantifying the probability of an undesirable outcome becomes a challenging uncertainty quantification problem which I approach with a distributional optimization lens. Distributional robust optimization (DRO) has recently gained prominence as a paradigm for making data-driven decisions which are protected against adverse overfitting effects. We justify the effectiveness of this paradigm by pointing out that certain DRO formulations indeed enjoy optimal statistical properties. Furthermore, DRO formulations can also be tailored to efficiently protect decisions against overfitting even when working with messy corrupted data. Finally, as such formulations are often computationally tractable they provide a practical road to the development of tomorrow’s trustworthy decision systems.\n\n\nBiographie\nDr. Bart Van Parys is an associate professor in the stochastics group at the National research institute for mathematics and computer science (CWI) in Amsterdam, the Netherlands. Before joining CWI, he was an Assistant Professor in Operations Research and Statistics at the MIT Sloan School of Management. His research is located on the interface between optimization and machine learning, focusing on the development of novel mathematical methodologies and algorithms for better decision making based on data. Bart received a PhD degree with Manfred Morari from ETHZ in 2015."
  }
]