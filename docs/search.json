[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Séminaires à venir",
    "section": "",
    "text": "Nos séminaires sont webdiffusés via la plateforme Zoom. Remplissez le formulaire suivant pour recevoir les informations de connexion par courriel.\nOur seminars are broadcasted live on Zoom. You can sign-up using this form to get Zoom credentials by email.\n\n\n\n\n\n\nDate\n\n\nHeure\n\n\nInvité(e)\n\n\nTitre de l’exposé\n\n\nLocal\n\n\n\n\n\n\nven., 10 nov. 2023\n\n\n13h00\n\n\nRenyuan Xu\n\n\nLearning to Simulate Tail-risk Scenarios\n\n\nsalle Accra\n\n\n\n\nven., 24 nov. 2023\n\n\n15h30\n\n\nLeilei Zeng\n\n\nState-dependent Sampling in Observational Cohort Studies\n\n\nsalle 5340, pavillon André-Aisenstadt\n\n\n\n\nmer., 20 déc. 2023\n\n\n11h00\n\n\nMario Ghoussoub\n\n\nEquilibria in Centralized Insurance Markets: Monopolistic vs. Competitive Pricing\n\n\nsalle 4488, André-Aisenstadt\n\n\n\n\nven., 26 janv. 2024\n\n\n15h30\n\n\nTrevor Campbell\n\n\nÀ déterminer\n\n\nsalle à déterminer\n\n\n\n\nven., 2 févr. 2024\n\n\n13h30\n\n\nSamuel Burer\n\n\nÀ déterminer\n\n\nsalle à déterminer\n\n\n\n\nven., 16 févr. 2024\n\n\n10h00\n\n\nJim Luedtke\n\n\nÀ déterminer\n\n\nsalle à déterminer\n\n\n\n\nven., 23 févr. 2024\n\n\n15h30\n\n\nVeronika Ročková\n\n\nÀ déterminer\n\n\nsalle à déterminer\n\n\n\n\nven., 15 mars 2024\n\n\n15h30\n\n\nDaniele Durante\n\n\nÀ déterminer\n\n\nsalle à déterminer\n\n\n\n\nmer., 20 mars 2024\n\n\n10h30\n\n\nLucia Sbragia\n\n\nÀ déterminer\n\n\nsalle à déterminer\n\n\n\n\nven., 5 avr. 2024\n\n\n11h00\n\n\nNicholas Rivers\n\n\nÀ déterminer\n\n\nsalle à déterminer\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "presentations/archives/2023-03-17.html",
    "href": "presentations/archives/2023-03-17.html",
    "title": "Predictive inference for travel time on transportation networks",
    "section": "",
    "text": "Résumé\nRecent statistical methods fitted on large-scale GPS data can provide accurate estimations of the expected travel time between two points. However, little is known about the distribution of travel time, which is key to decision-making across a number of logistic problems. With sufficient data, single road-segment travel time can be well approximated. The challenge lies in understanding how to aggregate such information over a route to arrive at the route-distribution of travel time. We develop a novel statistical approach to this problem. We show that, under general conditions, without assuming a distribution of speed, travel time divided by route distance follows a Gaussian distribution with route-invariant population mean and variance. We develop efficient inference methods for such parameters and propose asymptotically tight population prediction intervals for travel time. Using road-level information (e.g., traffic flow), we further develop a trip-specific Gaussian-based predictive distribution, resulting in tight prediction intervals for short and long trips. Our methods, implemented in an R package, are illustrated in a real-world case study using mobile GPS data, showing that our trip-specific and population intervals both achieve the 95% theoretical coverage levels. Compared to alternative approaches, our trip-specific predictive distribution achieves (a) the theoretical coverage at every level of significance, (b) tighter prediction intervals, (c) less predictive bias, and (d) more efficient estimation and prediction procedures. This makes our approach promising for low-latency, large-scale transportation applications.\n\n\nBiographie\nDr. Mohamad Elmasri est stagiaire postdoctoral au sein du département de statistique de l’Université de Toronto, financé par une bourse du CRSNG. Il était précédemment chercheur au sein de l’équipe ETA & Routing chez Lyft et postdoctorant au MILA et à HEC Montréal. Ses intérêts de recherche sont en statistique bayésienne et computationnelle, notamment pour l’analyse de réseaux et les modèles graphiques. Mohamad a complété son doctorat à McGill avec David Stephens et travaillé au sein de l’UNESCO."
  },
  {
    "objectID": "presentations/archives/2023-04-14.html",
    "href": "presentations/archives/2023-04-14.html",
    "title": "Efficient evaluation of prediction rules in semi-supervised settings under stratified sampling",
    "section": "",
    "text": "Résumé\nIn many contemporary applications, large amounts of unlabelled data are readily available while labelled examples are limited. There has been substantial interest in semi-supervised learning (SSL) which aims to leverage unlabelled data to improve estimation or prediction. However, current SSL literature focuses primarily on settings where labelled data are selected uniformly at random from the population of interest. Stratified sampling, while posing additional analytical challenges, is highly applicable to many real-world problems. Moreover, no SSL methods currently exist for estimating the prediction performance of a fitted model when the labelled data are not selected uniformly at random. In this paper, we propose a two-step SSL procedure for evaluating a prediction rule derived from a working binary regression model based on the Brier score and overall misclassification rate under stratified sampling. In step I, we impute the missing labels via weighted regression with nonlinear basis functions to account for stratified sampling and to improve efficiency. In step II, we augment the initial imputations to ensure the consistency of the resulting estimators regardless of the specification of the prediction model or the imputation model. The final estimator is then obtained with the augmented imputations. We provide asymptotic theory and numerical studies illustrating that our proposals outperform their supervised counterparts in terms of efficiency gain. Our methods are motivated by electronic health record (EHR) research and validated with a real data analysis of an EHR-based study of diabetic neuropathy.\n\n\nBiographie\nDr. Jessica Gronsbell est professeure adjointe au département de statistique à l’Université de Toronto. Ses intérêts de recherche sont centrés sur le développement des méthodes statistiques et d’algorithme d’apprentissage automatique pour les dossiers médicaux électroniques et les données médicales mobiles. Avant de rejoindre l’Université de Toronto, Jessica a reçu un baccalauréat en mathématiques appliquées à l’Université de la Californie (Berkeley) et un doctorat en biostatistique sous la direction de Tianxi Cai à l’Université Harvard. Elle a ensuite fait un stage postdoctoral avec Lu Tian à Stanford et passé quelques années comme scientifique des données dans le groupe Mental Health Research and Development group au sein du groupe Verily Life Sciences chez Alphabet."
  },
  {
    "objectID": "presentations/archives/2022-03-23.html",
    "href": "presentations/archives/2022-03-23.html",
    "title": "Comparative Probability Metrics: Using Posterior Probabilities to Account for Practical Equivalence in A/B tests",
    "section": "",
    "text": "Résumé\nOnline controlled experiments (i.e., A/B tests) have become an extremely valuable tool used by internet and technology companies for purposes of advertising, product development, product improvement, customer acquisition, and customer retention to name a few. The data-driven decisions that result from these experiments are typically informed by null hypothesis significance tests and analyses based on p-values. However, attention has recently been drawn to the shortcomings of hypothesis testing, and an emphasis has been placed on the development of new methodologies that overcome these shortcomings. We propose the use of posterior probabilities to facilitate comparisons that account for practical equivalence and that quantify the likelihood that a result is practically meaningful, as opposed to statistically significant. We call these posterior probabilities comparative probability metrics (CPMs). This Bayesian methodology provides a flexible and intuitive means of making meaningful comparisons by directly calculating, for example, the probability that two variants are practically equivalent, or the probability that one variant is practically superior to another. In this talk we describe a unified framework for constructing and estimating such probabilities, and we discuss a sample size determination methodology that may be used to determine how much data is required to calculate trustworthy CPMs.\n\n\nBiographie\nDr. Nathaniel Stevens est professeur adjoint à l’Université de Waterloo au sein du département de statistique et de science actuarielle. Après avoir complété son doctorat en 2015 à l’Université de Waterloo, il a travaillé de 2015 à 2018 à l’Université de San Francisco, où il a développé le programme de maîtrise en science des données. Nathaniel est un spécialiste des devis expérimentaux et des tests A/B. Il est actuellement directeur du programme BISRG à Waterloo et oeuvre à l’interface de la science des données et des statistiques industrielles. Le côté novateur de ses publications a été récompensé, notamment avec le prix Søren Bisgaard en 2021 pour un article paru dans Quality Engineering. Nathaniel est actuellement président du groupe de Science des données de la Société statistique du Canada."
  },
  {
    "objectID": "presentations/archives/2019-09-10.html",
    "href": "presentations/archives/2019-09-10.html",
    "title": "Optimal picking policies for e-commerce warehouses",
    "section": "",
    "text": "Résumé\nIn e-commerce warehouses, online retailers increase their efficiency by using a mixed-shelves (or scattered storage) concept, where unit loads are purposefully broken down into single items, which are individually stored in multiple locations. Irrespective of the stock keeping units a customer jointly orders, this storage strategy increases the likelihood that somewhere in the warehouse the items of the requested stock keeping units will be in close vicinity, which may significantly reduce an order picker’s unproductive walking time. In this talk, we optimize picker routing through such mixed-shelves warehouses. Specifically, we introduce a generic exact algorithmic framework that covers a multitude of picking policies, independently of the underlying picking zone layout, and is suitable for real-time applications. This framework embeds a bidirectional layered graph algorithm which provides the best known performance for the simple picking problem with a single depot and no further attributes. We compare three different real-world e-commerce warehouse settings that differ slightly in their application of scattered storage and in their picking policies. Based on these, we derive additional layouts and settings that yield further managerial insights. Our results reveal that the right combination of drop-off points, dynamic batching, the utilization of picking carts, and the picking zone layout can greatly improve the picking performance. In particular, some combinations of policies yield efficiency increases of more than 20%.\n\n\nBiographie\nMaximilian Schiffer is an Assistant Professor of Operations and Supply Chain Management at TUM School of Management, Technical University of Munich. Before joining TU Munich, he was a Visiting Postdoctoral Scholar at Stanford University and a Postdoctoral Scholar at RWTH Aachen University. Dr. Schiffer is an Associate Member of the GERAD. He received a Ph.D. degree in Operations Research from RWTH Aachen University in 2017. Dr. Schiffer’s expertise lies in in the field of Operations Research and Management Science, specifically in Dynamic Programming, (Mixed) Integer Programming, Metaheuristics, Exacts, Robust Optimization, Machine Learning, and Forecasting, applied to a variety of complex application fields, e.g., Transportation Problems, Supply Chains, Production Networks, and Big Data. His research on electric vehicles and logistics networks with intermediate stops has been awarded with numerous prizes, e.g., the INFORMS TSL Dissertation Prize and the GOR Doctoral Dissertation Prize."
  },
  {
    "objectID": "presentations/archives/2022-10-14.html",
    "href": "presentations/archives/2022-10-14.html",
    "title": "Optimal and robust combination of forecasts via constrained optimization and shrinkage",
    "section": "",
    "text": "Résumé\nWe introduce various methods that combine forecasts using constrained optimization with penalty. A non-negativity constraint is imposed on the weights, and several penalties are considered, taking the form of a divergence from a reference combination scheme. In contrast with most of the existing approaches, our framework performs forecast selection and combination in one step, allowing for potentially sparse combining schemes. Moreover, by exploiting the analogy between forecasts combination and portfolio optimization, we provide the analytical expression of the optimal penalty strength when penalizing with the L2-divergence from the equally-weighted scheme. An extensive simulation study and two empirical applications allow us to investigate the impact of the divergence function, the reference scheme, and the non-negativity constraint on the predictive performance. Our results suggest that the proposed models outperform those considered in previous studies.\n\n\nBiographie\nFrédéric Vrins a obtenu un PhD de l’École polytechnique de Louvain (UCLouvain) en 2007 dans le domaine du traitement de signaux adaptifs. Il a travaillé sur des méthodes de séparations de signaux dans le secteur biomédical, avant de se réorienter dans le secteur bancaire où il a travaillé comme analyste quantitatif pendant 7 ans. Depuis 2014, il est professeur titulaire au sein de l’institut de finance quantitative de l’École de gestion de Louvain, en plus d’être professeur affilié à HEC Montréal."
  },
  {
    "objectID": "presentations/archives/2021-04-07.html",
    "href": "presentations/archives/2021-04-07.html",
    "title": "When does eco-efficiency rebound or backfire? An analytical model",
    "section": "",
    "text": "Résumé\nIt is known that an eco-efficiency strategy, which saves resources in the production process, may be offset by a rebound effect; it may even backfire. Less known are the exact conditions under which eco-efficiency rebounds or backfires. This article fills the gap by providing an analytical model of the rebound and backfire effects. We propose an optimal control framework of dynamic pricing and eco-efficiency investment, for which eco-efficiency reduces the unit production cost and boosts the demand of environmentally concerned consumers. Results, which hold with a general demand formulation, examine the analytic conditions for the rebound and backfire effects. They also highlight the possibility of a reverse rebound effect. Such results pave the way to sounder sustainability strategies.\n\n\nBiographie\nRégis Chevanaz is an associate professor of managerial economics and operations management at KEDGE Business School. His research focuses on operations management, firm micro-economics, and business analytics. He holds a PhD degree from Telecom Paris and an HDR from Université Nice Sophia Antipolis and his research has been featured notably in the European Journal of Operational Research and in Applied Economics."
  },
  {
    "objectID": "presentations/archives/2023-02-24.html",
    "href": "presentations/archives/2023-02-24.html",
    "title": "Personalized Dynamic Pricing with Machine Learning: High Dimensional Features and Heterogeneous Elasticity",
    "section": "",
    "text": "Résumé\nWe consider a seller who can dynamically adjust the price of a product at the individual customer level, by utilizing information about customers’ characteristics encoded as a \\(d\\)-dimensional feature vector. We assume a personalized demand model, parameters of which depend on \\(s\\) out of the \\(d\\) features. The seller initially does not know the relationship between the customer features and the product demand, but learns this through sales observations over a selling horizon of \\(T\\) periods. We prove that the seller’s expected regret, i.e., the revenue loss against a clairvoyant who knows the underlying demand relationship, is at least of order \\(s\\sqrt{T}\\)% under any admissible policy. We then design a near-optimal pricing policy for a “semi-clairvoyant” seller (who knows which \\(s\\) of the \\(d\\) features are in the demand model) that achieves an expected regret of order \\(s\\sqrt{T}\\log T\\). We extend this policy to a more realistic setting where the seller does not know the true demand predictors, and show that this policy has an expected regret of order \\(s\\sqrt{T} (\\log d＋\\log T)\\), which is also near-optimal. Finally, we test our theory on simulated data and on a data set from an online auto loan company in the United States. On both data sets, our experimentation-based pricing policy is superior to intuitive and/or widely-practiced customized pricing methods such as myopic pricing and segment-then-optimize policies. Furthermore, our policy improves upon the loan company’s historical pricing decisions by 47% in expected revenue over a six-month period.\n\n\nBiographie\nDr. Gah-Yi Ban est professeur agrégée à l’école de gestion de Imperial College à Londres et anciennement de la London Business School et de l’Université du Maryland. Elle détient un doctorat en recherche opérationnelle de l’Université de Californie à Berkeley (UCB) et sert en qualité de rédactrice ajointe de Management Science, M&SOM et de Operations Research Letter. Ses intérêts de recherche sont en analytique des mégadonnées, plus spécialement la prise de décision dans des environnements incertains en haute-dimension avec des applications en gestion. Ses recherches ont été couronnées du prix Best OM Paper in Operations Research award en 2021."
  },
  {
    "objectID": "presentations/archives/2019-10-11.html",
    "href": "presentations/archives/2019-10-11.html",
    "title": "Optimizing the design of textual indices: Impression management and Sentometrics",
    "section": "",
    "text": "Résumé\nTextual sentiment analysis requires to select relevant texts and design weighting schemes to aggregate the intra-textual, across-text and across-time polarity expressed in textual communications. The selection and weighting decisions need to be optimized for the intended use case and account for the underlying data features like impression management by the author and the non i.i.d. nature of the textual data. The seminar will combine an overview of methodology together with application to CEO letters and media articles.\n\n\nBiographie\nKris Boudt is professor of finance and econometrics at Ghent University, Vrije Universiteit Brussel and Vrije Universiteit Amsterdam. He is part of the core team at Sentometrics. He teaches the online courses “Introduction to portfolio analysis in R” and “GARCH models in R” at Datacamp. He is also affiliated with the KU Leuven and an invited lecturer at the University of Illinois in Chicago, Renmin University, SWUFE and the University of Aix-Marseille. Kris Boudt obtained his PhD in 2008 for his developments in the modelling and estimation of financial risk under a non-normal distribution. He has published his research in the Journal of Econometrics, Journal of Portfolio Management, Journal of Statistical Software, and the Review of Finance, among others. Kris Boudt received several awards for outstanding research and refereeing and is an active contributor to the open source community."
  },
  {
    "objectID": "presentations/archives/2020-12-04.html",
    "href": "presentations/archives/2020-12-04.html",
    "title": "Risk-neutral moments based estimation of continuous time jump-diffusion models",
    "section": "",
    "text": "Résumé\nThis paper provides a novel methodology for estimating option pricing models based on risk‐neutral moments. We synthesize the distribution extracted from a panel of option prices and exploit linear relationships between risk‐neutral cumulants and latent factors within the continuous time affine stochastic volatility framework. We find that fitting the Andersen et al. (Journal of Financial Economics, 2015, 117(3), 558–584) option valuation model to risk‐neutral moments captures the bulk of the information in option prices. Our estimation strategy is effective, easy to implement, and robust, as it allows for a direct linear filtering of the latent factors and a quasi‐maximum likelihood estimation of model parameters. From a practical perspective, employing risk‐neutral moments instead of option prices also helps circumvent several sources of numerical errors and substantially lessens the computational burden inherent in working with a large panel of option contracts.\n\n\nBiographie\nBruno Feunou is a Research Advisor at the Bank of Canada’s Financial Markets Department. Before this position at the Bank of Canada, he worked at Duke University as a post-doc associate. He completed his Ph.D degree at the University of Montreal. Bruno Feunou research interest are option and bond pricing, the linkages between term structure of interest rate and the macroeconomy."
  },
  {
    "objectID": "presentations/archives/2020-09-11.html",
    "href": "presentations/archives/2020-09-11.html",
    "title": "Machine Learning for Causal Inference",
    "section": "",
    "text": "Résumé\nGiven advances in machine learning over the past decades, it is now possible to accurately solve difficult non-parametric prediction problems in a way that is routine and reproducible. In this talk, I’ll discuss how machine learning tools can be rigorously integrated into observational study analyses, and how they interact with classical statistical ideas around randomization, semiparametric modeling, double robustness, etc. I’ll also survey some recent advances in methods for treatment heterogeneity. When deployed carefully, machine learning enables us to develop causal estimators that reflect an observational study design more closely than basic linear regression based methods.\n\n\nBiographie\nStefan Wager is an assistant professor of Operations, Information, and Technology at Stanford University. He completed a PhD in statistics at the same university in 2016 with Brad Efron and Guenther Walther, and spent a year as a postdoctoral researcher at Columbia University. His research focuses on adapting ideas from machine learning to statistical problems that arise in scientific applications. His research interests are broad anc include causal inference, non-parametric statistics, uses of subsampling for data analysis, and empirical Bayes methods."
  },
  {
    "objectID": "presentations/archives/2023-05-18.html",
    "href": "presentations/archives/2023-05-18.html",
    "title": "Consensus and Dissensus in Multi-population Multi-agent Systems",
    "section": "",
    "text": "Résumé\nThe talk will start with a general overview of mean field games approach to decision making in multi-agent dynamical systems in both model-based and model-free settings and discuss connections to finite-population games. Following this general introduction, the talk will focus on the structured setting of discrete-time infinite-horizon linear-quadratic-Gaussian dynamic games, where the players are partitioned into finitely-many populations with an underlying graph topology — a framework motivated by paradigms where consensus and dissensus co-exist. Mean field games approach will be employed to arrive at approximate Nash equilibria, and learning algorithms will be presented for the model-free setting, along with sample complexity analysis.\n\n\nBiographie\nTamer Başar est titulaire émérite de la chaire dotée Swanlund et professeur émérite au Center for Advanced Study du département d’ingénierie électrique et informatique de l’Université de l’Illinois à Urbana-Champaign. Il est également professeur de recherche au Coordinated Science Laboratory et au Information Trust Institute et directeur exécutif de Illinois at Singapore Pte. Ltd. Dr. Başar est détenteur d’un Ph.D. de l’Université Yale. Il est le (co-)auteur de près de 1000 publications dans les domaines du contrôle optimal, robuste et adaptif, stochastique, de la théorie de l’estimation et des jeux dynamiques. Il est également l’auteur de l’ouvrage Dynamic Noncooperative Game Theory, coauteur de Game Theory for Next Generation Wireless and Communication Networks: Modeling, Analysis, and Design et (co-)éditeur de plusieurs ouvrages de référence dans le domaine. Tamer Başar a reçu de nombreux prix durant sa carrière: il est notamment membre de l’académie nationale d’ingénierie des États-Unis et fellow de l’IEEE depuis 1983. Une biographie complète est disponible en ligne."
  },
  {
    "objectID": "presentations/archives/2023-04-24.html",
    "href": "presentations/archives/2023-04-24.html",
    "title": "Spatial scale-aware tail dependence modeling for high-dimensional spatial extremes",
    "section": "",
    "text": "Résumé\nExtreme events over large spatial domains like the contiguous United States may exhibit highly heterogeneous tail dependence characteristics, yet most existing spatial extremes models yield only one dependence class over the entire spatial domain. To accurately characterize `storm dependence’ in analysis of extreme events, we propose a mixture model that achieves flexible dependence properties and allows high-dimensional inference for extremes of spatial processes. We modify the popular random scale construction that multiplies a (transformed) Gaussian random field by a single radial variable; that is, we add non-stationarity to the Gaussian process while allowing the radial variable to vary smoothly across space. As the level of extremeness increases, this single model exhibits both long-range asymptotic independence and short-range weakening dependence strength that can lead to either asymptotic dependence or independence.\n\n\nBiographie\nDr. Ben Shaby est professeur aggrégé au département de statistique du Colorado State University depuis 2019, en provenance de Pennsylvania State. Ses intérêts de recherche gravitent autour des modèles Bayésiens pour les données spatio-temporelles extrêmes. Dr. Shaby a reçu un doctorat de Cornell encadré par David Ruppert et Marty Wells avant de compléter des stages postdoctoraux à SAMSI, Duke et à l’Université de la Californie à Berkeley avec Cari Kaufman. Ben a reçu un CAREER Award de la NSF en 2019."
  },
  {
    "objectID": "presentations/archives/2020-03-05.html",
    "href": "presentations/archives/2020-03-05.html",
    "title": "Tail risk and style dependence in the fund industry: A multivariate extreme value approach",
    "section": "",
    "text": "Résumé\nIn this paper, we study the connectedness between extreme losses of hedge funds, a crucial feature for systemic risk management. To do so, we exploit cross-sections of hedge funds monthly returns grouped by investment styles, and build a time- varying measure of tail dependence across styles. Relying on extreme value theory and regression techniques, we study the dynamics of the tail dependencies between fund styles conditional on factors reflecting the economic uncertainty and the stock market performance. The resulting tail dependence measures are used to construct a time-varying network between extreme losses of the various investment styles. We show that during a crisis period, while the extremal dependence between some pairs of investment styles remains stable, other pairs show a striking increase of their extremal connectedness. Our results highlight that a proactive regulatory framework should account for the dynamic nature of the tail dependence and its link with financial stress.\nThis is a joint work with Julien Hambuckers and Marie Lambert.\n\n\nBiographie\nLinda Mhalla is a postdoctoral researcher at HEC Montréal. She did her studies in Mathematics and Statistics at EPFL and completed her PhD in 2018 from the University of Geneva, under the supervision of Prof. Valérie Chavez-Demoulin and Prof. Elvezio Ronchetti. Her research interests are in extreme value theory, tail dependence modelling, and causal inference. She is the recipient of a postdoctoral fellowship from the Swiss National Science Foundation."
  },
  {
    "objectID": "presentations/archives/2023-10-11.html",
    "href": "presentations/archives/2023-10-11.html",
    "title": "Disaster Resilient Cities: An OR Approach to Disaster Management",
    "section": "",
    "text": "Résumé\nThere have been decades of research in the field of humanitarian logistics, and academics in the field of logistics are becoming more and more interested in it. That being said, we still acquire insight and identify new problems with each disaster.\nWe have also observed various humanitarian logistics applications during Covid-19, e.g. for PCR test sites and vaccination centers. Unfortunately, the recent earthquake in Turkey has led us to re-evaluate the response cycle of disaster management. Close inspection reveals that this response phase actually leads to a variety of new applications of distribution logistics problems.\nWe have conducted many meetings and workshops with municipalities that were very active during the response of the Maraş Earthquakes. Many municipalities aim to have “earthquake resistant cities” with correct action plans and being ready for potential disasters. Based on our discussions with these municipalities, we have developed an “ideal action plan”. We also investigated the potential decision problems and linked these problems with OR literature.\n\n\nBiographie\nDr. Bahar Y. Kara est professeure titulaire au département de génie industriel de l’Université Bilkent, où elle a effectué toutes ses études et de laquelle elle a obtenu un doctorat sous la direction de B.C. Tansel en 1999. Elle a effectué un postdoctorat en gestion à McGill avant de retourner en Turquie. Ses intérêts de recherches sont en logistique, notamment logistique humanitaire et distribution, construction de réseau et problèmes de routages. Dr. Kara est membre fondatrice du comité exécutive EURO pour les opérations humanitaires (HOpe). Elle est également membre du conseil exécutif de l’Université Bilkent et la société turque de recherche opérationnelle. Dr. Kara a édité quatre ouvrages de référence et publié plus de 80 articles. Elle est rédactrice adjointe de Transportation Research-Part B, du Journal of Operational Research Society, de IIE Transactions, et de Socio Economic Planning and Science."
  },
  {
    "objectID": "presentations/archives/2020-01-09.html",
    "href": "presentations/archives/2020-01-09.html",
    "title": "Some statistical applications of generative neural networks",
    "section": "",
    "text": "Résumé\nWe present some examples of how the field of statistics can benefit from the Deep Learning movement. First, using generative neural networks (GNNs), we are now able to produce quasi Monte Carlo samples from “almost any” copula model. For example, we can do this even for mixtures of copulas with singular components. Second, using GNNs, we can now model and, most importantly, forecast multivariate time series without having to restrict ourselves to using only a few parametric copula families to describe the underlying multivariate dependence. We have empirical evidence that a better dependence model does indeed translate into better forecasts.\nThis is joint work with Marius Hofert and Avinash Prasad.\n\n\nBiographie\nMu Zhu is a professor of statistics at the University of Waterloo and a Fellow of the American Statistical Association. A Phi Beta Kappa graduate of Harvard University, he obtained his PhD from Stanford University. His research has received a prestigious Discovery Accelerator Supplement Award from the Natural Sciences and Engineering Research Council of Canada (NSERC). Mu’s main research interests are machine learning, multivariate analysis, and network data analysis, with their applications to bioinformatics, health informatics, and data mining."
  },
  {
    "objectID": "presentations/archives/2023-10-13.html",
    "href": "presentations/archives/2023-10-13.html",
    "title": "Managing Hedge Fund Liquidity Risks",
    "section": "",
    "text": "Résumé\nWe study hedge fund optimal portfolios in the presence of market and funding liquidity risks. We consider a two-period economy with a single hedge fund. The fund has access to cash which is available every period and to an illiquid asset which pays off only at the end of the second period. Funding liquidity risk takes the form of a random proportion of the fund’s assets under management being withdrawn by clients in period one. The fund can then liquidate a part of the illiquid position by bidding on a secondary market where a random haircut on the effective selling price is applied. We solve the allocation problem of the fund and find its optimal portfolio. Whereas the cash buffer is monotonously decreasing in the secondary market liquidity, we show that the fund’s default probability is bell-shaped. Finally, we apply our model in an asset pricing framework for different hedge fund strategies to see how both risks are priced over time. This is joint work with Guillaume Roussellet (McGill University).\n\n\nBiographie\nSerge Darolles est Professeur de Finance à l’Université Paris-Dauphine où il enseigne l’économétrie de la finance et la finance empirique depuis 2012. Avant de rejoindre Dauphine, il a travaillé pour Lyxor Asset Management entre 2000 et 2012, où il a développé des modèles mathématiques pour diverses stratégies d’investissement. Il a également occupé des postes de consultants à la Caisse des Dépôts et Consignations, la Banque Paribas et le Commissariat à l’Energie Atomique. Serge est spécialisée est spécialisé en économétrie de la finance et a écrit de nombreux articles publiés dans de nombreuses revues académiques. Il est également membre du Conseil consultatif scientifique de l’AMF. Serge est titulaire d’un doctorat en mathématiques appliquées de l’Université de Toulouse et d’un DESS de l’ENSAE, Paris."
  },
  {
    "objectID": "presentations/archives/2020-11-05.html",
    "href": "presentations/archives/2020-11-05.html",
    "title": "Developing an Open Energy Outlook for the United States Using Tools for Energy Model Optimization and Analysis (Temoa)",
    "section": "",
    "text": "Résumé\nClimate change coupled with rapid technological innovation is driving large scale change in the global energy system. Computer models of the energy system – referred to as energy optimization system models – provide a way to examine future energy system evolution, test the effects of proposed policy, and explore the role of future uncertainty. In this talk, I will describe Tools for Energy Model Optimization and Analysis (Temoa), an open source energy system optimization model designed to conduct rigorous uncertainty analysis. Temoa is currently being used to develop an Open Energy Outlook for the United States, which aims to apply best practices of policy-focused energy system modeling, ensure transparency, build a networked community, and work towards a common purpose: examining possible U.S. energy system futures to inform energy and climate policy efforts.\n\n\nBiographie\nDr. DeCarolis is a Professor in the Department of Civil, Construction, and Environmental Engineering at North Carolina State University. His research program is focused on addressing energy and environmental challenges at the intersection of engineering, economics, and public policy. His primary focus is the development and application of energy system models to analyze how energy technology and public policy can shape a sustainable future. He received his PhD in Engineering and Public Policy from Carnegie Mellon University in 2004 and is the recipient of an NSF CAREER Award."
  },
  {
    "objectID": "presentations/archives/2021-10-04.html",
    "href": "presentations/archives/2021-10-04.html",
    "title": "Selective inference on trees",
    "section": "",
    "text": "Résumé\nThe ever-increasing scope and scale of data collection has shifted the focus of data collection away from testing pre-specified hypotheses and towards hypothesis generation. Researchers are often interested in performing exploratory data analysis on a data set to generate hypotheses, then to validate those hypotheses in that same data via tests of significance. Unfortunately, this type of “double-dipping” can lead to extremely inflated type I error rates.\nIn this talk, I will consider double-dipping on trees. First, I will focus on trees generated by hierarchical clustering, and consider testing for differences between clusters obtained by cutting the tree. I will propose a selective inference approach to test for a difference in means between two clusters that properly accounts for the fact that the choice of null hypothesis was made based on the data. Second, I will consider trees generated using the CART algorithm, and will again use a selective inference approach to conduct inference on the means of the terminal nodes. Applications include single-cell RNA-sequencing data and the Box Lunch Study.\nThis is joint work with Jacob Bien (University of Southern California), Daniela Witten (University of Washington), and Anna Neufeld (University of Washington).\n\n\nBiographie\nLucy Gao est professeur adjointe au département de statistique et de sciences actuarielles de l’Université de Waterloo. Elle est détentrice d’un doctorat en biostatistique de l’Université de Washington où elle a étudiée sous la gouverne de Daniela Witten.\nDr. Gao est intéressée par le développement de méthodologie statistique en lien avec la statistique multidimensionnelle, l’apprentissage statistique et l’inférence sélective, appliquée à des problèmes scientifiques en biologie et en sciences de la santé. Elle travaille également sur la planification optimale de devis expérimentaux."
  },
  {
    "objectID": "presentations/archives/2023-05-19.html",
    "href": "presentations/archives/2023-05-19.html",
    "title": "Markov Chain-based Policies for Multi-stage Stochastic Integer Linear Programming",
    "section": "",
    "text": "Résumé\nWe introduce a novel aggregation framework to address multi-stage stochastic programs with mixed-integer state variables and continuous local variables (MSILPs). Our aggregation framework imposes additional structure to the integer state variables by leveraging the information of the underlying stochastic process, which is modeled as a Markov chain (MC). We present an exact solution method to the aggregated MSILP, which can also be used in an approximation form to obtain dual bounds and implementable feasible solutions. Moreover, we apply two-stage linear decision rule (2SLDR) approximations and propose MC-based variants to obtain high-quality decision policies with significantly reduced computational effort. We test the proposed methodologies in a novel MSILP model for hurricane disaster relief logistics planning.\n\n\nBiographie\nMerve Bodur is an Assistant Professor in the Department of Mechanical and Industrial Engineering at the University of Toronto. She also held a Dean’s Spark Professorship in the Faculty of Applied Science and Engineering (2018-2021). She is a faculty associate of University of Toronto Transportation Research Institute, Smart Freight Centre, and Centre for Healthcare Engineering at the University of Toronto. Currently, she is the INFORMS Optimization Society Vice Chair of Integer and Discrete Optimization. She obtained her Ph.D. from University of Wisconsin-Madison and did a postdoc at Georgia Institute of Technology. She received her B.S. in Industrial Engineering and B.A. in Mathematics from Bogazici University, Turkey. Her research interests include stochastic programming, integer programming, multiobjective optimization and combinatorial optimization, with applications in a variety of areas such as scheduling, transportation, power systems, healthcare and telecommunications."
  },
  {
    "objectID": "presentations/archives/2023-10-05.html",
    "href": "presentations/archives/2023-10-05.html",
    "title": "Strategic Sector Coupling? Market Power in Heat and Power Markets",
    "section": "",
    "text": "Résumé\nDecarbonisation of the power sector envisages vast uptake of variable renewable energy (VRE) technologies. Coupling between heat and power sectors via combined heat and power (CHP) plants could provide the flexibility needed to mitigate intermittent VRE output. However, firms with CHP plants could use the link between the two energy sectors to manipulate electricity prices. We use a bi-level model to investigate the incentives for such strategic behaviour. At the upper level, a firm with both heat-only and CHP plants determines its heat output and is constrained by power-market operations at the lower level. Such a strategic firm produces more (less) heat from its CHP (heat-only) plant vis-à-vis the social optimum in order to constrain its maximum power output. Thus, it uses its leverage to manufacture scarcity in the power market to boost the electricity price. In order to mitigate welfare losses from such strategic behaviour, we devise a regulatory package consisting of (i) a subsidy on CHP output and (ii) a modification to the heat contract. This is joint work with Sebastian Maier (University College London)\n\n\nBiographie\nDr. Afzal S. Siddiqui est titulaire d’un doctorat en génie industriel et opérations de l’Université de la Californie à Berkeley et est professeur au département de science et systèmes informatiques de l’Université de Stockholm. Il était précédemment professeur d’économie de l’énergie au département des sciences statistique de UCL et professeur affilié à Aalto et HEC Montréal. Ses intérêts de recherche sont dans l’application des méthodes de recherche opérationnelle pour l’analyse de la prise de décision dans un contexte d’incertitude et de compétition dans le secteur énergétique."
  },
  {
    "objectID": "presentations/archives/2021-02-19.html",
    "href": "presentations/archives/2021-02-19.html",
    "title": "Joint integrative analysis of multiple data sources with correlated vector outcomes",
    "section": "",
    "text": "Résumé\nWe consider the joint estimation of regression parameters from multiple potentially heterogeneous data sources with correlated vector outcomes. The primary goal of this joint integrative analysis is to estimate covariate effects on all vector outcomes through a marginal regression model in a statistically and computationally efficient way. We present a general class of distributed estimators that can be implemented in a parallelized computational scheme. Modelling, computational and theoretical challenges are overcome by first fitting a local model within each data source and then combining local results while accounting for correlation between data sources. This approach to distributed estimation and inference is formulated using Hansen’s generalized method of moments but implemented via an asymptotically equivalent and communication-efficient meta-estimator. We show both theoretically and numerically that the proposed method yields efficiency improvements and is computationally fast. We illustrate the proposed methodology with the joint integrative analysis of metabolic pathways in a large multi-cohort study.\n\n\nBiographie\nEmily Hector is an Assistant Professor of Statistics at North Carolina State University. She earned a B.Sc. in mathematics (Honours Probability and Statistics) from McGill University and a PhD in Biostatistics at the University of Michigan, working under the supervision of Peter X.-K. Song. Her current methodological interests revolve around data integration, especially of correlated, heterogeneous, high-dimensional data, estimating equations and the generalized method of moments and methods that leverage recent computing and algorithmic developments, with applications in metabolomics, neuroimaging and wearable devices."
  },
  {
    "objectID": "presentations/archives/2022-02-25.html",
    "href": "presentations/archives/2022-02-25.html",
    "title": "Differentially private inference via noisy optimization",
    "section": "",
    "text": "Résumé\nWe propose a general optimization-based framework for computing differentially private M-estimators and a new method for constructing differentially private confidence regions. Firstly, we show that robust statistics can be used in conjunction with noisy gradient descent or noisy Newton methods in order to obtain optimal private estimators with global linear or quadratic convergence, respectively. We establish local and global convergence guarantees, under both local strong convexity and self-concordance, showing that our private estimators converge with high probability to a nearly optimal neighborhood of the non-private M-estimators. Secondly, we tackle the problem of parametric inference by constructing differentially private estimators of the asymptotic variance of our private M-estimators. This naturally leads to approximate pivotal statistics for constructing confidence regions and conducting hypothesis testing. We demonstrate the effectiveness of a bias correction that leads to enhanced small-sample empirical performance in simulations. We illustrate the benefits of our methods in several numerical examples.\n\n\nBiographie\nDr. Marco Avella est professeur adjoint en prétitularisation conditionnelle au département de statistique de l’Université Columbia. Il a complété un doctorat au sein du Geneva School of Economics and Management (GSEM) à l’Université de Genève sous la direction d’Elvezio Ronchetti et un stage postdoctoral au MIT. Son domaine d’expertise principal se situe à l’intersection de la statistique robuste, l’apprentissage automatique et les données en haute dimension."
  },
  {
    "objectID": "presentations/archives/2021-03-17.html",
    "href": "presentations/archives/2021-03-17.html",
    "title": "Convex Optimization",
    "section": "",
    "text": "Résumé\nConvex optimization has emerged as useful tool for applications that include data analysis and model fitting, resource allocation, engineering design, network design and optimization, finance, and control and signal processing. After an overview of the mathematics, algorithms, and software frameworks for convex optimization, we turn to common themes that arise across applications, such as sparsity and relaxation. We describe recent work on real-time embedded convex optimization, in which small problems are solved repeatedly in millisecond or microsecond time frames.\n\n\nBiographie\nStephen P. Boyd is the Samsung Professor of Engineering, Professor of Electrical Engineering in the Information Systems Laboratory, and chair of the Electrical Engineering Department at Stanford University. His current research focus is on convex optimization applications in control, signal processing, machine learning, and finance.\nProfessor Boyd received a PhD in EECS from U. C. Berkeley in 1985. In 1985 he joined the faculty of Stanford’s Electrical Engineering Department. He is the author of many research articles and four books. His group has produced many open source tools, including CVX (with Michael Grant), CVXPY (with Steven Diamond), Convex.jl (with Madeleine Udell and others), and CVXR (with Anqi Fu and A. Narasimhan), widely used parser-solvers for convex optimization. His group’s CVXGEN software is used in many applications, including the SpaceX Falcon 9 landing system. Stephen P. Boyd has received many awards and honors for his research in control systems engineering and optimization, including an ONR Young Investigator Award, a Presidential Young Investigator Award, and the AACC Donald P. Eckman Award. In 2013, he received the IEEE Control Systems Award, given for outstanding contributions to control systems engineering, science, or technology. In 2012, Michael Grant and he were given the Mathematical Optimization Society’s Beale-Orchard-Hays Award, given every three years for excellence in computational mathematical programming. He is a Fellow of the IEEE, SIAM, and INFORMS, a Distinguished Lecturer of the IEEE Control Systems Society, a member of the US National Academy of Engineering (NAE), a foreign member of the Chinese Academy of Engineering (CAE), and a foreign member of the National Academy of Engineering of Korea (NAEK). He has been invited to deliver more than 90 plenary and keynote lectures at major conferences in control, optimization, signal processing, and machine learning."
  },
  {
    "objectID": "presentations/archives/2023-01-23.html",
    "href": "presentations/archives/2023-01-23.html",
    "title": "Records Analysis in Climate Attribution",
    "section": "",
    "text": "Résumé\nNumerical climate models are complex and combine a large number of physical processes. They are key tools in quantifying the relative contribution of potential anthropogenic causes (e.g., the current increase in greenhouse gases) on high-impact atmospheric variables like heavy rainfall or temperatures. These so-called climate extreme event attribution problems are particularly challenging in a multivariate context, that is, when the atmospheric variables are measured on a possibly high-dimensional grid. In addition, global climate models like any in sillico numerical experiments are affected by different types of bias. In this talk, I will discuss about how to combine to two statistical theories to assess causality in the context of extreme event attribution. In addition, the question of uncertainties quantification that remains a challenge in any climate attribution analysis will be explored from various directions. In particular, a simple model bias correction step for records will described in details. To illustrate our approach, we infer emergence times in precipitation from the CMIP5 and CMIP6 archives.\nThis is joint work with Anna Kiriliouk, Paula Gonzalez Soulivanh Thao and Julien Worms.\n\n\nBiographie\nPhilippe Naveau est chercheur scientifique senior au Laboratoire des Sciences du Climat et de l’Environnement du CNRS à Gif-sur-Yvette (Saclay, France). Il travaille notamment sur la détection et l’attribution d’événements extrêmes. Phillipe a obtenu un doctorat en statistique de Colorado State University en 1998 sous la direction de R. Tweedie et R. Davis et a travaillé à University of Boulder et NCAR avant de rejoindre le CNRS."
  },
  {
    "objectID": "presentations/archives/2020-01-31.html",
    "href": "presentations/archives/2020-01-31.html",
    "title": "Longitudinal functional regression: tests of significance",
    "section": "",
    "text": "We consider longitudinal functional regression, where, for each subject, the response consists of multiple curves observed at different time visits. We discuss tests of significance in two general settings. First, when there are no additional covariates, we develop a hypothesis testing methodology for formally assessing that the mean function does not vary over time. Second, in the presence of other covariates, we propose a testing procedure to determine the significance of the covariate’s time-varying effect formally. The methods account for the complex dependence structure of the response and are computationally efficient. Numerical studies confirm that the testing approaches have the correct size and are have a superior power relative to available competitors. We illustrate the methods on a real data application."
  },
  {
    "objectID": "presentations/archives/2020-01-31.html#résumé",
    "href": "presentations/archives/2020-01-31.html#résumé",
    "title": "Longitudinal functional regression: tests of significance",
    "section": "",
    "text": "We consider longitudinal functional regression, where, for each subject, the response consists of multiple curves observed at different time visits. We discuss tests of significance in two general settings. First, when there are no additional covariates, we develop a hypothesis testing methodology for formally assessing that the mean function does not vary over time. Second, in the presence of other covariates, we propose a testing procedure to determine the significance of the covariate’s time-varying effect formally. The methods account for the complex dependence structure of the response and are computationally efficient. Numerical studies confirm that the testing approaches have the correct size and are have a superior power relative to available competitors. We illustrate the methods on a real data application."
  },
  {
    "objectID": "presentations/archives/2020-01-31.html#biographie-de-la-conférencière",
    "href": "presentations/archives/2020-01-31.html#biographie-de-la-conférencière",
    "title": "Longitudinal functional regression: tests of significance",
    "section": "Biographie de la conférencière:",
    "text": "Biographie de la conférencière:\nAna-Maria Staicu is Associate Professor in the Department of Statistics, North Carolina State University. She completed a PhD in 2007 from University of Toronto CA, under the supervision of Nancy Reid. Before joining NC State in 2009, she was a Brunel research fellow at the University of Bristol UK and also worked with Ciprian Crainiceanu and Ray Carroll. Her research interests are primarily in functional data analysis, longitudinal data analysis, nonparametric statistics, and brain imaging analysis. It has been applied to brain tractography studies of MS and brain imaging studies more general, wearable computing, animal studies, and environmental studies. She is a recipient of the NSF Career Award."
  },
  {
    "objectID": "presentations/brownbag/PHD_seminars_1.html",
    "href": "presentations/brownbag/PHD_seminars_1.html",
    "title": "On general semi-closed-form solutions for VIX derivative pricing",
    "section": "",
    "text": "Subject\nMost pricing methods for VIX futures and European VIX options rely on the existence of the squared VIX moment generating function. Yet this function does not exist for some state-of-the-art option pricing models, which prevents their widespread use. This paper presents semi-closed form solutions for VIX futures and European VIX option prices that rely on the characteristic functions of the squared VIX. These pricing formulas are applicable to a wide class of models—virtually all exponentially affine models in the literature, among others—as the characteristic function always exists. We also test our newly proposed pricing methodologies against usual benchmarks in the literature and report that they lead to more efficient and accurate prices. The preprint is available on SSRN."
  },
  {
    "objectID": "presentations/actuel/2023-12-20.html",
    "href": "presentations/actuel/2023-12-20.html",
    "title": "Equilibria in Centralized Insurance Markets: Monopolistic vs. Competitive Pricing",
    "section": "",
    "text": "Résumé\nThree fundamental concerns of any risk-sharing market are the willingness of agents to engage in an exchange, Pareto efficiency of allocations of the aggregate risk, and a characterization of equilibria in the market. In sequential-move insurance markets with monopolistic pricing, the notion of a Stackelberg Equilibrium (SE) has gained recent popularity as an equilibrium concept. In this talk, I will go over some of our recent work on the characterization of these equilibria, the examination of their relationship with Pareto-efficient (PE) allocations, as well as some extensions thereof. Specifically, while we show that SE lead to PE allocations, we also show that only those PE allocations that make the policyholder(s) indifferent between suffering the loss and entering into the market can be decentralized through a SE. We interpret the latter result as indicative of the limitations of SE as an equilibrium concept in this literature. We then extend this market structure by introducing strategic price competition on the supply side between several insurers. We argue that the notion of a Subgame Perfect Nash Equilibrium (SPNE) is the appropriate solution concept for analyzing equilibria in that market, and that it is an extension of SE to the case of multiple insurer. We characterize SPNE and show that they lead to PE allocations. Additionally, we show that under mild conditions, the policyholder realizes a strict welfare gain, which addresses the aforementioned concerns with SE and thereby ultimately reflects the benefit to the policyholder of competition on the supply side.\n\n\nBiographie\nMario Ghoussoub est professeur agrégé au départment de statistique et science actuarielle de l’Université de Waterloo, où il a obtenu son doctorat en 2011 avant de faire un stage postdoctoral à l’Université de Montréal. Après un passage à Imperial College, il est revenu à Waterloo. Ses intérêts de recherches portent principalement sur l’incertitude entourant les modèles et la théorie du choix sous incertitude et ambiguité, avec applications en assurance, mesures de risques et gestion, finance comportementale quantitative et la théorie du partage du risque."
  },
  {
    "objectID": "presentations/actuel/2024-02-23.html",
    "href": "presentations/actuel/2024-02-23.html",
    "title": "À déterminer",
    "section": "",
    "text": "Résumé\n\n\nBiographie"
  },
  {
    "objectID": "presentations/actuel/2023-11-10.html",
    "href": "presentations/actuel/2023-11-10.html",
    "title": "Learning to Simulate Tail-risk Scenarios",
    "section": "",
    "text": "Résumé\nThe estimation of loss distributions for dynamic portfolios requires the simulation of scenarios representing realistic joint dynamics of their components. Scalability to large or heterogeneous portfolios involving multiple asset classes is particularly challenging, as is the accurate representation of tail risk.\nWe propose a novel data-driven approach for the simulation of realistic multi-asset scenarios with a particular focus on the accurate estimation of tail risk for a given class of static and dynamic portfolios selected by the user. By exploiting the joint elicitability property of Value-at-Risk (VaR) and Expected Shortfall (ES), we design a Generative Adversarial Network (GAN) architecture capable of learning to simulate price scenarios that preserve tail risk features for these benchmark trading strategies, leading to consistent estimators for their VaR and ES.\nFrom a theoretical perspective, we show that different choices of score functions lead to different optimization landscapes and different complexities in GAN training. In addition, we prove that the generator in our GAN architecture enjoys a universal approximation property under the criteria of tail risk measures. In addition, we prove the bi-level optimization formulation between the generator and the discriminator is equivalent to a max-min game, leading to a more effective and practical formulation for training. From an empirical perspective, we demonstrate the accuracy and scalability of our method via extensive simulation experiments using synthetic and market data. Our results show that, in contrast to other data-driven scenario generators, our proposed scenario simulation method correctly captures tail risk for both static and dynamic portfolios in the input datasets.\nThis is based on joint work with Rama Cont, Mihai Cucuringu, and Chao Zhang (Oxford).\n\n\nBiographie\nDr. Renyuan Xu est professeure adjointe WiSE Gabilan au départment de génie industriel et des systèmes à l’Université de la Californie du Sud. Avant de se joindre à USC, elle a passé deux années à Oxford avec Rama Cont comme fellow de recherche. Renyuan a obtenu son doctorat de l’Université de la Californie à Berkeley en 2019. Ses intérêts de recherche sont en théorie du contrôle et des jeux stochastiques, l’apprentissage par renforcement et l’apprentissage automatique avec application en finance et dans les transactions à haute fréquence."
  },
  {
    "objectID": "presentations/actuel/2024-03-15.html",
    "href": "presentations/actuel/2024-03-15.html",
    "title": "À déterminer",
    "section": "",
    "text": "Résumé\n\n\nBiographie\nDr. Daniele Durante est professeur adjoint à l’Université Bocconi depuis 2017 et chercheur affilié à l’institut de sciences des données et d’analytique de Bocconi, du centre DONDENA de recherche en dynamique sociale et politique publique et du Laboratoire de recherche sur la crise du coronavirus. Daniele a obtenu son doctorat sous la direction de Bruno Scarpa, co-encadré par David Dunson, à l’Université de Padoue, où ila également complété sa maîtrise et un stage post-doctoral. Il est éditeur associé de Biometrika, du Journal of Computational and Graphical Statistics et du Journal of Multivariate Analysis. Durante travaille sur la recherche disciplinaire et la méthodologie Bayésienne; il est le récipiendaire de nombreux prix, dont le prix Laplace, le prix Mitchell, et le prix de la meilleure thèse de doctorat de la Società Italiana di Statistica."
  },
  {
    "objectID": "presentations/actuel/2023-11-24.html",
    "href": "presentations/actuel/2023-11-24.html",
    "title": "State-dependent Sampling in Observational Cohort Studies",
    "section": "",
    "text": "Résumé\nObservational cohort studies of chronic disease involve the recruitment and follow-up of a sample of individuals with the goal of learning about the course of the disease, the effect of fixed and time-varying risk factors. Analysis of this information is often facilitated by using multistate models with intensity functions governing transition between disease states. Chronic disease studies often involve conditions for recruitment, for example incident cohort involves individuals who are healthy at accrual, prevalent cohort samples individuals who have already developed the disease, and a length biased sampling includes individual who are alive at the time of recruitment. In this talk we discuss the impact of ignoring state-dependent sampling in life history analysis and the ways of addressing the issue using auxiliary information. A longitudinal study of aging and cognition among religious sisters is used to illustrate the related methodology.\n\n\nBiographie\nDr. Leilei Zeng joined the Department of Statistics and Actuarial Science at the University of Waterloo as an associate professor in 2011, as Graham Trust Chair in Health Statistics (2011-2016). Professor Zeng received her masters and PhD in Biostatistics from the University of Waterloo (1999-2005). She then worked as a postdoctoral fellow and then as an assistant professor (2006-2011) at Simon Fraser University (SFU).\nHer research interests lies in the development of statistical methodologies for public health and medical research. Specific research topics include methods for event history and longitudinal data analysis, multistate models, marginal models, incomplete observed data, design of clinical and epidemiological studies, and model misspecification and evaluation. Her current main research collaborations and applications are related to studies in Rheumatic diseases, dementia and Alzheimer disease, and stress and reproduction in women."
  },
  {
    "objectID": "brownbag.html",
    "href": "brownbag.html",
    "title": "Causeries",
    "section": "",
    "text": "Les causeries aux cycles supérieurs sont organisés par les étudiant(e)s du doctorat et de la maîtrise en finance et en ingénierie financière. Pour information, contactez l’organisateur Étienne Bacon.\n\n\n\n\n\n\nDate\n\n\nHeure\n\n\nInvité(e)\n\n\nTitre de l’exposé\n\n\nLocal\n\n\n\n\n\n\nven., 20 oct. 2023\n\n\n13h30\n\n\nEtienne Bacon\n\n\nOn general semi-closed-form solutions for VIX derivative pricing\n\n\nsalle Cogeco\n\n\n\n\nven., 20 oct. 2023\n\n\n13h00\n\n\nSebastien Legros\n\n\nQuality Issues of Implied Volatilities of Index and Stock Options in the OptionMetrics IvyDB Database\n\n\nsalle Cogeco\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "archives.html",
    "href": "archives.html",
    "title": "Séminaires passés",
    "section": "",
    "text": "Date\n\n\nInvité(e)\n\n\nTitre de l’exposé\n\n\n\n\n\n\n13 oct. 2023\n\n\nSerge Darolles\n\n\nManaging Hedge Fund Liquidity Risks\n\n\n\n\n11 oct. 2023\n\n\nBahar Yetis Kara\n\n\nDisaster Resilient Cities: An OR Approach to Disaster Management\n\n\n\n\n5 oct. 2023\n\n\nAfzal S. Siddiqui\n\n\nStrategic Sector Coupling? Market Power in Heat and Power Markets\n\n\n\n\n29 sept. 2023\n\n\nRyan Campbell\n\n\nStatistical inference for multivariate extremes via a geometric approach\n\n\n\n\n12 sept. 2023\n\n\nLambert De Monte\n\n\nMultivariate extremes – A geometric Bayesian inference approach\n\n\n\n\n19 mai 2023\n\n\nMerve Bodur\n\n\nMarkov Chain-based Policies for Multi-stage Stochastic Integer Linear Programming\n\n\n\n\n18 mai 2023\n\n\nTamer Başar\n\n\nConsensus and Dissensus in Multi-population Multi-agent Systems\n\n\n\n\n16 mai 2023\n\n\nJocelyn Martel\n\n\nInstitutional Ownership and the Resolution of Financial Distress\n\n\n\n\n24 avr. 2023\n\n\nBenjamin Shaby\n\n\nSpatial scale-aware tail dependence modeling for high-dimensional spatial extremes\n\n\n\n\n14 avr. 2023\n\n\nJessica Gronsbell\n\n\nEfficient evaluation of prediction rules in semi-supervised settings under stratified sampling\n\n\n\n\n17 mars 2023\n\n\nMohamad Elmasri\n\n\nPredictive inference for travel time on transportation networks\n\n\n\n\n24 févr. 2023\n\n\nGah-Yi Ban\n\n\nPersonalized Dynamic Pricing with Machine Learning: High Dimensional Features and Heterogeneous Elasticity\n\n\n\n\n17 févr. 2023\n\n\nSébastien Laurent\n\n\nAutoregressive conditional betas\n\n\n\n\n8 févr. 2023\n\n\nKris Boudt\n\n\nCourt Shopping, Pro-Debtor Bias, and Bankruptcy Outcomes\n\n\n\n\n3 févr. 2023\n\n\nKatya Scheinberg\n\n\nOverview of adaptive stochastic optimization methods.\n\n\n\n\n23 janv. 2023\n\n\nPhilippe Naveau\n\n\nRecords Analysis in Climate Attribution\n\n\n\n\n20 janv. 2023\n\n\nRosnel Sessinou\n\n\nPrecision Least Squares: Estimation and Inference in High-Dimensional Linear Regression Models\n\n\n\n\n6 déc. 2022\n\n\nCourtney Paquette\n\n\nStochastic Algorithms in the Large\n\n\n\n\n25 nov. 2022\n\n\nRebecca Steorts\n\n\n(Almost) All of Entity Resolution\n\n\n\n\n11 nov. 2022\n\n\nRoxana Dumitrescu\n\n\nOptimal stopping mean-field games: a linear programming formulation and applications to entry-exit games in electricity markets\n\n\n\n\n2 nov. 2022\n\n\nMarie Lambert\n\n\nEmployee Views of Leveraged Buy-Out Transactions\n\n\n\n\n28 oct. 2022\n\n\nDerek Bingham\n\n\nComputer Model Emulation Using Deep Gaussian Processes\n\n\n\n\n14 oct. 2022\n\n\nFrédéric Vrins\n\n\nOptimal and robust combination of forecasts via constrained optimization and shrinkage\n\n\n\n\n10 juin 2022\n\n\nLeopoldo Catania\n\n\nThe Leverage Effect and Propagation\n\n\n\n\n10 mai 2022\n\n\nTias Guns\n\n\nLearning and reasoning with constraint solving\n\n\n\n\n10 mai 2022\n\n\nEmanuele Guidotti\n\n\nEfficient Estimation of Bid-Ask Spreads from Open, High, Low, and Close Prices\n\n\n\n\n28 avr. 2022\n\n\nMargarida Carvalho\n\n\nToward social welfare and fairness in kidney exchange programs\n\n\n\n\n23 mars 2022\n\n\nNathaniel Stevens\n\n\nComparative Probability Metrics: Using Posterior Probabilities to Account for Practical Equivalence in A/B tests\n\n\n\n\n25 févr. 2022\n\n\nMarco Avella Medina\n\n\nDifferentially private inference via noisy optimization\n\n\n\n\n5 nov. 2021\n\n\nJustin Johnson Kakeu\n\n\nSize Distribution of Firms and Strategic Investments in Large Markets: A Stochastic Mean Field Game Approach\n\n\n\n\n29 oct. 2021\n\n\nTiffany Timbers\n\n\nOpinionated practices for teaching reproducibility: motivation, guided instruction and practice\n\n\n\n\n4 oct. 2021\n\n\nLucy Gao\n\n\nSelective inference on trees\n\n\n\n\n18 mai 2021\n\n\nBethany White, Jastaranpreet Singh\n\n\nRethinking Introductory Statistics for Life Sciences Programs\n\n\n\n\n6 mai 2021\n\n\nSanjiv R. Das, Daniel N. Ostrov\n\n\nDynamic Optimization for Multi-Goals-Based Wealth Management\n\n\n\n\n7 avr. 2021\n\n\nRégis Chevanaz\n\n\nWhen does eco-efficiency rebound or backfire? An analytical model\n\n\n\n\n17 mars 2021\n\n\nStephen P. Boyd\n\n\nConvex Optimization\n\n\n\n\n19 févr. 2021\n\n\nEmily Hector\n\n\nJoint integrative analysis of multiple data sources with correlated vector outcomes\n\n\n\n\n22 janv. 2021\n\n\nEmanuele Guidotti\n\n\nAsymptotic Expansion Formulas for Diffusion Processes Based on the Perturbation Method\n\n\n\n\n4 déc. 2020\n\n\nBruno Feunou\n\n\nRisk-neutral moments based estimation of continuous time jump-diffusion models\n\n\n\n\n27 nov. 2020\n\n\nIlze Kalnina\n\n\nHigh-Frequency Factor Models and Regressions\n\n\n\n\n5 nov. 2020\n\n\nJoseph DeCarolis\n\n\nDeveloping an Open Energy Outlook for the United States Using Tools for Energy Model Optimization and Analysis (Temoa)\n\n\n\n\n23 oct. 2020\n\n\nMadhu Kalimipalli\n\n\nBanking networks, systemic risk, and the credit cycle in emerging markets\n\n\n\n\n16 oct. 2020\n\n\nOzlem Ergun\n\n\nOptimizing Post-Disruption Response Operations to Improve Resilience of Critical Infrastructure Systems\n\n\n\n\n9 oct. 2020\n\n\nNingyuan Chen\n\n\nThe Use of Binary Choice Forests to Model and Estimate Discrete Choices\n\n\n\n\n11 sept. 2020\n\n\nStefan Wager\n\n\nMachine Learning for Causal Inference\n\n\n\n\n5 mars 2020\n\n\nLinda Mhalla\n\n\nTail risk and style dependence in the fund industry: A multivariate extreme value approach\n\n\n\n\n31 janv. 2020\n\n\nAna-Maria Staicu\n\n\nLongitudinal functional regression: tests of significance\n\n\n\n\n9 janv. 2020\n\n\nMu Zhu\n\n\nSome statistical applications of generative neural networks\n\n\n\n\n29 nov. 2019\n\n\nAhmed Abdulla\n\n\nLife at the (grid edge): Evaluating energy decentralization strategies in California\n\n\n\n\n16 oct. 2019\n\n\nKeven Bluteau\n\n\nMedia and the stock market: Their relationship and abnormal dynamics around earnings announcements\n\n\n\n\n11 oct. 2019\n\n\nKris Boudt\n\n\nOptimizing the design of textual indices: Impression management and Sentometrics\n\n\n\n\n10 sept. 2019\n\n\nMaximilian Schiffer\n\n\nOptimal picking policies for e-commerce warehouses\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "presentations/actuel/2024-02-16.html",
    "href": "presentations/actuel/2024-02-16.html",
    "title": "À déterminer",
    "section": "",
    "text": "Résumé\n\n\nBiographie\nDr. Jim Luedtke est professeur titulaire au département d’ingénierie industrielle et système de l’Université de Wisconsin-Madison. Il a obtenu son doctorat de Georgia Institute of Technology en 2007. Spécialiste de l’optimisation nonlinéaire, stochastique et linéaire avec entiers mixtes, il est actif au sein de la société INFORMS d’optimisation et rédacteur adjoint de Mathematical Programming Computation."
  },
  {
    "objectID": "presentations/actuel/2024-02-02.html",
    "href": "presentations/actuel/2024-02-02.html",
    "title": "À déterminer",
    "section": "",
    "text": "Résumé\n\n\nBiographie"
  },
  {
    "objectID": "presentations/actuel/2024-01-26.html",
    "href": "presentations/actuel/2024-01-26.html",
    "title": "À déterminer",
    "section": "",
    "text": "Résumé\n\n\nBiographie\nDr. Trevor Campbell est professeur agrégé au département de statistique de l’Université de la Colombie-Britannique à Vancouver. Ses recherches portent sur l’inférence Bayésienne, notamment le développement d’algorithmes flexible et automatisés, les données en lignes, la théorie bayésienne et les méthodes bayésiennes nonparamétriques. Trevor a complété son doctorat au Laboratoire d’information et de systèmes de décision au MIT sous la tutelle de Jonathan How et un postdoctorat avec Tamara Broderick."
  },
  {
    "objectID": "presentations/actuel/2024-04-05.html",
    "href": "presentations/actuel/2024-04-05.html",
    "title": "À déterminer",
    "section": "",
    "text": "Résumé\n\n\nBiographie\nProspective students\nNicholas Rivers is Associate Professor at the Graduate School of Public and International Affairs and the Institute of the Environment at the University of Ottawa. His research focuses on the economic evaluation of environmental policies, using econometric and computational methods. He has received awards and grants for his research from the Trudeau Foundation, the Social Science and Humanities Research Council, and the National Science and Engineering Research Council. He serves as an expert panel member for the Canadian Climate Institute and as the Director for graduate programs at the Institute of the Environment. From 2011-2021, he was Canada Research Chair in Climate and Energy Policy. From 2016-2022 he was a co-editor of the Journal of Environmental Economics and Management."
  },
  {
    "objectID": "presentations/actuel/2024-03-20.html",
    "href": "presentations/actuel/2024-03-20.html",
    "title": "À déterminer",
    "section": "",
    "text": "Résumé\n\n\nBiographie"
  },
  {
    "objectID": "presentations/brownbag/PHD_seminars_2.html",
    "href": "presentations/brownbag/PHD_seminars_2.html",
    "title": "Quality Issues of Implied Volatilities of Index and Stock Options in the OptionMetrics IvyDB Database",
    "section": "",
    "text": "Subject\nSebastien will be presenting the paper Quality Issues of Implied Volatilities of Index and Stock Options in the OptionMetrics IvyDB Database by Martin Wallmeier. This paper discusses problems related to the OptionMetrics database and how to solve them."
  },
  {
    "objectID": "presentations/archives/2021-10-29.html",
    "href": "presentations/archives/2021-10-29.html",
    "title": "Opinionated practices for teaching reproducibility: motivation, guided instruction and practice",
    "section": "",
    "text": "Résumé\nIn the data science courses at the University of British Columbia, we define data science as the study, development and practice of reproducible and auditable processes to obtain insight from data. While reproducibility is core to our definition, most data science learners enter the field with other aspects of data science in mind, for example predictive modelling, which is often one of the most interesting topic to novices. This fact, along with the highly technical nature of the industry standard reproducibility tools currently employed in data science, present out-ofthe gate challenges in teaching reproducibility in the data science classroom. Put simply, students are not as intrinsically motivated to learn this topic, and it is not an easy one for them to learn. What can a data science educator do? Over several iterations of teaching courses focused on reproducible data science tools and workflows, we have found that providing extra motivation, guided instruction and lots of practice are key to effectively teaching this challenging, yet important subject. Here we present examples of how we deeply motivate, effectively guide and provide ample practice opportunities to data science students to effectively engage them in learning about this topic.\n\n\nBiographie\nTiffany Timbers est professeure adjointe volet enseignement au département de statistique de l’Université de la Colombie-Britannique (UBC) à Vancouver et co-directrice du programme de maîtrise en science des données (Vancouver) de UBC. Dans le cadre de son travail, elle enseigne et supervise le développement du cursus pour une application responsable de la science des données appliquée à des problèmes pratiques. Parmi ses cours favoris figure un cours des cycles supérieurs sur le développement de logiciels collaboratifs, qui traite de création de paquets R et Python à l’aide des outils et d’un flux de travail moderne."
  },
  {
    "objectID": "presentations/archives/2022-10-28.html",
    "href": "presentations/archives/2022-10-28.html",
    "title": "Computer Model Emulation Using Deep Gaussian Processes",
    "section": "",
    "text": "Résumé\nComputer models are often used to explore physical systems. Increasingly, there are cases where the model is fast, the code is not readily accessible to scientists, but a large suite of model evaluations is available. In these cases, an \"emulator\" is used to stand in for the computer model. This work was motivated by a simulator for the chirp mass of binary black hole mergers where no output is observed for large portions of the input space and more than \\(10^6\\) simulator evaluations are available. This poses two problems: (i) the need to address the discontinuity when observing no chirp mass; and (ii) performing statistical inference with a large number of simulator evaluations. The traditional approach for emulation is to use a stationary Gaussian process (GP) because it provides a foundation for uncertainty quantification for deterministic systems. We explore the impact of the choices when setting up the deep GP on posterior inference, apply the proposed approach to the real application and propose a sequential design approach for identifying new simulations.\n\n\nBiographie\nDerek Bingham est professeur titulaire et directeur du département de statistique et de sciences actuarielles de l’Université Simon Fraser à Vancouver, où il a complété en 1999 un doctorat en statistique sous la gouverne de Randy Sitter. Dr Bingham est un spécialiste des devis expérimentaux, et travaille sur la calibration, l’émulation et la quantification de l’incertitude pour des modèles Bayésiens. Sa recherche se concentre actuellement sur le développement de modèles statistiques permettant de combiner des observations de modèles physiques avec des simulations par ordinateur à grande échelle. Derek a été professeur au département de statistique de l’Université de Michigan et titulaire d’une chaire de recherche du Canada niveau 2 en statistique industrielle de 2003 à 2013. Il a été le récipiendaire du prix CRM-SSC en statistique en 2013."
  },
  {
    "objectID": "presentations/archives/2022-11-02.html",
    "href": "presentations/archives/2022-11-02.html",
    "title": "Employee Views of Leveraged Buy-Out Transactions",
    "section": "",
    "text": "Résumé\nThis paper offers a comprehensive view of employee satisfaction: we make use of 700,000 ratings, in addition to 500,000 written reviews posted by employees of all ranks, in different industries, types of companies, and in companies that underwent different types of ownership changes. Employee satisfaction is lower following a company acquisition; more so when it is a Leveraged Buy-Out (LBO). However, there is significant heterogeneity. Our first key finding is that previous ownership type is the main source of heterogeneity for employee satisfaction. A large decrease is observed for Public-to-Private transactions only. The employee drop in satisfaction in these types of transactions is four times larger than in a traditional M&A. We do not find however any difference between Private-to-Private transactions and regular M&As. We conducted a Latent Dirichlet Allocation analysis on about 500,000 written cons reviews and show that for Public-to-Private transactions, employees complain specifically about layoffs, restructuring and cost cutting as well as about the management team. However, these employees also complain less about the operations or their benefits which were a problem before the transaction.\n\n\nBiographie\nMarie Lambert has a joint Ph.D. in Finance from the Universities of Liège and Luxembourg (2010). She is Full Professor and Vice-Dean for Research at HEC Liège - Management School of the University of Liège. She leads the track “Banking and Asset Management” of the master in management and teaches courses on Asset Management, Alternative Investments, Corporate Finance and Financial Modeling. Marie is also Affiliate Professor at EDHEC Business School (Nice) and at Paris Dauphine as well as a Research Associate at the EDHEC Risk Institute and a research fellow of the Quantitative Management Initiative. She is an Associate Editor of Global Finance Journal, Credit and Capital Markets, and Applied Finance Letters. She sits at the management board of the BENELUX Corporate Finance Network. Marie has developed a research expertise in asset pricing models, market anomalies, investment styles (value, growth investing), and hedge funds. Her work has been published in international peer-reviewed journals and has been presented to leading academic and professional conferences. Her current research interests lie in sustainable finance (e.g., climate risk pricing, disagreement in the measurement of firm sustainable ratings, performance analysis of sustainable investments) and private equity."
  },
  {
    "objectID": "presentations/archives/2023-01-20.html",
    "href": "presentations/archives/2023-01-20.html",
    "title": "Precision Least Squares: Estimation and Inference in High-Dimensional Linear Regression Models",
    "section": "",
    "text": "Résumé\nAs the least squares estimator can be cast to depend only on the precision matrix, we show that a consistent estimator of the latter can be directly used to obtain an expression of the former, even in high-dimensional regression problems where the number of covariates can be larger than the sample size. Since bias can still occur when using consistent regularized precision matrix estimators, we show how to construct a nearly unbiased least squares estimator irrespective of the sparsity within the data generating process. We call this the precision least squares estimator and show that it is asymptotically Gaussian and delivers uniformly valid inference. We employ precision least squares to estimate the predictive connectedness among the daily asset returns of 88 global banks. We find evidence that such connections drastically decrease during crisis periods. Network density (modularity) is then proposed as an empirical measure of crisis proximity.\n\n\nBiographie\nRosnel Sessinou est actuellement stagiaire postdoctoral à HEC Montréal. Il est détenteur d’un doctorat en sciences économiques de l’Université Aix-Marseille. Ses intérêts de recherche portent sur l’économétrie, la finance, l’apprentissage statistique et la statistique en haute-dimension."
  },
  {
    "objectID": "presentations/archives/2022-05-10b.html",
    "href": "presentations/archives/2022-05-10b.html",
    "title": "Efficient Estimation of Bid-Ask Spreads from Open, High, Low, and Close Prices",
    "section": "",
    "text": "Résumé\nThis paper formally derives an efficient estimator of the bid-ask spread from open, high, low, and close prices. The estimator is asymptotically unbiased and optimally combines the full set of price data to minimize the estimation variance. In absence of quote data, it delivers the most accurate estimates of bid-ask spreads theoretically, numerically, and empirically. The estimator is easy to calculate and has a broad applicability in empirical finance.\n\n\nBiographie\nEmanuele Guidotti est doctorant en finance à l’Université de Neuchâtel spécialisé dans la tarification empirique des actifs. Il est également chercheur associé au CREST, Japan Science and Technology Agency depuis 2017 au sein du projet YUIMA, une équipe internationale de recherche qui oeuvre à développer un environnement complet pour l’estimation et la simulation d’équations différentielles stochastiques. Il est également partenaire au sein de Algo Finance Sagl, une jeune pousse spécialisé dans la création de logiciels et d’algorithmes en finance pour le secteur de la gestion d’actifs. Emanuele enseigne à titre de professeur associé à l’Université de Milan au sein du programme de maîtrise en science des données et économie. Emanuele détient une licence en physique et une maîtrise en économie et en finance quantitive, toutes deux décernées par l’Université de Milan cum laude."
  },
  {
    "objectID": "presentations/archives/2021-01-22.html#résumé",
    "href": "presentations/archives/2021-01-22.html#résumé",
    "title": "Asymptotic Expansion Formulas for Diffusion Processes Based on the Perturbation Method",
    "section": "Résumé:",
    "text": "Résumé:\nDiffusion processes are a class of models that plays a prominent role in describing the time-continuous evolution of phenomena in the natural and social sciences. However, only in very few cases the stochastic differential equation driving the process can be analytically solved. Based on the perturbation method, we present asymptotic expansion formulas to generate accurate approximations to the solution of arbitrary diffusions. In particular, we expand the characteristic function of the process. Then, the approximated expectation and moments are computed by differentiation, and the approximated transition density is written in terms of Hermite polynomials by applying Fourier transform. The computational efficiency, accuracy, and flexibility of the method are assessed via experiments conducted against closed-form solutions and Monte Carlo simulations in tasks involving density approximation, expectations, moments, filtering, and functionals of generic diffusion processes."
  },
  {
    "objectID": "presentations/archives/2021-01-22.html#biographie-du-conférencier",
    "href": "presentations/archives/2021-01-22.html#biographie-du-conférencier",
    "title": "Asymptotic Expansion Formulas for Diffusion Processes Based on the Perturbation Method",
    "section": "Biographie du conférencier:",
    "text": "Biographie du conférencier:\nEmanuele Guidotti est doctorant en finance à l’Université de Neuchâtel spécialisé dans la tarification empirique des actifs. Il est également chercheur associé au CREST, Japan Science and Technology Agency depuis 2017 au sein du projet YUIMA, une équipe internationale de recherche qui oeuvre à développer un environnement complet pour l’estimation et la simulation d’équations différentielles stochastiques. Il est également partenaire au sein de Algo Finance Sagl, une jeune pousse spécialisé dans la création de logiciels et d’algorithmes en finance pour le secteur de la gestion d’actifs. Emanuele enseigne à titre de professeur associé à l’Université de Milan au sein du programme de maîtrise en science des données et économie. Emanuele détient une licence en physique et une maîtrise en économie et en finance quantitive, toutes deux décernées par l’Université de Milan cum laude."
  },
  {
    "objectID": "presentations/archives/2023-09-12.html",
    "href": "presentations/archives/2023-09-12.html",
    "title": "Multivariate extremes – A geometric Bayesian inference approach",
    "section": "",
    "text": "Résumé\nMultivariate extreme value theory (MEVT) is a branch of probability and statistics concerned with the characterisation of the extremes of finite-dimensional random vectors and the estimation of the probability of joint, rare events. Of particular interest is the task of extrapolating beyond the range of observed data; common environmental applications include modelling extreme hydrological events linked with flooding, harmful and damaging wind gusts as well as heatwaves intensity or duration and their impacts on livelihood.\nThe lack of natural ordering of vectors and the schism between the classes of asymptotically dependent and asymptotically independent random vectors gave rise to various theoretical and modelling frameworks. The geometric approach to MEVT arises through the study of suitably scaled sample clouds―or suitably scaled independent observations from random vectors―and their convergence in probability onto compact and star-shaped limit sets. The estimation of the shape of these limit sets and their associated gauge functions respectively allows for the estimation of well-known coefficients of extremal dependence and of the probability of rare or extreme events. Using a radial-angular decomposition of the random vector of interest, we consider the distribution of radial exceedances of high quantiles of the distribution of radii given angles. Using a limiting Poisson point process likelihood, we present a method using information both from the distribution of the radial exceedances and from the distribution of the angles along which the exceedances occur. We adopt a Bayesian approach in which special emphasis is placed on Hilbert space projections of Gaussian Markov random fields on spheres as flexible non-parametric models for the prior distribution of the gauge function and for the prior distribution of angles. We showcase our method with a simulation study and two case studies of river flow and sea level extremes. Joint work with Ioannis Papastathopoulos, Ryan Campbell, Håvard Rue.\n\n\nBiographie\nLambert De Monte est un doctorant en statistique à l’Université d’Édimbourg spécialisé dans l’analyse de valeurs extrêmes et les méthodes probabilistes de prévisions. Il est encadré par Ioannis Papastathopoulos. Originaire de Montréal, il est détenteur d’une maîtrise en mathématiques de l’Université McGill et d’un B.Sc. en mathématiques et informatique."
  },
  {
    "objectID": "presentations/archives/2022-06-10.html",
    "href": "presentations/archives/2022-06-10.html",
    "title": "The Leverage Effect and Propagation",
    "section": "",
    "text": "Résumé\nThis paper proposes a new way to measure the leverage effect and its propagation over time. We also show that, with respect to the newly proposed measure, common volatility models like the GJRGARCH, the Exponential GARCH, and the asymmetric SV can be inaccurate to correctly represent the leverage effect and its propagation for financial time series. We propose to modify the variance recursion of common volatility models by including an auxiliary leverage process which allows for a proper representation of the leverage effect and its propagation over time. Empirical results indicate that the inclusion of the auxiliary leverage process improves both in sample and out of sample.\n\n\nBiographie\nLeopoldo Catania est professeur agrégé au sein de l’école de gestion et de sciences sociales de l’Université d’Aarhus au Danemark. Ses intérêts de recherche gravitent autour de l’économétrie financière et de l’analyse des séries chronologiques, notamment pour le développement et l’estimation de modèles pour la gestion quantitative du risque unidimensionnel et multidimensionnel, l’estimation de la densité prédictive de niveaux de retours, la dépendance temporelle et la volatilité. Il est titulaire d’une maîtrise en finance de l’Université Sapienza de Rome et d’un doctorat en économie et finance de l’Université de Rome II (Tor Vergata)."
  },
  {
    "objectID": "presentations/archives/2023-02-03.html",
    "href": "presentations/archives/2023-02-03.html",
    "title": "Overview of adaptive stochastic optimization methods.",
    "section": "",
    "text": "Résumé\nRecently a variety of stochastic variants of adaptive methods have been developed and analyzed. These include stochastic step search, trust region and cubicly regularized Newton methods. Such methods adapt the step size parameter and use it to dictate the accuracy required or stochastic approximations. The requirements on stochastic approximations are, thus, also adaptive and in principle can be biased and even inconsistent. The step size parameters in these method can increase and decrease based on the perceived progress, but unlike the deterministic case they are not bounded away from zero. This creates obstacles in complexity analysis of such methods. We will show how by viewing such algorithms as stochastic processes with martingale behavior we can derive bounds on expected complexity that also apply in high probability. We also show that it is possible to derive a lower bound on step size parameters in high probability for the methods in this general framework. We will discuss various stochastic settings, where the framework easily applies, such as expectation minimization, black box and simulation optimization, expectation minimization with corrupt samples, etc.\n\n\nBiographie\nDr. Katya Scheinberg is a Professor and Director of Graduate Studies at the School of Operations Research and Information Engineering at Cornell University. Prior to joining Cornell she was the Harvey E. Wagner Endowed Chair Professor at the Industrial and Systems Engineering Department at Lehigh University. She attended Moscow University for her undergraduate studies and received her PhD degree from Columbia University. She worked at the IBM T.J. Watson Research Center as a research staff member for over a decade before joining Lehigh in 2010. Katya’s main research areas are related to developing practical algorithms (and their theoretical analysis) for various problems in continuous optimization, such as convex optimization, derivative free optimization, machine learning, quadratic programming, etc. She is an Informs Fellow, a recipient of the Lagrange Prize from SIAM and MOS, the Farkas Prize from Informs Optimization Society and the Outstanding Simulation Publication award from Informs Simulation Society. Katya is currently the editor-in-chief of Mathematics of Operations Research, and co-editor of Mathematical Programming. She served as the Chair of SIAM Activity Group on Optimization from 2020 until 2022."
  },
  {
    "objectID": "presentations/archives/2023-02-17.html",
    "href": "presentations/archives/2023-02-17.html",
    "title": "Autoregressive conditional betas",
    "section": "",
    "text": "Résumé\nThis paper introduces an autoregressive conditional beta (ACB) model that allows regressions with dynamic betas (or slope coefficients) and residuals with GARCH conditional volatility. The model fits in the quasi score-driven approach recently pro- posed in the literature, and it is semi-parametric in the sense that the distributions of the innovations are not necessarily specified. The time-varying betas are allowed to depend on past shocks and exogenous variables. We establish the existence of a stationary solution for the ACB model, the invertibility of the score-driven filter for the time-varying betas, and the asymptotic properties of one-step and multistep QMLEs for the new ACB model. The finite sample properties of these estimators are studied by means of an extensive Monte Carlo study. Finally, we also propose a strategy to test for the constancy of the conditional betas. In a financial application, we find evidence for time-varying conditional betas and highlight the empirical relevance of the ACB model in a portfolio and risk management empirical exercise. This is joint work with Francisco Blasques and Christian Francq.\n\n\nBiographie\nDr. Sébastien Laurent est professeur titulaire en économétrie à l’école d’économie d’Aix-Marseille (AMSE) et membre de l’Institut d’administration des entreprises (IAE). Son expertise de recherche est en économétrie financière et computationnelle. Il est détenteur d’une maîtrise et d’un doctorat en économétrie financière de l’Université Maastricht. Il est l’auteur de 42 publications dans des revues avec comités de relecture et co-éditeur du Handbook of Volatility Models and Their Applications paru en 2012."
  },
  {
    "objectID": "presentations/archives/2022-12-06.html",
    "href": "presentations/archives/2022-12-06.html",
    "title": "Stochastic Algorithms in the Large",
    "section": "",
    "text": "Résumé\nIn this talk, I will present a framework, inspired by random matrix theory, for analyzing the dynamics of stochastic optimization algorithms (e.g., stochastic gradient descent (SGD) and momentum (SGD + M)) when both the number of samples and dimensions are large. Using this new framework, we show that the dynamics of optimization algorithms on a least squares problem with random data become deterministic in the large sample and dimensional limit. In particular, the limiting dynamics for stochastic algorithms are governed by a Volterra equation. From this model, we identify a stability measurement, the implicit conditioning ratio (ICR), which regulates the ability of SGD+M to accelerate the algorithm. When the batch size exceeds this ICR, SGD+M converges linearly at a rate of \\(O(1/\\sqrt{\\kappa})\\), matching optimal full-batch momentum (in particular performing as well as a full-batch but with a fraction of the size). For batch sizes smaller than the ICR, in contrast, SGD+M has rates that scale like a multiple of the single batch SGD rate. We give explicit choices for the learning rate and momentum parameter in terms of the Hessian spectra that achieve this performance. Finally we show this model matches performances on real data sets.\n\n\nBiographie\nCourtney Paquette est professeur adjointe à l’université McGill et détentrice d’une chaire en IA Canada. La recherche de Dr. Paquette est grossièrement centrée sur le devis et l’analyse d’algorithmes pour les problèmes d’optimisation à grande échelle, motivés par des applications en science des données. Courtney Paquette a obtenu un doctorat en mathématiques de l’Université de Washington en 2017, avant d’entreprendre un stage postdoctoral à l’Université Lehigh et l’Université de Waterloo, cette fois en qualité de chercheuse NSF. Elle a également occupé un poste de scientifique chez Google Research, Brain Montreal (2019-2020). Sa recherche est financée par le biais de sa chaire CIFAR, par le MILA, le CRSNG et le FRQNT."
  },
  {
    "objectID": "presentations/archives/2023-02-08.html",
    "href": "presentations/archives/2023-02-08.html",
    "title": "Court Shopping, Pro-Debtor Bias, and Bankruptcy Outcomes",
    "section": "",
    "text": "Résumé\nThe Bankruptcy Venue Reform Act of 2021 requires large corporations to file for bankruptcy in their principle place of business or where significant assets are located. We study the impact of this bill on the firm’s chances to emerge successfully from bankruptcy, and on the leniency of court judges in grating motions that affect the debtor’s access to cash, access to credit and control of the reorganization plan. We use these three dimensions to quantify the effective pro-debtor bias of Chapter 11 bankruptcy judges based on their decisions within the first 120 days of a voluntary Chapter 11 reorganization of large, publicly held companies between 2004 and 2020. We find a positive association between court shopping and pro-debtor bias, but no effect on firm survival. Our results suggest that the refiling rate can be improved by providing more opportunities to creditors to be involved in the reorganization plan.\n\n\nBiographie\nKris Boudt is professor of finance and econometrics at Ghent University, Vrije Universiteit Brussel and Vrije Universiteit Amsterdam. He is part of the core team at Sentometrics. He teaches the online courses “Introduction to portfolio analysis in R” and “GARCH models in R” at Datacamp. He is also affiliated with the KU Leuven and an invited lecturer at the University of Illinois in Chicago, Renmin University, SWUFE and the University of Aix-Marseille. Kris Boudt obtained his PhD in 2008 for his developments in the modelling and estimation of financial risk under a non-normal distribution. He has published his research in the Journal of Econometrics, Journal of Portfolio Management, Journal of Statistical Software, and the Review of Finance, among others. Kris Boudt received several awards for outstanding research and refereeing and is an active contributor to the open source community."
  },
  {
    "objectID": "presentations/archives/2020-10-16.html",
    "href": "presentations/archives/2020-10-16.html",
    "title": "Optimizing Post-Disruption Response Operations to Improve Resilience of Critical Infrastructure Systems",
    "section": "",
    "text": "Résumé\nCritical infrastructure systems (CIS) underpin almost every aspect of the modern society by enabling the essential functions through overlaying service networks. After a disruption impacting the CIS, the functionality of the overlaying service networks degrades. Thus, after an extreme event, in order to minimize the negative impact to society, it is crucial to restore the disrupted CIS as soon as possible. In this talk, we focus on disruptions created by natural hazards on transportation CIS and develop methods to efficiently plan the post-disaster response operations. In the aftermath of a natural disaster, the transportation network is disrupted due to the debris blocking the roads and obstructing the flow of relief aid and search-and-rescue teams between critical facilities and disaster sites. In the first few days following a disaster, in order to deliver aid to those in need, blocked roads must be cleared by pushing the debris to the sides. In this context, we define the road network recovery problem (RNRP) as finding a schedule to clear the roads with limited resources such that all the service demanding locations are served in the shortest possible time. First, we address the deterministic RNRP and propose a novel network science inspired measure to quantify the criticality of the components within a disrupted service network and develop a restoration heuristic. Next, we consider RNRP with stochastic demand and propose an approximate dynamic programming approach for identifying an effective policy under uncertainty.\n\n\nBiographie\nDr. Özlem Ergun is a professor in Mechanical and Industrial Engineering at Northeastern University. Dr. Ergun’s research focuses on design and management of large-scale and decentralized networks. She has applied her work on network design, management, and resilience to problems arising in many critical systems including transportation, pharmaceuticals, and healthcare. She has worked with organizations that respond to emergencies and humanitarian crises around the world, including USAID, UNWFP, UNHCR, IFRC, OXFAM America, CARE USA, FEMA, USACE, CDC, AFCEMA, and MedShare International. Dr. Ergun is currently serving as a member of the National Academies Committee on Building Adaptable and Resilient Supply Chains after Hurricanes Harvey, Irma, and Maria. Within INFORMS, Dr. Ergun has been a leader in establishing a strong community of OR/MS professionals with an interest in public programs. She was the President of INFORMS Section on Public Programs, Service and Needs in 2013. She currently serves as the Area Editor at the Operations Research journal for Policy Modeling and the Public Sector Area. Dr. Ergun is also a founding co-chair of the annual Health and Humanitarian Logistics Conference, held annually since 2009. In addition, Dr. Ergun was the Vice President of Membership and Professional Recognition on the INFORMS Board of Directors, 2011 - 2015. Prior to joining Northeastern she was the Coca-Cola Associate Professor in the School of Industrial and Systems Engineering at Georgia Institute of Technology, where she also co-founded and co-directed the Health and Humanitarian Systems Research Center at the Supply Chain and Logistics Institute. She received a B.S. in Operations Research and Industrial Engineering from Cornell University in 1996 and a Ph.D. in Operations Research from the Massachusetts Institute of Technology in 2001."
  },
  {
    "objectID": "presentations/archives/2022-05-10.html",
    "href": "presentations/archives/2022-05-10.html",
    "title": "Learning and reasoning with constraint solving",
    "section": "",
    "text": "Résumé\nIndustry and society are increasingly automating processes, which requires solving constrained optimisation problems. This includes vehicle routing, demand-response planning, rostering and more. To find not just optimal solutions, but also ‘desirable’ solution by the end user, it is increasingly important to offer tools that automatically learn from the user and the environment and that support the constraint modelling in interpretable ways.\nIn this talk I will provide an overview of three different ways in which part of the problem specification can be learned from data. This includes learning from the user (preference learning in VRP), learning from the environment (end-to-end decision focussed learning) and explanation generation, that sit at the intersection of learning and reasoning.\nAs part of this work, we are building a modern constraint programming language called CPMpy(http://cpmpy.readthedocs.io) that eases integration of multiple constraint solving paradigms with machine learning and other scientific python libraries. I will shortly highlight its possibilities beyond the above cases, as well as our larger vision of conversational human-aware technology for optimisation.\n\n\nBiographie\nTias Guns is Associate Professor at the DTAI lab of KU Leuven, in Belgium. His research is at the intersection of machine learning and combinatorial optimisation.\nTias’ expertise is in the hybridisation of machine learning systems with constraint solving systems, more specifically building constraint solving systems that reason both on explicit knowledge as well as knowledge learned from data. For example learning the preferences of planners in vehicle routing, and solving new routing problems taking both operational constraints and learned human preferences into account; or building energy price predictors specifically for energy-aware scheduling, and planning maintenance crews based on expected failures. He was awarded a prestigious EU ERC Consolidator grant in 2021 to work on conversational human-aware technology for optimisation."
  },
  {
    "objectID": "presentations/archives/2019-10-16.html",
    "href": "presentations/archives/2019-10-16.html",
    "title": "Media and the stock market: Their relationship and abnormal dynamics around earnings announcements",
    "section": "",
    "text": "Résumé\nThis paper investigates the abnormal tone dynamics of media news articles about firms near earnings announcements and how it relates to earnings results and the stock market. We document a post-earnings abnormal tone drift, suggesting inertia in the media opinion. Also, we find that negative cumulative abnormal tone at the earnings events predicts a subsequent stock price reversal. Higher investor’s attention, due to event coverage by the three type of sources (i.e., newswire, newspaper, web publications), amplifies the reversal. Finally, a high level of abnormal share turnover at those events suggests that the reversal is due to overreacting investors.\n\n\nBiographie\nKeven Bluteau has a joint Ph.D. in finance and business economics from the University of Neuchâtel and Vrije Universiteit Brussel. His research focuses on volatility modeling and sentiment analysis applied to finance and business problems. His current work is centered around the field of “Sentometrics”, which lies at the intersection of sentiment analysis and econometrics. Keven is also a proponent of the open-source philosophy. He is the co-author of several R packages that are actively used in the industry, as well as in the research community."
  },
  {
    "objectID": "presentations/archives/2023-05-16.html",
    "href": "presentations/archives/2023-05-16.html",
    "title": "Institutional Ownership and the Resolution of Financial Distress",
    "section": "",
    "text": "Résumé\nWe analyze the distressed firm’s decision between Chapter 11 and an exchange offer. We construct a comprehensive data set on the financial characteristics and capital structure of 269 distressed firms, which, unlike previous studies, uses quarterly information and includes exhaustive data on equity and bond ownership by institutional investors. Logit regressions confirm that the firm’s restructuring decision depends on its financial characteristics as well as the equity and bond ownership of institutional investors. The impact of ownership varies across categories of investors and according to whether they hold equity or bonds. In general, equity ownership favours exchange offers while we find mixed evidence for bond ownership. In particular, hedge funds have a positive impact on the likelihood of exchange offers through their equity holdings but none through their bond holdings. VC/PE firm holdings are also unique in that higher equity holdings are associated with exchange offers and higher bond holdings with Chapter 11. Finally, the magnitude of equity and bond institutional ownership depend on the quarter at which they are measured, confirming the importance of using quarterly data. This is joint work with Timothy C.G. Fisher and Lorenzo Naranjo.\n\n\nBiographie\nDr. Jocelyn Martel est professeur titulaire au département de finance de l’école de gestion ESSEC depuis 2012 et spécialiste de l’économétrie financière. Son champ d’expertise est en théorie financière, faillites, restructurations et évaluations d’entreprises. Il est co-directeur de la chaire Amundi en gestion du risque et des actifs. Il a défendu son habilitation à diriger des recherches à Cergy-Pontoise en 2008 et a obtenu une maîtrise et un doctorat en économie de l’Université de Montréal."
  },
  {
    "objectID": "presentations/archives/2020-10-09.html",
    "href": "presentations/archives/2020-10-09.html",
    "title": "The Use of Binary Choice Forests to Model and Estimate Discrete Choices",
    "section": "",
    "text": "Résumé\nWe show the equivalence of discrete choice models and the class of binary choice forests, which are random forests based on binary choice trees. This suggests that standard machine learning techniques based on random forests can serve to estimate discrete choice models with an interpretable output. This is confirmed by our data-driven theoretical results which show that random forests can predict the choice probability of any discrete choice model consistently, with its splitting criterion capable of recovering preference rank lists. The framework has unique advantages: it can capture behavioral patterns such as irrationality or sequential searches; it handles nonstandard formats of training data that result from aggregation; it can measure product importance based on how frequently a random customer would make decisions depending on the presence of the product; it can also incorporate price information and customer features. Our numerical results show that using random forests to estimate customer choices represented by binary choice forests can outperform the best parametric models in synthetic and real datasets. The paper can be downloaded from this link.\n\n\nBiographie\nNingyuan Chen is assistant professor of the Department of Management at the University of Toronto, Mississauga. He received his PhD in Operations Research from Columbia University in 2015. His research interests include revenue management and dynamic pricing, networks, and statistics."
  },
  {
    "objectID": "presentations/archives/2023-09-29.html",
    "href": "presentations/archives/2023-09-29.html",
    "title": "Statistical inference for multivariate extremes via a geometric approach",
    "section": "",
    "text": "Résumé\nA recent addition to the set of modelling tools for multivariate extremes uses a geometric approach via the use of the so-called gauge function. The gauge function is intrinsically linked to the limit set, which itself is obtained by appropriately scaling random vectors whose marginal distributions are light-tailed, and allowing the sample size to grow arbitrarily large. The geometric shape of this limit set, and thus the gauge function, allows for a simple characterisation of the underlying dependence structure. Compared to other approaches, the geometric approach leads to statistical inference that is easier to interpret and has the ability to accurately estimate probabilities in extremal regions where other methods fail. The primary method proposed in this work relies on the use of parametric forms of gauge functions derived from known copulas. Time permitting, a more flexible semi-parametric Bayesian method to obtain a posterior fit of the limit set is presented. This allows for more accurate extremal probability estimates in dependence structures where non-Bayesian methods struggle. This is joint work with Jennifer Wadsworth (Lancaster University); the semi-parametric Bayesian work is done in collaboration with Ioannis Papastathopoulos and Lambert de Monte (University of Edinburgh).\n\n\nBiographie\nRyan Campbell est doctorant sous la tutelle de Jenny Wadsworth à l’Université Lancaster et spécialisé dans l’analyse de valeurs extrêmes. Originaire de Montréal, il est détenteur d’une maîtrise en mathématiques de l’Université McGill et a travaillé brièvement comme scientifique des données chez Desjardins Assurances."
  },
  {
    "objectID": "presentations/archives/2022-04-28.html",
    "href": "presentations/archives/2022-04-28.html",
    "title": "Toward social welfare and fairness in kidney exchange programs",
    "section": "",
    "text": "Résumé\nMatching markets are part of our daily lives, appearing on online platforms, school admissions and health systems. Their study attracts the interest of optimizers and game theorists. In this talk, we will focus on a particular matching market, the kidney exchange program (KEP), where combinatorial optimization and game theory play an important role.\nA patient in need of a kidney transplant who has an incompatible donor can register on a KEP. The program seeks compatible donor exchanges between these patient-donor pairs to maximize patient benefit. KEPs resulting from the combination of patient-donor pools from different agents (which can be transplant centers, regions or countries) are currently being formed, requiring a game theoretical analysis. In this talk, we will recall literature on multi-player KEPs and see the extension of graph theoretical results to this game. Then, through computational experiments inspired by the Canadian KEP, we will investigate the social welfare generated by a non-cooperative multi-player KEP. These experiments will reveal the multiplicity of socially optimal solutions, leading to a new research question regarding individual patient fairness: Given multiple socially optimal matching plans, how should we select among them? We will end this talk with a proposition to tackle this question as well as an optimization methodology to efficiently implement it in practice.\n\n\nBiographie\nDr. Margarida Carvalho est une mathématicienne portugaise spécialisée dans la programmation entière mixte, la théorie des jeux algorithmique et la complexité calculatoire. Elle est professeure adjointe au Département d’informatique et de recherche opérationnelle de l’Université de Montréal depuis 2018 et est détentrice de la chaire de recherche FRQ-IVADO en science des données pour la théorie des jeux combinatoires. Margarida Carvalho est détentrice d’un doctorat en sciences informatiques, pour lequel elle a obtenu le prix EURO Doctoral Dissertation Award 2018. Elle a effectué un stage postdoctoral au sein de la Chaire d’excellence en recherche du Canada sur la science des données pour la prise de décision en temps réel suite à ses études avant d’être embauchée à l’UdeM. Margarida est actuellement rédactrice adjointe des revues INFORMS Journal on Computing, OR Spectrum et Dynamic Games and Applications."
  },
  {
    "objectID": "presentations/archives/2022-11-25.html",
    "href": "presentations/archives/2022-11-25.html",
    "title": "(Almost) All of Entity Resolution",
    "section": "",
    "text": "Résumé\nWhether the goal is to estimate the number of people that live in a congressional district, to estimate the number of individuals that have died in an armed conflict, or to disambiguate individual authors using bibliographic data, all these applications have a common theme — integrating information from multiple sources. Before such questions can be answered, databases must be cleaned and integrated in a systematic and accurate way, commonly known as record linkage, de-duplication, or entity resolution. In this article, we review motivational applications and seminal papers that have led to the growth of this area. Specifically, we review the foundational work that began in the 1940’s and 50’s that have led to modern probabilistic record linkage. We review clustering approaches to entity resolution, semi- and fully supervised methods, and canonicalization, which are being used throughout industry and academia in applications such as human rights, official statistics, medicine, citation networks, among others. Finally, we discuss current research topics of practical importance. This is joint work with Olivier Binette.\n\n\nBiographie\nRebecca Steorts est professeure adjointe au département de sciences statistiques à l’Université Duke depuis 2015 et est affiliée au U.S. Census Bureau, où elle est mathématicienne statisticienne et responsable des accords coopératifs sur le recensement pour la résolution d’entités et la fusion d’identifiants. Elle a obtenu un doctorat en statistique à l’Université de la Floride encadrée par Malay Ghosh et a été professeure adjointe en visite à Carnegie Mellon entre 2012 et 2015, chapeautée par Stephen Fienberg. Dr. Steorts a publié une trentaine d’articles dans des revues arbitrées."
  },
  {
    "objectID": "presentations/archives/2019-11-29.html",
    "href": "presentations/archives/2019-11-29.html",
    "title": "Life at the (grid edge): Evaluating energy decentralization strategies in California",
    "section": "",
    "text": "Résumé\nAvoiding the worst consequences of climate change hinges on the transition to a deeply decarbonized global energy system. Recent studies suggest that the least costly and risky way in which this transition could unfold is through radical electrification that is dominated by low-carbon sources of electricity. Several visions of this future have been put forward, some of which hinge on revolutions at the grid-edge that enable smarter energy management and vehicle electrification.\nThis talk will cover recent and ongoing research on the next wave of technical and policy interventions at the grid-edge that have the potential to transform the electric power system. In California, policy makers are promoting distributed energy resources like residential solar and energy storage and expecting that they will both decarbonize and decentralize the electric power system. This presentation will describe a systematic analysis of the unintended effects of deploying residential energy storage (RES) systems. Our results show that RES systems predominantly increase emissions when users seek to minimize their electricity cost.\nAnother development at the grid-edge is the proliferation of electric vehicles (EV). California’s size and status as an early adopter of regulatory standards has given it influence in establishing trends which the rest of the nation inherits. The role it plays in efforts to decarbonize transportation could prove revelatory: the state accounts for 10% of new vehicle sales and 50% of EV sales in the U.S. However, one obstacle to policy development is the dearth of information about EV adopters’ attitudes towards incentives, charging, and potentially emergent challenges. This talk will present preliminary results from a large survey of EV adopters in Southern California that reveals these attitudes and allows modelers and policy makers to elaborate strategies for managing a grid with high EV penetration.\n\n\nBiographie\nAhmed Abdulla is Assistant Research Professor in the Department of Engineering and Public Policy at Carnegie Mellon University, and a Research Fellow in the Deep Decarbonization Initiative at the University of California, San Diego. His research uses large, empirical data, optimization and decision analysis to model the strategic development and deployment of innovative energy technologies that decarbonize the energy system and increase its resiliency. His work has been supported by the U.S. National Science Foundation, the Alfred P. Sloan Foundation and the John D. and Catherine T. MacArthur Foundation, among others. Results from his research have been published in leading journals, including Nature Climate Change and the Proceedings of the National Academy of Sciences; they have also been featured in the Wall Street Journal, Bloomberg News and The Los Angeles Times. Prior to Carnegie Mellon, Abdulla was an Assistant Research Scientist in the Center for Energy Research at the University of California, San Diego. He holds a PhD in Engineering and Public Policy from Carnegie Mellon University (2014) and a BS in Chemical Engineering from Princeton University (2009)."
  },
  {
    "objectID": "presentations/archives/2022-11-11.html",
    "href": "presentations/archives/2022-11-11.html",
    "title": "Optimal stopping mean-field games: a linear programming formulation and applications to entry-exit games in electricity markets",
    "section": "",
    "text": "Résumé\nIn this talk, we present recent results on the linear programming approach to stopping mean-field games in a general setting. This relaxed control approach allows to prove existence results under weak assumptions, and lends itself well to numerical implementation. We consider mean-field game problems where the representative agent chooses the optimal time to exit the game, where the instantaneous reward function and the coefficients of the state process may depend on the distribution of the other agents. Furthermore, we establish the equivalence between mean-field games equilibria obtained by the linear programming approach and the ones obtained via other approaches used in the previous literature. We then present a fictious play algorithm to approximate the mean-field game population dynamics in the context of the linear programming approach. Finally, we give an application of the theoretical and numerical contributions introduced in the first part of the talk to an entry-exit game in electricity markets. The talk is based on several works, joint with R. Aïd, G. Bouveret, M. Leutscher and P. Tankov.\n\n\nBiographie\nRoxana Dumitrescu is an associate professor at King’s College London, United Kingdom. She is a leading expert in stochastic control and mean-field games, with publications in leading journals in the field. She has been recently working on optimal stopping mean-field games, a new trend in the literature, and developed together with several co-authors a new approach to solve them based on a linear-programming formulation. Prior to the appointment at King’s College, she has been an associate researcher in the Mathematics Department at Humboldt University in Berlin and a member of the reasearch training group “Stochastic Analysis with Applications in Finance, Physics and Biology” (2015-2016). She defended her PhD in Mathematics at University Dauphine, in Paris (2015). During her PhD studies, she was a researcher in the Financial Mathematics Group at the National French Institute for Research in Computer Science and Automatics Control, INRIA, France."
  },
  {
    "objectID": "presentations/archives/2021-05-06.html",
    "href": "presentations/archives/2021-05-06.html",
    "title": "Dynamic Optimization for Multi-Goals-Based Wealth Management",
    "section": "",
    "text": "Résumé\nWe develop a dynamic programming methodology that seeks to maximize investor outcomes over multiple, potentially competing goals (such as upgrading a home, paying college tuition, or maintaining an income stream in retirement), even when financial resources are limited. Unlike Monte Carlo approaches currently in wide use in the wealth management industry, our approach uses investor preferences to dynamically make the optimal determination for fulfilling or not fulfilling each goal and for selecting the investor’s investment portfolio. This can be computed quickly, even for numerous investor goals spread over different or concurrent time periods, where each goal may allow for partial fulfillment or be all-or-nothing. The probabilities of attaining each (full or partial) goal under the optimal scenario are also computed, so the investor can ensure the algorithm accurately reflects their preference for the relative importance of each of their goals. These portfolio prescriptions are consistent with Prospect Theory.\n\n\nBiographie\nSanjiv Das is the William and Janice Terry Professor of Finance at Santa Clara University’s Leavey School of Business. He holds post-graduate degrees in Finance (M.Phil and Ph.D. from New York University), Computer Science (M.S. from UC Berkeley), an MBA from the Indian Institute of Management, Ahmedabad, B.Com in Accounting and Economics (University of Bombay, Sydenham College), and is also a qualified Cost and Works Accountant. He is a senior editor of The Journal of Investment Management, co-editor of The Journal of Derivatives, and Associate Editor of other academic journals. Prior to being an academic, Sanjiv Das worked in the derivatives business in the Asia-Pacific region as a Vice-President at Citibank. His current research interests include: the modeling of default risk, machine learning, social networks, derivatives pricing models, portfolio theory, and venture capital. He has published over eighty articles in academic journals, and has won numerous awards for research and teaching. His recent book “Derivatives: Principles and Practice” was published in May 2010. He currently also serves as a Senior Fellow at the FDIC Center for Financial Research.\nDaniel N. Ostrov is a Professor of Mathematics and Computer Science at Santa Clara University. He holds a PhD in Applied Mathematics and a MS in Engineering from Brown University. His research area is Mathematical Finance, with an emphasis on using techniques from control theory and partial differential equations to analyze questions concerning optimal investment and personal finance."
  },
  {
    "objectID": "presentations/archives/2021-11-05.html",
    "href": "presentations/archives/2021-11-05.html",
    "title": "Size Distribution of Firms and Strategic Investments in Large Markets: A Stochastic Mean Field Game Approach",
    "section": "",
    "text": "Résumé\nThis paper analyzes how firm size heterogeneity distribution affects capacity investments and profits in large competitive markets with both idiosyncratic and aggregate uncertainty shocks. We use a mean field game approach where firms’ sizes are heterogeneously distributed and decisions on capacity investments (addition) or disinvestments (withdrawal) are optimally made over time to enhance prospects of earning profits under uncertainty. A mean field game framework allows for consideration of markets with a large population of interacting firms. The dynamic of capacity decisions in the market equilibrium is characterized through a fully coupled system of forward-backward stochastic differential equations (FBSDEs), one evolving forward in time and one evolving backward in time. The equilibrium behavior of firms is solved in closed form and market dashboards are used to illustrate how changes in market uncertainty shocks affect the evolution of the size distribution of firms, the market prices, and the distribution of profits. Numerical simulations of the dynamics of the stochastic competitive market with a large number of firms suggest the followings:\n\nThere is a positive relation between growth rates and the size of firms, with larger firms growing faster.\nThe equilibrium size distribution of all firms exhibits two tails.\nThe equilibrium size distribution of large firms is highly asymmetrical and skewed toward the right.\nThe size distribution for small-sized firms is highly skewed to the left.\nA bimodal shape appears in the size distribution of medium-sized firms.\nThere is a U-shaped relation between the size category and the variance of growth rates in the sense that the variance of large-sized and small-sized firms is greater than the variance of medium-sized firms.\n\n\n\nBiographie\nDr. Justin Johnson Kakeu is an Assistant Professor of Economics at University of Prince Edward Island (UPEI) in Canada. He holds a Ph.D. in Economics from University of Montreal (Canada), a Master’s in Statistics and Economics, and a Master’s in Applied Mathematics and Mechanics. Before joining the University of Prince Edward, he taught at Georgia Institute of Technology and Morehouse College in USA.\nHis research interests include Energy and Environmental Economics, Dynamic Macroeconomics, Sustainable Finance and Investing, Uncertainty in Resource and Climate Change Policies."
  },
  {
    "objectID": "presentations/archives/2020-11-27.html",
    "href": "presentations/archives/2020-11-27.html",
    "title": "High-Frequency Factor Models and Regressions",
    "section": "",
    "text": "Résumé\nWe consider a nonparametric time series regression model. Our framework allows precise estimation of betas without the usual assumption of betas being piecewise constant. This property makes our framework particularly suitable to study individual stocks. We provide an inference framework for all components of the model, including idiosyncratic volatility and idiosyncratic jumps. Our empirical analysis investigates the largest data set in the high-frequency literature. First, we use all traded stocks from NYSE, AMEX, and NASDAQ stock markets for 1996-2017 to construct the five Fama-French factors and the momentum factor at the 5-minute frequency. Second, we document the key empirical properties across all the stocks and the new factors, and apply the nonparametric time series regression model with the new high-frequency Fama-French factors. We find that this factor model is effective in explaining the systematic component of the risk of individual stocks. In addition, we provide evidence that idiosyncratic jumps are related to idiosyncratic events such as earnings disappointments. This is joint work with Y. Ait-Sahalia and D. Xiu\n\n\nBiographie\nIlze Kalnina is an Assistant Professor of Economics at the Poole College of Management of North Carolina State University (NCSU). She researches nonparametric estimation and inference for volatility with high frequency data. Prior to joining NC State, Kalnina was an assistant professor at the University of Montreal. Kalnina received a doctorate degree in economics from the London School of Economics."
  },
  {
    "objectID": "presentations/archives/2021-05-18.html",
    "href": "presentations/archives/2021-05-18.html",
    "title": "Rethinking Introductory Statistics for Life Sciences Programs",
    "section": "",
    "text": "Résumé\nDespite reproducibility concerns in science, an increased awareness of the prevalence of statistical errors in life sciences research, and even a tightening up of quantitative reporting standards in journals, statistical errors continue to permeate life sciences research. Yet, life science students often only need to take one Statistics course in their undergraduate programs, if that. What can we, as educators, do to prepare future life scientists for statistical practice in research when we have such limited time with them?       In a collaboration between the Department of Statistical Sciences and the Human Biology Program at the University of Toronto, a second-year undergraduate course was introduced a few years ago to integrate statistics instruction with research design to improve the quantitative training of life sciences students. A study was conducted in this course to explore students’ perceptions about statistical practice in the life sciences and their preparedness use statistical methods appropriately in research. Student attitudes and self-efficacies for Statistics, as well as their abilities to recognize and handle problems related to statistical practice in life sciences research was assessed by way of surveys administered at the beginning and end of the course. In this talk, we will highlight this course, share key findings of the study, and reflect on how the results can help inform future course offerings and training initiatives to better prepare our students to effectively engage with Statistics in research.  \n\n\nBiographie\nDr. Bethany White holds a BScH in Mathematics and Statistics from Acadia University and a PhD in Statistics/Biostatistics and a M.Math in Statistics, both from the University of Waterloo. She is the Associate Chair for Undergraduate Studies in Statistics and an Associate Professor, Teaching Stream, in the Department of Statistical Sciences at the University of Toronto. Her research interests involve the impact of technology-enhanced and simulation activities on student learning and attitudes toward statistics. She also has a pedagogical interest in the quantitative training of life sciences students. She served on the Statistical Society of Canada (SSC) Statistical Education Section Executive Committee between 2013-2016 (President of the Section for 2014-2015) and is currently on the SSC Board of Directors, and has served on the editorial boards of a couple of statistics education journals and on organizing committees for statistics and science education workshops and conferences in Canada and the US.\nDr. Jasty Singh received her PhD in Immunology from the University of Toronto, where she studied approaches to make immune cells from stem cells. Her postdoctoral fellowship (Institute of Biomaterials and Biomedical Engineering, University of Toronto) enabled her to extend these interests into the stem cell bioengineering space, where she engineered immune cells and developed clinically relevant biomaterials for use in immunotherapy. Currently, Jasty is an Assistant Professor, Teaching Stream with the Department of Immunology at the University of Toronto. Her previous research experiences have guided her pedagogical interests, which, in part, focus on preparing undergraduate/graduate life sciences researchers to engage with statistics in research."
  },
  {
    "objectID": "presentations/archives/2020-10-23.html",
    "href": "presentations/archives/2020-10-23.html",
    "title": "Banking networks, systemic risk, and the credit cycle in emerging markets",
    "section": "",
    "text": "Résumé\nWe undertake a large-scale empirical examination of systemic risk among 1048 financial institutions in a large sample of 23 emerging markets, broken down into 5 regions. This work extends the large literature on systemic risk in the US, Europe, and other developed countries to emerging markets, which is relatively under-researched. We present a novel systemic risk score for each financial system by region, across time. This measure is additively decomposable and attributable to each financial institution, and may be used as an objective and quantifiable measure of whether a bank is a SIFI (systemically important financial institution). The level and timing of systemic risk is heterogenous across the 5 regions, and this risk is concentrated in a few banks, more so pre-crisis than post-crisis. Credit and network effects account for over 2/3 of systemic risk (and in some cases, almost all of the risk), and the remaining comes from individual bank variables. Spillovers of systemic risks across regions are mostly contemporaneous within the quarter. A primary principal component (default level) accounts for 1/2 of the variation in systemic risk across the regions with the next two principal components (policy uncertainty and liquidity) accounting for 1/5 each. Aggregate default risk in a region is statistically predictable using our systemic risk metric, thereby supporting timely macro-prudential policy-making.\n\n\nBiographie\nMadhu Kalimipalli is an Equitable Life of Canada Fellow and Full Professor in Finance at the Lazaridis School of Business and Economics, Wilfrid Laurier University, Waterloo, Canada, and the Director of PhD and Research-based Master’s Programs in Management. Prior to joining Laurier in 2000, he was a visiting Assistant professor at the Faculty of Management in McGill University. He holds a PhD in Finance from the Bauer College of Business, University of Houston, and MA degrees in Economics from Rutgers University and the Gokhale Institute of Politics and Economics, Pune, India. His research has appeared in premier finance journals such as Journal of Financial Economics, Journal of Financial Intermediation, Journal of Banking and Finance, Journal of Empirical Finance, among others."
  }
]